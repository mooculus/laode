\documentclass{ximera}

\input{./preamble.tex}

\title{c13.tex}

\begin{document}
\begin{abstract}
BADBAD
\end{abstract}
\maketitle

\chapter{Matrix Normal Forms}
\label{C:HDeigenvalues}

\normalsize
 
In this chapter we generalize to $n\times n$ matrices the theory of matrix 
normal forms presented in Chapter~\ref{Chap:Planar} for $2\times 2$ 
matrices.  In this theory we ask: What is the simplest form that a matrix 
can have up to {\em similarity\/}.  After first presenting several 
preliminary results, the theory culminates in the Jordan normal form theorem, Theorem~\ref{T:Jordan}. 

The first of the matrix normal form results --- every matrix with 
$n$ distinct real eigenvalues can be diagonalized --- is presented 
in Section~\ref{S:RDM}.  The basic idea is that when a matrix has $n$
distinct real eigenvalues, then it has $n$ linearly independent 
eigenvectors.  In Section~\ref{S:CSE} we discuss matrix normal forms 
when the matrix has $n$ distinct eigenvalues some of which are complex.  
When an $n\times n$ matrix has fewer than $n$ linearly independent 
eigenvectors, it must have multiple eigenvalues and generalized eigenvectors.  
This topic is discussed in Section~\ref{S:MGE}.  The Jordan normal form theorem 
is introduced in Section~\ref{S:JNF} and describes similarity of matrices when 
the matrix has fewer than $n$ independent eigenvectors.  The proof is 
given in Appendix~\ref{S:Jordan}.

We introduced Markov matrices in Section~\ref{S:TransitionApplied}.  
One of the theorems discussed there has a proof that relies on the 
Jordan normal form theorem, and we prove this theorem in 
Appendix~\ref{S:TransitionTheory}.
 




\section{Real Diagonalizable Matrices} 
\label{S:RDM}

An $n\times n$ matrix is {\em real diagonalizable\/}
\index{real diagonalizable} if it is similar \index{similar} to a
diagonal matrix\index{matrix!diagonal}.  More precisely, 
an $n\times n$ matrix $A$ is
real diagonalizable if there exists an invertible $n\times n$
matrix S such that
\[
D=S\inv AS
\]
is a diagonal matrix.  In this section we investigate when a
matrix is diagonalizable.  In this discussion we assume that all
matrices have real entries.

We begin with the observation that not all matrices are real
diagonalizable.  We saw in Example~\ref{E:triangular} that the
diagonal entries of the diagonal matrix $D$ are the eigenvalues
of $D$. Theorem~\ref{T:similareigen} states that similar
matrices have the same eigenvalues.  Thus if a matrix is real
diagonalizable, then it must have real eigenvalues.  It follows,
for example, that the $2\times 2$ matrix 
\[
\mattwo{0}{-1}{1}{0}
\]
is not real diagonalizable, since its eigenvalues are $\pm i$. 

However, even if a matrix $A$ has real eigenvalues, it need not
be diagonalizable.  For example, the only matrix similar to the
identity matrix $I_n$ is the identity matrix itself.  To verify
this point, calculate
\[
S\inv I_n S = S\inv S = I_n.
\]
Suppose that $A$ is a matrix all of whose eigenvalues are equal
to $1$.  If $A$ is similar to a diagonal matrix $D$, then $D$
must have all of its eigenvalues equal to $1$.  Since the
identity matrix is the only diagonal matrix with all eigenvalues
equal to $1$, $D=I_n$.  So, if $A$ is similar to a diagonal
matrix, it must itself be the identity matrix.  Consider,
however, the $2\times 2$ matrix
\[
A=\mattwo{1}{1}{0}{1}.
\]
Since $A$ is triangular, it follows that both eigenvalues of $A$
are equal to $1$.  Since $A$ is not the identity matrix, it
cannot be diagonalizable. More generally, if $N$ is a nonzero
strictly upper triangular $n\times n$ matrix, then the matrix
$I_n+N$ is not diagonalizable.  \index{matrix!strictly upper
triangular}

These examples show that complex eigenvalues are always
obstructions to real diagonalization and multiple real eigenvalues
are sometimes obstructions to diagonalization.  Indeed, 

\begin{thm}  \label{T:diagsimple}
Let $A$ be an $n\times n$ matrix with $n$ distinct real
eigenvalues.  
Then $A$ is real diagonalizable\index{real diagonalizable}.
\end{thm}  \index{eigenvalue}\index{eigenvalue!real!distinct}

There are two ideas in the proof of Theorem~\ref{T:diagsimple}, and 
they are summarized in the following lemmas.

\begin{lemma} \label{L:simpleeigen}
Let $\lambda_1,\ldots,\lambda_k$ be distinct real eigenvalues
for an $n\times n$ matrix $A$.  Let $v_j$ be eigenvectors
associated with the eigenvalue $\lambda_j$.  Then
$\{v_1,\ldots,v_k\}$ is a linearly independent set.
\end {lemma} \index{eigenvector}\index{linearly!independent}

\begin{proof} We prove the lemma by using induction on $k$.  When $k=1$
the proof is simple, since $v_1\neq 0$.  So we can assume that
$\{v_1,\ldots,v_{k-1}\}$ is a linearly independent set. 

Let $\alpha_1,\ldots,\alpha_k$ be scalars such that
\begin{equation}  \label{e:linindep}
\alpha_1 v_1 + \cdots + \alpha_k v_k = 0.
\end{equation}
We must show that all $\alpha_j=0$.

Begin by multiplying both sides of \Ref{e:linindep} by $A$, to
obtain: 
\begin{eqnarray}
0 & = & A(\alpha_1 v_1 + \cdots + \alpha_k v_k) \nonumber \\
& = & \alpha_1 Av_1 + \cdots + \alpha_k Av_k \label{e:linother}\\
& = & \alpha_1 \lambda_1 v_1 + \cdots + \alpha_k \lambda_k v_k.\nonumber
\end{eqnarray}

Now subtract $\lambda_k$ times \Ref{e:linindep} from \Ref{e:linother},
to obtain:
\[
\alpha_1(\lambda_1-\lambda_k)v_1 + \cdots +
\alpha_{k-1}(\lambda_{k-1}-\lambda_k)v_{k-1} = 0.
\]
Since $\{v_1,\ldots,v_{k-1}\}$ is a linearly independent set, it
follows that 
\[
\alpha_j(\lambda_j-\lambda_k)=0,
\]
for $j=1,\ldots,k-1$.  Since all of the eigenvalues are
distinct, $\lambda_j-\lambda_k\neq 0$ and $\alpha_j=0$ for
$j=1,\ldots,k-1$. Substituting this information into
\Ref{e:linindep} yields $\alpha_k v_k=0$.  Since $v_k\neq 0$, 
$\alpha_k$ is also equal to zero.  \end{proof}

\begin{lemma}  \label{L:eigenv-diag}
Let $A$ be an $n\times n$ matrix.  Then $A$ is real diagonalizable if
and only if $A$ has $n$ real linearly independent 
eigenvectors\index{eigenvector!linearly independent}.
\end{lemma}  \index{real diagonalizable}

\begin{proof}  Suppose that $A$ has $n$ linearly independent eigenvectors 
$v_1,\ldots,v_n$.  Let $\lambda_1,\ldots,\lambda_n$ be the 
corresponding eigenvalues of $A$; that is, $Av_j=\lambda_jv_j$.
Let $S=(v_1|\cdots|v_n)$ be the $n\times n$ matrix whose columns are the 
eigenvectors $v_j$.  We claim that $D=S\inv AS$ is a diagonal matrix.
Compute
\[
D=S\inv AS=S\inv A(v_1|\cdots|v_n)=S\inv(Av_1|\cdots|Av_n)=
S\inv(\lambda_1v_1|\cdots|\lambda_nv_n).
\]
It follows that 
\[
D=(\lambda_1S\inv v_1|\cdots|\lambda_nS\inv v_n).
\]
Note that 
\[
S\inv v_j=e_j,
\]
since
\[
Se_j = v_j.
\]
Therefore,
\[
D= (\lambda_1e_1|\cdots|\lambda_ne_n)
\]
is a diagonal matrix.  

Conversely, suppose that $A$ is a real diagonalizable matrix.  Then there
exists an invertible matrix $S$ such that $D=S\inv AS$ is diagonal.  Let
$v_j = Se_j$.  We claim that $\{v_1,\ldots,v_n\}$ is a linearly independent 
set of eigenvectors of $A$.

Since $D$ is diagonal, $De_j=\lambda_je_j$ for some real number $\lambda_j$. 
It follows that
\[
Av_j = SDS\inv v_j = SDS\inv Se_j = SDe_j = \lambda_j Se_j = \lambda_jv_j.
\]  
So $v_j$ is an eigenvector of $A$.  Since the matrix $S$ is invertible, its
columns are linearly independent.  Since the columns of $S$ are $v_j$, the
set $\{v_1,\ldots,v_n\}$ is a linearly independent set of eigenvectors of
$A$, as claimed. \end{proof}



\begin{proof}[Theorem~\ref{T:diagsimple}] Let
$\lambda_1,\ldots,\lambda_n$ be the distinct eigenvalues of 
$A$ and let $v_1,\ldots,v_n$ be the corresponding eigenvectors.
Lemma~\ref{L:simpleeigen} implies that $\{v_1,\ldots,v_n\}$ is 
a linearly independent set in $\R^n$ and therefore a basis.
Lemma~\ref{L:eigenv-diag} implies that $A$ is diagonalizable.  \end{proof}

\subsection*{Diagonalization Using MATLAB}
\index{diagonalization!in \protect\Matlab}

Let
\begin{equation*}
A= \left( \begin{array}{rrr} -6 & 12 & 4 \\
 8 & -21 & -8 \\
  -29 & 72 & 27 \end{array} \right).
\end{equation*}
We use \Matlab to answer the questions:  Is $A$ real diagonalizable 
and, if it is, can we find the matrix $S$ such that $S\inv AS$ is diagonal?
We can find the eigenvalues of $A$ by typing {\tt eig(A)}. \Matlabp's
response is:
\begin{verbatim}
ans =
   -2.0000
   -1.0000
    3.0000
\end{verbatim}
Since the eigenvalues of $A$ are real and distinct, 
Theorem~\ref{T:diagsimple} states that $A$ can be diagonalized.  
That is, there is a matrix $S$ such that 
\[
S\inv AS = \left(\begin{array}{rrr} -1 & 0 & 0 \\ 0 & -2 & 0\\
0 & 0 & 3 \end{array}\right)
\]
The proof of Lemma~\ref{L:eigenv-diag} tells us how to find the 
matrix $S$.  We need to find the eigenvectors $v_1,v_2,v_3$ 
associated with the eigenvalues $-1,-2,3$, respectively.  Then 
the matrix $(v_1|v_2|v_3)$ whose columns are the eigenvectors is 
the matrix $S$. To verify this construction we first find the 
eigenvectors of $A$ by typing
\begin{verbatim}
v1 = null(A+eye(3));
v2 = null(A+2*eye(3));
v3 = null(A-3*eye(3));
\end{verbatim} 
Now type {\tt S = [v1 v2 v3]} to obtain
\begin{verbatim}
S =
    0.8729    0.7071         0
    0.4364    0.0000    0.3162
   -0.2182    0.7071   -0.9487
\end{verbatim}
Finally, check that $S\inv AS$ is the desired diagonal matrix by 
typing {\tt inv(S)*A*S}\index{\computer!inv} to obtain
\begin{verbatim}
ans =
   -1.0000    0.0000         0
    0.0000   -2.0000   -0.0000
    0.0000         0    3.0000
\end{verbatim}

It is cumbersome to use the 
{\tt null}\index{\computer!null} command to find 
eigenvectors and \Matlab has been preprogrammed to do these
computations automatically.  We can use the {\tt eig} command 
to find the eigenvectors and eigenvalues of a matrix $A$, as 
follows.  Type
\begin{verbatim}
[S,D] = eig(A)
\end{verbatim}\index{\computer!eig}
and \Matlab responds with 
\begin{verbatim}
S =
   -0.7071    0.8729   -0.0000
   -0.0000    0.4364   -0.3162
   -0.7071   -0.2182    0.9487
 
D = 
   -2.0000         0         0
         0   -1.0000         0
         0         0    3.0000
\end{verbatim}
The matrix $S$ is the transition matrix\index{matrix!transition} 
whose columns are the 
eigenvectors of $A$ and the matrix $D$ is a diagonal matrix whose 
$j^{th}$ diagonal entry is the eigenvalue of $A$ corresponding to 
the eigenvector in the $j^{th}$ column of $S$.


\EXER

\TEXER

\begin{exercise} \label{c10.3.1}
Let $A=\mattwo{0}{3}{3}{0}$.  
\begin{itemize}
\item[(a)]  Find the eigenvalues and eigenvectors of $A$.
\item[(b)]  Find an invertible matrix $S$ such that $S\inv AS$ is a 
diagonal matrix $D$.  What is $D$?
\end{itemize}
\end{exercise}

\begin{exercise} \label{c10.3.2}
The eigenvalues of 
\[
A=\left(\begin{array}{rrr} -1 & 2 & -1\\ 3& 0 & 1 \\ -3 & -2 & -3 \end{array}
\right)
\]
are $2,-2,-4$.  Find the eigenvectors of $A$ for each of these eigenvalues and 
find a $3\times 3$ invertible matrix $S$ so that $S\inv AS$ is diagonal.
\end{exercise} 

\begin{exercise} \label{c10.3.3}
Let
\[
A=\left(\begin{array}{rrr} -1 & 4 & -2 \\ 0 & 3 & -2 \\ 0 & 4 & -3 \end{array}
\right).
\]
Find the eigenvalues and eigenvectors of $A$, and find an invertible  
matrix $S$ so that $S\inv AS$ is diagonal.
\end{exercise}

\begin{exercise} \label{c10.3.4}
Let $A$ and $B$ be similar $n\times n$ matrices.
\begin{itemize}
\item[(a)]  Show that if $A$ is invertible, then $B$ is invertible.
\item[(b)]  Show that $A+A\inv$ is similar to $B+B\inv$.
\end{itemize}
\end{exercise}

\begin{exercise} \label{c10.3.5}
Let $A$ and $B$ be $n\times n$ matrices.  Suppose that $A$ is real 
diagonalizable and that $B$ is similar to $A$.  Show that $B$ is 
real diagonalizable.
\end{exercise}

\begin{exercise} \label{c10.3.6}
Let $A$ be an $n\times n$ real diagonalizable matrix. Show that $A+\alpha I_n$
is also real diagonalizable.
\end{exercise}

\begin{exercise} \label{c10.3.6A}
Let $A$ be an $n\times n$ matrix with a real eigenvalue $\lambda$ and 
associated eigenvector $v$.  Assume that all other eigenvalues of $A$ are
different from $\lambda$.  Let $B$ be an $n\times n$ matrix that commutes
with $A$; that is, $AB=BA$.  Show that $v$ is also an eigenvector for $B$.
\end{exercise}

\begin{exercise} \label{c10.3.6B}
Let $A$ be an $n\times n$ matrix with distinct real eigenvalues and let $B$ 
be an $n\times n$ matrix that commutes with $A$.  Using the result of
Exercise~\ref{c10.3.6A}, show that there is a matrix $S$ that simultaneously
diagonalizes $A$ and $B$; that is, $S\inv AS$ and $S\inv BS$ are both
diagonal matrices.
\end{exercise}

\begin{exercise} \label{c10.3.6C}
Let $A$ be an $n\times n$ matrix all of whose eigenvalues equal $\pm 1$. Show
that if $A$ is diagonalizable, the $A^2=I_n$.
\end{exercise}

\begin{exercise} \label{c10.3.6D}
Let $A$ be an $n\times n$ matrix all of whose eigenvalues equal $0$ and $1$. 
Show that if $A$ is diagonalizable, the $A^2=A$.
\end{exercise}

\CEXER

\begin{exercise} \label{c10.3.7}
Consider the $4\times 4$ matrix
\begin{equation*}
C =\left(\begin{array}{rrrr}  12 & 48 & 68 & 88 \\ -19 & -54 & -57 & -68\\
22 & 52 & 66 & 96 \\ -11 & -26 & -41 & -64 \end{array}\right).
\end{equation*}
Use \Matlab to show that the eigenvalues of $C$ are real and distinct.
Find a matrix $S$ so that $S\inv CS$ is diagonal.  
\end{exercise}

\noindent In Exercises~\ref{c10.3.8a} -- \ref{c10.3.8b} use \Matlab 
to decide whether or not the given matrix is real diagonalizable.
\begin{exercise} \label{c10.3.8a}
\begin{equation*}
A=\left(
\begin{array}{rrrr}
      -2.2 & 4.1&-1.5&-0.2\\
      -3.4 & 4.8&-1.0& 0.2\\
      -1.0 & 0.4& 1.9& 0.2\\
     -14.5 &17.8&-6.7& 0.6
\end{array}
\right).
\end{equation*}
\end{exercise}
\begin{exercise} \label{c10.3.8b}
\begin{equation*}
B=\left(
\begin{array}{rrrrr}
      1.9 & 2.2 & 1.5 & -1.6 & -2.8\\
      0.8 & 2.6 & 1.5 & -1.8 & -2.0\\
      2.6 & 2.8 & 1.6 & -2.1 & -3.8\\
      4.8 & 3.6 & 1.5 & -3.1 & -5.2\\
     -2.1 & 1.2 & 1.7 & -0.2 &  0.0
\end{array} \right).
\end{equation*}
\end{exercise}



\section{Simple Complex Eigenvalues}  \label{S:CSE}

Theorem~\ref{T:diagsimple} states that a matrix $A$ with real
unequal eigenvalues may be diagonalized. It follows that 
in an appropriately chosen basis (the basis of eigenvectors), 
matrix multiplication\index{matrix!multiplication} by $A$ acts 
as multiplication by these real eigenvalues.  Moreover, geometrically, 
multiplication by $A$ stretches or contracts vectors in 
eigendirections (depending on whether the eigenvalue is greater
than or less than $1$ in absolute value).

The purpose of this section is to show that a similar kind of diagonalization 
is possible when the matrix has distinct complex eigenvalues. See
Theorem~\ref{T:diagsimplecpx} and Theorem~\ref{T:Complexdiag}.  We show 
that multiplication by a matrix with complex eigenvalues corresponds
to multiplication by complex numbers.  We also show that multiplication by 
complex eigenvalues correspond geometrically to rotations\index{rotation} 
as well as expansions\index{expansion} and contractions\index{contraction}.

\subsubsection*{The Geometry of Complex Eigenvalues: Rotations and
Dilatations}

Real $2\times 2$ matrices are the smallest real matrices where complex 
eigenvalues can possibly occur.  In Chapter~\ref{Chap:Planar}, 
Theorem~\ref{T:putinform}(b) we discussed the classification of such matrices 
up to similarity.  Recall that if the eigenvalues of a $2\times 2$ matrix $A$ 
are $\sigma\pm i\tau$, 
then $A$ is similar to the matrix 
\begin{equation} \label{E:normalfm2}
\mattwo{\sigma}{-\tau}{\tau}{\sigma}.
\end{equation}
Moreover, the basis in which $A$ has the form \Ref{E:normalfm2} is found
as follows.  Let $v=w_1+iw_2$ be the eigenvector of $A$ corresponding to 
the eigenvalue $\sigma-i\tau$.  Then $\{w_1,w_2\}$ is the desired 
basis.\index{basis}

Geometrically, multiplication of vectors in $\R^2$ by \Ref{E:normalfm2} is 
the same as a rotation followed by a dilatation\index{dilatation}.  More 
specifically, let $r=\sqrt{\sigma^2+\tau^2}$.  So the point $(\sigma,\tau)$ 
lies on the circle of radius $r$ about the origin, and there is an angle 
$\theta$ such that $(\sigma,\tau)=(r\cos\theta,r\sin\theta)$.  Now we can 
rewrite \Ref{E:normalfm2} as
\[
\mattwo{\sigma}{-\tau}{\tau}{\sigma}
= r\mattwo{\cos\theta}{-\sin{\theta}}{\sin\theta}{\cos\theta}=rR_{\theta},
\]
where $R_\theta$ is rotation counterclockwise through angle $\theta$.  From 
this discussion we see that geometrically complex eigenvalues are associated 
with rotations followed either by stretching ($r>1$) or contracting ($r<1$).

As an example, consider the matrix
\begin{equation}  \label{E:exampA}
A = \mattwo{2}{1}{-2}{0}.
\end{equation}
The characteristic polynomial\index{characteristic polynomial}
of $A$ is $p_A(\lambda)=\lambda^2-2\lambda+2$.
Thus the eigenvalues of $A$ are $1\pm i$, and $\sigma=1$ and $\tau=1$ for
this example.  An eigenvector associated to the eigenvalue $1-i$ is 
$v=(1,-1-i)^t=(1,-1)^t+i(0,-1)^t$. Therefore, 
\[
B = S\inv A S = \mattwo{1}{-1}{1}{1} \quad\mbox{where}\quad 
S = \mattwo{1}{0}{-1}{-1},
\]
as can be checked by direct calculation.  Moreover, we can rewrite
\[
B = \sqrt{2}\mattwo{\frac{\sqrt{2}}{2}}{-\frac{\sqrt{2}}{2}}
{\frac{\sqrt{2}}{2}}{\frac{\sqrt{2}}{2}} =
\sqrt{2}R_{\frac{\pi}{4}}.
\]
So, in an appropriately chosen coordinate system, multiplication by $A$ rotates
vectors counterclockwise by $45^\circ$ and then expands the result by a 
factor of $\sqrt{2}$.  See Exercise~\ref{c10.4.rotate}.

\subsubsection*{The Algebra of Complex Eigenvalues: Complex Multiplication}
\index{eigenvalue!complex}

We have shown that the normal form\index{normal form} \Ref{E:normalfm2} can 
be interpreted geometrically as a rotation followed by a dilatation.  There 
is a second algebraic interpretation of \Ref{E:normalfm2}, and this 
interpretation is based on multiplication by complex numbers.

Let $\lambda=\sigma+i\tau$ be a complex number and consider the matrix 
associated with complex multiplication, that is, the linear mapping
\begin{equation}  \label{e:cplxmap}
z\mapsto \lambda z
\end{equation}
on the complex plane.  By identifying real and imaginary parts, we can 
rewrite \Ref{e:cplxmap} as a real $2\times 2$ matrix in the following way. 
Let $z=x + iy$.  Then
\[
\lambda z = (\sigma+i\tau)(x+iy) = (\sigma x - \tau y) + 
i(\tau x + \sigma y).
\]
Now identify $z$ with the vector $(x,y)$; that is, the vector whose first
component is the real part of $z$ and whose second component is the 
imaginary part.  Using this identification the complex number $\lambda z$
is identified with the vector $(\sigma x - \tau y,\tau x + \sigma y)$. 
So, in real coordinates and in matrix form, \Ref{e:cplxmap} becomes
\[
\vectwo{x}{y}\mapsto \vectwo{\sigma x - \tau y}{\tau x + \sigma y} =
\mattwo{\sigma}{-\tau}{\tau}{\sigma} \vectwo{x}{y}.
\]
That is, the matrix corresponding to multiplication of $z=x+iy$ by the 
complex number $\lambda=\sigma+i\tau$ is the one that multiplies the 
vector $(x,y)^t$ by the normal form matrix \Ref{E:normalfm2}.

\subsubsection*{Direct Agreement Between the Two Interpretations of 
\protect\Ref{E:normalfm2}}   

We have shown that matrix multiplication by \Ref{E:normalfm2} may be thought 
of either algebraically as multiplication by a complex number (an eigenvalue)
or geometrically as a rotation followed by a dilatation.  We now show how to
go directly from the algebraic interpretation to the geometric interpretation.

Euler's formula\index{Euler's formula} (Chapter~\ref{Chap:Planar}, 
\Ref{E:Euler}) states that
\[
e^{i\theta} = \cos\theta + i\sin\theta
\]
for any real number $\theta$. It follows that we can write a 
complex number $\lambda=\sigma+i\tau$ in polar form as
\[
\lambda = re^{i\theta}
\]
where $r^2=\lambda\overline{\lambda}=\sigma^2+\tau^2$, $\sigma=r\cos\theta$,
and $\tau=r\sin\theta$.  

Now consider multiplication by $\lambda$ in polar form.  Write 
$z=se^{i\varphi}$ in polar form, and compute
\[
\lambda z = re^{i\theta}se^{i\varphi} = rse^{i(\varphi+\theta)}.
\]
It follows from polar form that multiplication of $z$ by 
$\lambda=re^{i\theta}$  rotates 
$z$ through an angle $\theta$ and dilates the result by the factor $r$.
Thus Euler's formula directly relates the geometry of 
rotations\index{rotation} and 
dilatations\index{dilatation} with the 
algebra of multiplication by a complex number.


\subsection*{Normal Form Matrices with Distinct Complex Eigenvalues}
\index{normal form}\index{eigenvalue!complex!distinct}

In the first parts of this section we have discussed a geometric and 
an algebraic approach to matrix multiplication by $2\times 2$ matrices with 
complex eigenvalues.   We now turn our attention to classifying $n\times n$ 
matrices that have distinct eigenvalues, whether these eigenvalues are real 
or complex.  We will see that there are two ways to frame this classification 
--- one algebraic (using complex numbers) and one geometric (using rotations 
and dilatations).

\subsubsection*{Algebraic Normal Forms: The Complex Case}

Let $A$ be an $n\times n$ matrix with real entries and $n$ distinct 
eigenvalues $\lambda_1,\ldots,\lambda_n$.  Let $v_j$ be an eigenvector 
associated with the eigenvalue $\lambda_j$.  By methods that are 
entirely analogous to those in Section~\ref{S:RDM} we can diagonalize 
the matrix $A$ over the complex numbers.  The resulting theorem is 
analogous to Theorem~\ref{T:diagsimple}.  

More precisely, the $n\times n$ matrix $A$ is {\em complex diagonalizable\/} 
\index{complex diagonalizable}
if there is a complex $n\times n$ matrix $T$ such that
\[
T\inv AT = \left(\begin{array}{cccc} \lambda_1 & 0 & \cdots & 0 \\
	0 & \lambda_2 & \cdots & 0\\
	\vdots & \vdots & \ddots & \vdots \\
	0 & 0 & \cdots & \lambda_n \end{array} \right).
\]

\begin{thm}  \label{T:diagsimplecpx}
Let $A$ be an $n\times n$ matrix with $n$ distinct eigenvalues.  Then $A$ 
is complex diagonalizable.
\end{thm}  \index{eigenvalue}

The proof of Theorem~\ref{T:diagsimplecpx} follows from a theoretical 
development virtually word for word the same as that used to prove 
Theorem~\ref{T:diagsimple} in Section~\ref{S:RDM}.  Beginning from the 
theory that we have developed so far, the difficulty in proving this 
theorem lies in the need to base the theory of linear algebra on complex 
scalars rather than real scalars.  We will not pursue that development here.

As in Theorem~\ref{T:diagsimple}, the proof of Theorem~\ref{T:diagsimplecpx} 
shows that the complex matrix $T$ is the matrix whose columns are
the eigenvectors $v_j$ of $A$; that is,
\[
T = (v_1|\cdots|v_n).
\] 

Finally, we mention that the computation of inverse\index{inverse}
matrices with complex entries is the same as that for matrices with real 
entries.  That is, row reduction\index{row!reduction}
of the $n\times 2n$ matrix $(T|I_n)$ leads, when $T$ is
invertible, to the matrix $(I_n|T\inv)$.

\subsubsection*{Two Examples}

As a first example, consider the normal form $2\times 2$ matrix 
\Ref{E:normalfm2} that has eigenvalues $\lambda$ and $\overline{\lambda}$, 
where $\lambda=\sigma+i\tau$.   Let 
\[
B = \mattwo{\sigma}{-\tau}{\tau}{\sigma} \AND 
C = \mattwo{\lambda}{0}{0}{\overline{\lambda}}.
\]
Since the eigenvalues of $B$ and $C$ are identical, 
Theorem~\ref{T:diagsimplecpx} implies that there is a $2\times 2$ complex 
matrix $T$ such that
\begin{equation}  \label{E:BCsimilar}
C = T\inv B T.
\end{equation}
Moreover, the columns of $T$ are the complex eigenvectors $v_1$ and $v_2$
associated to the eigenvalues $\lambda$ and $\overline{\lambda}$.

It can be checked that the eigenvectors of $B$ are $v_1=(1,-i)^t$ and 
$v_2=(1,i)^t$.  On setting 
\[
T = \mattwo{1}{1}{-i}{i},
\]
it is a straightforward calculation to verify that $C=T\inv BT$.  


As a second example, consider the matrix 
\begin{equation*}
A = \left(\begin{array}{rrr}     4  &   2   &  1\\
     2  &  -3  &   1\\  1 &   -1  &  -3 \end{array} \right).
\end{equation*}
Using \Matlab we find the eigenvalues of $A$ by typing {\tt eig(A)}. 
They are:
\begin{verbatim}
ans =
   4.6432          
  -3.3216 + 0.9014i
  -3.3216 - 0.9014i
\end{verbatim}
We can diagonalize (over the complex numbers) using \Matlab --- indeed 
\Matlab is programmed to do these calculations over the complex numbers. 
Type {\tt [T,D] = eig(A)} and obtain
\begin{verbatim}
T =
   0.9604    -0.1299 + 0.1587i  -0.1299 - 0.1587i
   0.2632     0.0147 - 0.5809i   0.0147 + 0.5809i
   0.0912     0.7788 - 0.1173i   0.7788 + 0.1173i

D =
   4.6432          0                  0          
        0    -3.3216 + 0.9014i        0          
        0          0            -3.3216 - 0.9014i
\end{verbatim}
This calculation can be checked by typing {\tt inv(T)*A*T} to see that 
the diagonal matrix\index{matrix!diagonal} {\tt D} appears.  
One can also check that the columns of 
{\tt T} are eigenvectors of $A$.

Note that the development here does not depend on the matrix $A$ having 
real entries.  Indeed, this diagonalization can be completed using 
$n\times n$ matrices with complex entries --- and \Matlab can handle such 
calculations.


\subsubsection*{Geometric Normal Forms: Block Diagonalization}
\index{normal form!geometric}

There is a second normal form theorem based on the geometry of rotations 
and dilatations for real $n\times n$ matrices $A$.  In this normal form 
we determine all matrices $A$ that have distinct eigenvalues --- up to 
similarity by real $n\times n$ matrices $S$.  The normal form results in 
matrices that are block diagonal with either $1\times 1$ blocks or 
$2\times 2$ blocks of the form \Ref{E:normalfm2} on the diagonal.

A real $n\times n$ matrix is in 
{\em real block diagonal form\/}
\index{matrix!block diagonal!real}
if it is a block diagonal matrix
\begin{equation} \label{e:blockform}
\left(\begin{array}{cccc} B_1 & 0 & \cdots & 0 \\
	0 & B_2 & \cdots & 0\\
	\vdots & \vdots & \ddots & \vdots \\
	0 & 0 & \cdots & B_m \end{array} \right),
\end{equation}
where each $B_j$ is either a $1\times 1$ block 
\[
B_j = \lambda_j
\]
for some real number $\lambda_j$ or a $2\times 2$ block
\begin{equation} \label{e:2x2block}
B_j =   \mattwo{\sigma_j}{-\tau_j}{\tau_j}{\sigma_j}
\end{equation}
where $\sigma_j$ and $\tau_j\neq 0$ are real numbers.  A
matrix is {\em real block diagonalizable\/} if it is 
similar\index{similar} to a real block diagonal form matrix.

Note that the real eigenvalues of a real block diagonal form matrix 
are just the real numbers $\lambda_j$ that occur in the $1\times 1$
blocks.  The complex eigenvalues are the eigenvalues of the
$2\times 2$ blocks $B_j$ and are $\sigma_j\pm i\tau_j$.


\begin{thm} \label{T:Complexdiag}
Every $n\times n$ matrix $A$ with $n$ distinct eigenvalues is 
real block diagonalizable.
\end{thm}

We need two preliminary results.  
\begin{lemma} \label{L:indepcomplx}
Let $\lambda_1,\ldots,\lambda_q$ be distinct (possible complex) eigenvalues 
of an $n\times n$ matrix $A$.  Let $v_j$ be a (possibly complex)
eigenvector associated with the eigenvalue $\lambda_j$.  Then
$v_1,\ldots,v_q$ are linearly independent in the sense that if
\begin{equation}  \label{E:indepcomplx}
\alpha_1v_1 + \cdots + \alpha_qv_q = 0
\end{equation}
for (possibly complex) scalars $\alpha_j$, then $\alpha_j=0$ for all $j$.
\end{lemma}

\begin{proof}  The proof is identical in spirit with the proof of 
Lemma~\ref{L:simpleeigen}.  Proceed by induction on $q$.  When $q=1$ the
lemma is trivially valid, as $\alpha v=0$ for $v\neq 0$ implies that
$\alpha=0$, even when $\alpha\in\C$ and $v\in\C^n$.  

By induction assume the lemma is valid for $q-1$.  Now apply $A$ to 
\Ref{E:indepcomplx} obtaining
\[
\alpha_1\lambda_1v_1 + \cdots + \alpha_q\lambda_qv_q = 0.
\]
Subtract this identity from $\lambda_q$ times \Ref{E:indepcomplx}, and obtain
\[
\alpha_1(\lambda_1-\lambda_q)v_1 + \cdots +
\alpha_{q-1}(\lambda_{q-1}-\lambda_q)v_{q-1} = 0.
\]
By induction 
\[
\alpha_j(\lambda_j-\lambda_q) = 0
\]
for $j=1,\ldots, q-1$.  Since the $\lambda_j$ are distinct it follows that 
$\alpha_j=0$ for $j=1,\ldots, q-1$.  Hence \Ref{E:indepcomplx} implies that 
$\alpha_qv_q = 0$; since $v_q\neq 0$, $\alpha_q=0$.  \end{proof}

\begin{lemma}  \label{L:rlcmplx}
Let $\mu_1,\ldots,\mu_k$ be distinct real eigenvalues of an $n\times n$ 
matrix $A$ and let $\nu_1,\overline{\nu}_1\ldots,\nu_\ell,\overline{\nu}_\ell$
be distinct complex conjugate eigenvalues of $A$.  Let $v_j\in\R^n$ be
eigenvectors associated to $\mu_j$ and let $w_j=w_j^r+iw_j^i$ be eigenvectors
associated with the eigenvalues $\nu_j$.  Then the $k+2\ell$ vectors 
\[
v_1,\ldots,v_k,w_1^r,w_1^i,\ldots,w_\ell^r,w_\ell^i
\]
in $\R^n$ are linearly independent.
\end{lemma}

\begin{proof}  Let $w=w^r+iw^i$ be a vector in $\C^n$ and let $\beta^r$ and
$\beta^i$ be real scalars.  Then 
\begin{equation}  \label{E:realcmplx}
\beta^rw^r + \beta^iw^i = \beta w + \overline{\beta} \overline{w},
\end{equation}
where $\beta = \frac{1}{2}(\beta^r-i\beta^i)$.  Identity \Ref{E:realcmplx} 
is verified by direct calculation.

Suppose now that 
\begin{equation}  \label{E:rlcplxlc} 
\alpha_1v_1+\cdots+\alpha_kv_k + \beta_1^rw_1^r+\beta_1^iw_1^i + \cdots +
\beta_\ell^rw_\ell^r+\beta_\ell^iw_\ell^i = 0
\end{equation}
for real scalars $\alpha_j$, $\beta_j^r$ and $\beta_j^i$.  Using 
\Ref{E:realcmplx} we can rewrite \Ref{E:rlcplxlc} as
\[
\alpha_1v_1+\cdots+\alpha_kv_k + \beta_1w_1+\overline{\beta}_1\overline{w}_1 
+ \cdots + \beta_\ell w_\ell+\overline{\beta}_\ell\overline{w}_\ell = 0,
\]
where $\beta_j = \frac{1}{2}(\beta_j^r-i\beta_j^i)$.  Since the eigenvalues 
\[
\mu_1,\ldots,\mu_k,\nu_1,\overline{\nu}_1\ldots,\nu_\ell,\overline{\nu}_\ell
\]
are all distinct, we may apply Lemma~\ref{L:indepcomplx} to conclude that 
$\alpha_j=0$ and $\beta_j=0$.  It follows that $\beta_j^r=0$ and
$\beta_j^i=0$, as well, thus proving linear independence.  \end{proof}. 


\begin{proof}[Theorem~\ref{T:Complexdiag}]   Let $\mu_j$ for 
$j=1,\ldots,k$ be the real eigenvalues of $A$ and let 
$\nu_j,\overline{\nu}_j$ for $j=1,\ldots,\ell$ be the complex eigenvalues of 
$A$. Since the eigenvalues are all distinct, it follows that $k+2\ell=n$.

Let $v_j$ and $w_j=w_j^r+iw_j^i$ be eigenvectors associated with the 
eigenvalues $\mu_j$ and $\overline{\nu}_j$.  It follows from 
Lemma~\ref{L:rlcmplx} that the $n$ real vectors
\begin{equation}  \label{e:complexeigen}
v_1,\ldots,v_k,w_1^r,w_1^i,\ldots,w_\ell^r,w_\ell^i
\end{equation}
are linearly independent and hence form a basis for $\R^n$.

We now show that $A$ is real block diagonalizable.  Let $S$ be the $n\times
n$ matrix whose columns are the vectors in \Ref{e:complexeigen}.  Since
these vectors are linearly independent, $S$ is invertible.  We claim that 
$S\inv AS$ is real block diagonal.  This statement is verified by direct
calculation.

First, note that $Se_j=v_j$ for $j=1,\ldots,k$ and compute
\[
(S\inv AS)e_j = S\inv Av_j = \mu_j S\inv v_j = \mu_j e_j.
\]
It follows that the first $k$ columns of $S\inv AS$ are zero except for the 
diagonal entries, and those diagonal entries equal $\mu_1,\ldots,\mu_k$.

Second, note that $Se_{k+1}=w_1^r$ and $Se_{k+2}=w_1^i$.  Write the complex 
eigenvalues as
\[
\nu_j = \sigma_j+i\tau_j.
\]
Since 
$Aw_1 = \overline{\nu}_1w_1$, it follows that
\begin{eqnarray*}
Aw_1^r+iAw_1^i & = & (\sigma_1-i\tau_1)(w_1^r+iw_1^i)\\
& = & (\sigma_1w_1^r+\tau_1w_1^i) + i(-\tau_1w_1^r +\sigma_1w_1^i).
\end{eqnarray*}
Equating real and imaginary parts leads to 
\begin{equation}  \label{e:complexsimple}
\begin{array}{ccc} Aw_1^r & = & \sigma_1w_1^r+\tau_1w_1^i\\
Aw_1^i & = &   -\tau_1w_1^r+\sigma_1w_1^i. \end{array}
\end{equation}
Using \Ref{e:complexsimple}, compute
\[
(S\inv AS)e_{k+1} = S\inv Aw_1^r = S\inv(\sigma_1w_1^r+\tau_1w_1^i) 
= \sigma_1e_{k+1} + \tau_1e_{k+2}.
\]
Similarly, 
\[
(S\inv AS)e_{k+2} = S\inv Aw_1^i = S\inv(-\tau_1w_1^r+\sigma_1w_1^i)
= -\tau_1e_{k+1} + \sigma_1e_{k+2}.
\]
Thus, the $k^{th}$ and $(k+1)^{st}$ columns of $S\inv AS$ have the desired
diagonal block in the $k^{th}$ and $(k+1)^{st}$ rows, and have all other 
entries equal to zero.

The same calculation is valid for the complex eigenvalues 
$\nu_2,\ldots,\nu_\ell$.  Thus, $S\inv AS$ is real block diagonal, as 
claimed.   \end{proof}


\subsubsection*{\Matlab Calculations of Real Block Diagonal Form}
\index{real block diagonal form!in \protect\Matlab}

Let $C$ be the $4\times 4$ matrix
\begin{equation*}
C =\left(\begin{array}{rrrr}  1 & 0 & 2 & 3 \\ 2 & 1 & 4 & 6\\
-1 & -5 & 1 & 3 \\ 1 & 4 & 7 & 10 \end{array}\right).
\end{equation*}
Using \Matlab enter $C$ by typing {\tt e13\_2\_14} and find the 
eigenvalues of $C$ by typing {\tt eig(C)}\index{\computer!eig} 
to obtain
\begin{verbatim}
ans =
   0.5855 + 0.8861i
   0.5855 - 0.8861i
  -0.6399          
  12.4690    
\end{verbatim}
We see that $C$ has two real and two complex conjugate eigenvalues.
To find the complex eigenvectors associated with these eigenvalues,
type 
\begin{verbatim}
[T,D] = eig(C)
\end{verbatim}
\Matlab responds with
\begin{verbatim}
T =
  -0.0787 + 0.0899i  -0.0787 - 0.0899i   0.0464             0.2209          
   0.0772 + 0.2476i   0.0772 - 0.2476i   0.0362             0.4803          
  -0.5558 - 0.5945i  -0.5558 + 0.5945i  -0.8421            -0.0066          
   0.3549 + 0.3607i   0.3549 - 0.3607i   0.5361             0.8488          
D =
   0.5855 + 0.8861i        0                  0                  0          
        0             0.5855 - 0.8861i        0                  0          
        0                  0            -0.6399                  0          
        0                  0                  0            12.4690          
\end{verbatim}
The $4\times 4$ matrix $T$ has the eigenvectors of $C$ as columns.
The $j^{th}$ column is the eigenvector associated with the $j^{th}$
diagonal entry in the diagonal matrix $D$. 

To find the matrix $S$ that puts $C$ in real block diagonal form, we
need to take the real and imaginary parts of the eigenvectors 
corresponding to the complex eigenvalues and the real eigenvectors 
corresponding to the real eigenvalues.  In this case, type
\index{\computer!real}\index{\computer!imag}
\begin{verbatim}
S = [real(T(:,1)) imag(T(:,1)) T(:,3) T(:,4)]
\end{verbatim}
to obtain 
\begin{verbatim}
S =
   -0.0787    0.0899    0.0464    0.2209
    0.0772    0.2476    0.0362    0.4803
   -0.5558   -0.5945   -0.8421   -0.0066
    0.3549    0.3607    0.5361    0.8488
\end{verbatim} 
Note that the $1^{st}$ and $2^{nd}$ columns are the real and 
imaginary parts of the complex eigenvector.  Check that 
{\tt inv(S)*C*S} is the matrix in complex diagonal form
\begin{verbatim}
ans = 
    0.5855    0.8861    0.0000    0.0000
   -0.8861    0.5855    0.0000   -0.0000
    0.0000    0.0000   -0.6399    0.0000
   -0.0000   -0.0000   -0.0000   12.4690
\end{verbatim}
as proved in Theorem~\ref{T:Complexdiag}.



\EXER

\TEXER


\begin{exercise}  \label{c10.4.3}
Consider the $2\times 2$ matrix 
\[
A = \mattwo{3}{1}{-2}{1},
\]
whose eigenvalues are $2\pm i$ and whose associated eigenvectors are:
\[
\vectwoc{1-i}{2i} \AND \vectwoc{1+i}{-2i}
\]
Find a complex $2\times 2$ matrix $T$ such that $C=T\inv AT$ is complex
diagonal and a real $2\times 2$ matrix $S$ so that $B=S\inv AS$ is in real
block diagonal form.
\end{exercise}

\begin{exercise}  \label{c10.4.4}
Let 
\[
A=\mattwo{2}{5}{-2}{0}.
\]
Find a complex $2\times 2$ matrix $T$ such that $T\inv AT$ is complex
diagonal and a real $2\times 2$ matrix $S$ so that $S\inv AS$ is in real
block diagonal form.
\end{exercise}



\CEXER

\begin{exercise} \label{c10.4.rotate}
Use {\sf map}\index{\computer!map} to verify that 
the normal form matrices \Ref{E:normalfm2} are 
just rotations followed by dilatations.  In particular, use {\sf map} to 
study the normal form matrix
\[
A = \mattwo{1}{-1}{1}{1}.
\]
Then compare your results with the similar matrix
\[
B = \mattwo{2}{1}{-2}{0}.
\]
\end{exercise}

\begin{exercise} \label{c10.4.6}
Consider the $2\times 2$ matrix
\[
A = \mattwo{-0.8318}{-1.9755}{0.9878}{1.1437}.
\]
\begin{itemize}
\item[(a)]  Use \Matlab to find the complex conjugate eigenvalues and 
eigenvectors of $A$.
\item[(b)]  Find the real block diagonal normal form of $A$ and describe
geometrically the motion of this normal form on the plane.
\item[(c)]  Using {\sf map} describe geometrically how $A$ maps vectors in 
the plane to vectors in the plane. 
\end{itemize}
\end{exercise}


\noindent In Exercises~\ref{c10.4.7a} -- \ref{c10.4.7d} find a square real 
matrix $S$ so that $S\inv AS$ is in real block diagonal form and a complex 
square matrix $T$ so that $T\inv AT$ is in complex diagonal form.
\begin{exercise} \label{c10.4.7a}
\begin{equation*}
A = \left(\begin{array}{rrr}
    1 &     2 &     4 \\
    2 &    -4 &    -5\\
    1 &    10 &   -15
\end{array}\right).
\end{equation*}
\end{exercise}
\begin{exercise} \label{c10.4.7b}
\begin{equation*}
A = \left(\begin{array}{rrrr}
  -15.1220 &  12.2195 &  13.6098 &  14.9268 \\
  -28.7805 &  21.8049 &  25.9024 &  28.7317 \\
   60.1951 & -44.9512 & -53.9756 & -60.6829 \\
  -44.5122 &  37.1220 &  43.5610 &  47.2927
\end{array}\right).
\end{equation*}
\end{exercise} 
\begin{exercise} \label{c10.4.7c}
\begin{equation*}
A = \left(\begin{array}{rrrrrr}
    2.2125 &    5.1750 &    8.4250 &   15.0000 &   19.2500 &    0.5125 \\
   -1.9500 &   -3.9000 &   -6.5000 &   -7.4000 &  -12.0000 &   -2.9500\\
    2.2250 &    3.9500 &    6.0500 &    0.9000 &    1.5000 &    1.0250\\
   -0.2000 &   -0.4000 &         0 &    0.1000 &         0 &   -0.2000\\
   -0.7875 &   -0.8250 &   -1.5750 &    1.0000 &    2.2500 &    0.5125\\
    1.7875 &    2.8250 &    4.5750 &         0 &    4.7500 &    5.4875
\end{array}\right).
\end{equation*}
\end{exercise}
\begin{exercise} \label{c10.4.7d}
\begin{equation*}
A = \left(\begin{array}{rrr}
   -12 &    15 &     0\\
     1 &     5 &     2\\
    -5 &     1 &     5
\end{array}\right).
\end{equation*}
\end{exercise}



\section{Multiplicity and Generalized Eigenvectors}  \label{S:MGE}

The difficulty in generalizing the results in the previous two sections to
matrices with multiple eigenvalues stems from the fact that these matrices 
may not have enough (linearly independent) eigenvectors.  In this section we 
present the basic examples of matrices with a deficiency of eigenvectors, as
well as the definitions of algebraic and geometric multiplicity.  These 
matrices will be the building blocks of the Jordan normal form theorem --- 
the theorem that classifies all matrices up to similarity.

\subsubsection*{Deficiency in Eigenvectors for Real Eigenvalues}

An example of deficiency in eigenvectors is given by the following 
$n\times n$ matrix
\begin{equation}  \label{E:JnR}
J_n(\lambda_0)=\left(\begin{array}{cccccc} \lambda_0 & 1 & 0 & \cdots & 0 & 0\\
	0 & \lambda_0 & 1 & \cdots & 0 & 0 \\
	0 & 0 & \lambda_0  & \cdots & 0 & 0\\
	\vdots & \vdots & \vdots & \ddots & \vdots & \vdots\\
	0 & 0 & 0 & \cdots & \lambda_0 & 1 \\
	0 & 0 & 0 & \cdots & 0 & \lambda_0 \end{array}\right)
\end{equation}
where $\lambda_0\in\R$.  Note that $J_n(\lambda_0)$ has all diagonal 
entries equal to $\lambda_0$,
all superdiagonal entries equal to $1$, and all other entries equal 
to $0$. Since $J_n(\lambda_0)$ is upper triangular, all $n$ 
eigenvalues of $J_n(\lambda_0)$ are equal to $\lambda_0$.  However,
$J_n(\lambda_0)$ has only one linearly independent eigenvector.  To 
verify this assertion let 
\[
N = J_n(\lambda_0) - \lambda_0I_n.
\]
Then $v$ is an eigenvector of $J_n(\lambda_0)$ if and only if $Nv=0$.
Therefore, $J_n(\lambda_0)$ has a unique linearly independent 
eigenvector  if 
\begin{lemma}
{\rm nullity}$(N)=1$.
\end{lemma}\index{nullity}

\begin{proof}  In coordinates the equation $Nv=0$ is:
\[
\left(\begin{array}{cccccc} 0 & 1 & 0 & \cdots & 0 & 0\\
	0 & 0 & 1 & \cdots & 0 & 0 \\
	0 & 0 & 0  & \cdots & 0 & 0\\
	\vdots & \vdots & \vdots & \ddots & \vdots & \vdots\\
	0 & 0 & 0 & \cdots & 0 & 1 \\
	0 & 0 & 0 & \cdots & 0 & 0 \end{array}\right)
\left(\begin{array}{l} v_1 \\ v_2 \\ v_3 \\ \vdots \\ v_{n-1} \\v_n
	\end{array}\right) 
= \left(\begin{array}{l} v_2 \\ v_3 \\ v_4 \\ \vdots \\ v_n \\ 0
	\end{array}\right) = 0.
\]
Thus $v_2 = v_3 = \cdots v_n = 0$, and the solutions are all multiples 
of $e_1$.  Therefore, the nullity of $N$ is $1$.  \end{proof}

Note that we can express matrix multiplication by $N$ as
\begin{equation}  \label{e:Ndef}
\begin{array}{rcll} 
Ne_1 & = & 0 & \\
Ne_j & = & e_{j-1} & \quad j=2,\ldots,n .
\end{array}
\end{equation}
Note that \Ref{e:Ndef} implies that $N^n=0$.

The $n\times n$ matrix $N$ motivates the following definitions.  
\begin{Def} \label{D:multiplicities}
Let $\lambda_0$ be an eigenvalue of $A$.  The {\em algebraic multiplicity\/} 
\index{multiplicity!algebraic} of $\lambda_0$ is the number of times 
that $\lambda_0$ appears as a root of the characteristic polynomial 
$p_A(\lambda)$.  The {\em geometric multiplicity\/} 
\index{multiplicity!geometric} of $\lambda_0$ is the number of linearly 
independent eigenvectors of $A$ having eigenvalue equal to $\lambda_0$.
\end{Def}  
Abstractly, the geometric multiplicity is:
\[
{\rm nullity}(A-\lambda_0I_n).
\]

Our previous calculations show that the matrix $J_n(\lambda_0)$
has an eigenvalue $\lambda_0$ with algebraic multiplicity equal
to $n$ and geometric multiplicity equal to $1$.

\begin{lemma} 
The algebraic multiplicity of an eigenvalue is greater than 
or equal to its geometric multiplicity.
\end{lemma}

\begin{proof} For ease of notation we prove this lemma only for real 
eigenvalues, though the proof for complex eigenvalues is similar.  Let 
$A$ be an $n\times n$ matrix and let $\lambda_0$ be a real eigenvalue 
of $A$. Let $k$ be the geometric multiplicity of $\lambda_0$ and let 
$v_1,\ldots,v_k$ be $k$ linearly independent eigenvectors of $A$ with 
eigenvalue $\lambda_0$.   We can extend $\{v_1,\ldots,v_k\}$ to be a basis
${\cal V} = \{v_1,\ldots,v_n\}$ of $\R^n$.  In this basis, the matrix
of $A$ is 
\[
[A]_{\cal V} = \mattwo{\lambda_0 I_k}{(*)}{0}{B}.
\]
The matrices $A$ and $[A]_{\cal V}$ are similar matrices. Therefore,
they have the same 
characteristic polynomials\index{characteristic polynomial}
and the same eigenvalues 
with the same algebraic multiplicities.  It follows from 
Lemma~\ref{L:detblockdiag} that the characteristic polynomial of $A$ is:
\[
p_A(\lambda) = p_{[A]_{\cal V}}(\lambda) = (\lambda-\lambda_0)^kp_B(\lambda).
\]
Hence $\lambda_0$ appears as a root of $p_A(\lambda)$ at least 
$k$ times and the algebraic multiplicity of $\lambda_0$ 
is greater than or equal to $k$.   The same proof works when $\lambda_0$ 
is a complex eigenvalue --- but all vectors chosen must be complex rather
than real.   \end{proof}

\subsubsection*{Deficiency in Eigenvectors with Complex Eigenvalues}

An example of a real matrix with complex conjugate eigenvalues having 
geometric multiplicity less than algebraic multiplicity is the 
$2n \times 2n$ block matrix
\begin{equation} \label{E:JnC}
\widehat{J}_n(\lambda_0)=
\left(\begin{array}{cccccc} B & I_2 & 0 & \cdots & 0 & 0\\
	0 & B & I_2 & \cdots & 0 & 0 \\
	0 & 0 & B  & \cdots & 0 & 0\\
	\vdots & \vdots & \vdots & \ddots & \vdots & \vdots\\
	0 & 0 & 0 & \cdots & B & I_2 \\
	0 & 0 & 0 & \cdots & 0 & B \end{array}\right)
\end{equation}
where $\lambda_0=\sigma+i\tau$ and $B$ is the $2\times 2$ matrix
\[
B = \mattwo{\sigma}{-\tau}{\tau}{\sigma}.
\]

\begin{lemma}
Let $\lambda_0$ be a complex number.  Then the algebraic multiplicity
of the eigenvalue $\lambda_0$ in the $2n\times 2n$ matrix
$\widehat{J}_n(\lambda_0)$ is $n$ and the geometric multiplicity is $1$.
\end{lemma}

\begin{proof} We begin by showing that the eigenvalues of
$J=\widehat{J}_n(\lambda_0)$ are $\lambda_0$ and $\overline{\lambda_0}$, each 
with algebraic multiplicity $n$.  The characteristic polynomial of $J$ is 
$p_J(\lambda)=\det(J-\lambda I_{2n})$.  From Lemma~\ref{L:detblockdiag} of 
Chapter~\ref{C:D&E} and induction, we see that $p_J(\lambda)=p_B(\lambda)^n$. 
Since the eigenvalues of $B$ are $\lambda_0$ and $\overline{\lambda_0}$, we
have proved that the algebraic multiplicity of each of these eigenvalues in 
$J$ is $n$.
   
Next, we compute the eigenvectors of $J$.  Let $Jv=\lambda_0v$ 
and let $v=(v_1,\ldots,v_n)$ where each $v_j\in\C^2$.  Observe that 
$(J-\lambda_0I_{2n})v=0$ if and only if
\begin{eqnarray*}
Qv_1 + v_2 & = & 0 \\
& \vdots & \\
Qv_{n-1} + v_n & = & 0 \\
Qv_n & = & 0,
\end{eqnarray*}
where $Q=B-\lambda_0I_2$.
Using the fact that $\lambda_0=\sigma+i\tau$, it follows that 
\[
Q=B-\lambda_0I_2 = -\tau\mattwo{i}{1}{-1}{i}.
\]
Hence
\[
Q^2 = 2\tau^2i\mattwo{i}{1}{-1}{i}=-2\tau iQ.
\]
Thus
\[
0 = Q^2v_{n-1} + Qv_n
= -2\tau iQv_{n-1},
\]
from which it follows that $Qv_{n-1}+v_n = v_n = 0$.  Similarly, 
$v_2=\cdots=v_{n-1}=0$.  Since there is only one nonzero complex vector 
$v_1$ (up to a complex scalar multiple) satisfying
\[
Qv_1 = 0,
\]
it follows that the geometric multiplicity of $\lambda_0$ in the matrix
$\widehat{J}_n(\lambda_0)$ equals $1$.  \end{proof}

\begin{Def}  \label{D:jordanblock}
The real matrices $J_n(\lambda_0)$ when $\lambda_0\in\R$ and 
$\widehat{J}_n(\lambda_0)$ when $\lambda_0\in\C$ are {\em real Jordan
blocks\/}.  The matrices $J_n(\lambda_0)$ when $\lambda_0\in\C$ are (complex)
{\em Jordan blocks\/}.
\end{Def} \index{Jordan block}

\subsection*{Generalized Eigenvectors and Generalized Eigenspaces}

What happens when $n\times n$ matrices have fewer that $n$ linearly 
independent eigenvectors?  Answer: The matrices gain generalized 
eigenvectors.

\begin{Def}
A vector $v\in\C^n$ is a {\em generalized eigenvector\/} for the 
$n\times n$ matrix $A$ with eigenvalue $\lambda$ if
\begin{equation}  \label{e:geneig}
(A-\lambda I_n)^kv = 0
\end{equation}
for some positive integer $k$. The smallest integer $k$ for which
\Ref{e:geneig} is satisfied is called the {\em index\/}
\index{index} of the generalized eigenvector $v$.
\end{Def}  \index{eigenvector!generalized}
Note: Eigenvectors\index{eigenvector} are generalized eigenvectors with
index equal to $1$.

Let $\lambda_0$ be a real number and let $N=J_n(\lambda_0)-\lambda_0 I_n$.  
Recall that \Ref{e:Ndef} implies that $N^n=0$.  Hence every vector in $\R^n$ 
is a generalized eigenvector for the matrix $J_n(\lambda_0)$.  So 
$J_n(\lambda_0)$ provides a good example of a matrix whose lack of 
eigenvectors (there is only one independent eigenvector) is made up for by 
generalized eigenvectors (there are $n$ independent generalized eigenvectors).

Let $\lambda_0$ be an eigenvalue of the $n\times n$ matrix $A$ and let 
$A_0 = A-\lambda_0 I_n$.  For simplicity, assume that $\lambda_0$ is real.
Note that 
\[
\nulls(A_0)\subset\nulls(A_0^2)\subset\cdots\subset\nulls(A_0^k)\subset\cdots
\subset\R^n.
\]
Therefore, the dimensions of the null spaces are bounded above by $n$ and
there must be a smallest $k$ such that 
\[
\dim\nulls(A_0^k)=\dim\nulls(A_0^{k+1}).
\]
It follows that 
\begin{equation}  \label{E:nullsequal}
\nulls(A_0^k)=\nulls(A_0^{k+1}).
\end{equation}
\begin{lemma}  \label{L:Jordan}
Let $\lambda_0$ be a real eigenvalue of the $n\times n$ matrix $A$ and let 
$A_0 = A-\lambda_0 I_n$.  Let $k$ be the smallest integer for which 
\Ref{E:nullsequal} is valid.  Then 
\[
\nulls(A_0^k)=\nulls(A_0^{k+j})
\]
for every interger $j>0$.
\end{lemma}

\begin{proof} We can prove the lemma by induction on $j$ if we can show that 
\[
\nulls(A_0^{k+1})=\nulls(A_0^{k+2}).
\]
Since $\nulls(A_0^{k+1})\subset\nulls(A_0^{k+2})$, we need to show that
\[
\nulls(A_0^{k+2})\subset\nulls(A_0^{k+1}).
\]
Let $w\in\nulls(A_0^{k+2})$.  It follows that 
\[
A^{k+1}Aw = A^{k+2}w = 0;
\]
so $Aw\in\nulls(A_0^{k+1})=\nulls(A_0^k)$, by \Ref{E:nullsequal}.  Therefore,
\[
A^{k+1}w = A^k(Aw) = 0,
\]
which verifies that $w\in\nulls(A_0^{k+1})$.  \end{proof}

Let $V_{\lambda_0}$ be the set of all generalized eigenvectors of $A$ with 
eigenvalue $\lambda_0$.  Let $k$ be the smallest integer satisfying
\Ref{E:nullsequal}, then Lemma~\ref{L:Jordan} implies that 
\[
V_{\lambda_0}=\nulls(A_0^k)\subset \R^n
\]
is a subspace called the {\em generalized eigenspace\/}
\index{generalized eigenspace} of $A$ associated to the eigenvalue
$\lambda_0$.  It will follow from the Jordan normal form theorem (see
Theorem~\ref{T:Jordan}) that the dimension of $V_{\lambda_0}$ is the 
algebraic multiplicity of $\lambda_0$.


\subsubsection*{An Example of Generalized Eigenvectors}
\index{eigenvector!generalized}

Find the generalized eigenvectors of the $4\times 4$ matrix
\begin{equation*}
A=\left(\begin{array}{rrrr}
  -24 & -58 &  -2 &  -8\\
   15 &  35 &   1 &   4\\
    3 &   5 &   7 &   4\\
    3 &   6 &   0 &   6
\end{array}\right).
\end{equation*}
and their indices.  When finding generalized eigenvectors of a matrix $A$, 
the first two steps are:
\begin{enumerate}
\item[(i)]  Find the eigenvalues of $A$.
\item[(ii)]  Find the eigenvectors of $A$.
\end{enumerate}
After entering $A$ into \Matlab by typing {\tt e13\_3\_6}, we type 
{\tt eig(A)} and find that all of the eigenvalues of $A$ equal $6$.  Without 
additional information, there could be 1,2,3 or 4 linearly independent 
eigenvectors of $A$ corresponding to the eigenvalue $6$.  In \Matlab we 
determine the number of linearly independent eigenvectors by typing 
{\tt null(A-6*eye(4))} and obtaining
\begin{verbatim}
ans =
    0.8892         0
   -0.4446    0.0000
   -0.0262    0.9701
   -0.1046   -0.2425
\end{verbatim}
 
We now know that (numerically) there are two linearly independent
eigenvectors.  The next step is find the number of independent generalized 
eigenvectors of index 2.\index{index}  To complete this calculation, we find 
a basis for the null space of $(A-6I_4)^2$ by typing 
{\tt null((A-6*eye(4))\^{ }2)} obtaining
\begin{verbatim}
ans =
     1     0     0     0
     0     1     0     0
     0     0     1     0
     0     0     0     1
\end{verbatim}
Thus, for this example, all generalized eigenvectors that are not
eigenvectors have index $2$.

\EXER

\TEXER

\noindent In Exercises~\ref{c10.5.1a} -- \ref{c10.5.1d} determine the 
eigenvalues and their geometric and algebraic multiplicities for the 
given matrix.
\begin{exercise} \label{c10.5.1a}
$A = \left(\begin{array}{cccc} 2 & 0 &  0 & 0\\ 0 & 3 & 1 & 0 \\
0 & 0 & 3 & 0 \\ 0 & 0 & 0 & 4 \end{array}\right)$.
\end{exercise}
\begin{exercise} \label{c10.5.1b}
$B = \left(\begin{array}{cccc} 2 & 0 &  0 & 0\\ 0 & 2 & 0 & 0 \\
0 & 0 & 3 & 1 \\ 0 & 0 & 0 & 3 \end{array}\right)$.
\end{exercise}
\begin{exercise} \label{c10.5.1c}
$C = \left(\begin{array}{rrrr} -1 & 1 &  0 & 0\\ 0 & -1 & 0 & 0 \\
0 & 0 & -1 & 0 \\ 0 & 0 & 0 & 1 \end{array}\right)$.
\end{exercise}
\begin{exercise} \label{c10.5.1d}
$D = \left(\begin{array}{rrrr} 2 & -1 &  0 & 0\\ 1 & 2 & 0 & 0 \\
0 & 0 & 2 & 1 \\ 0 & 0 & 0 & 2 \end{array}\right)$.
\end{exercise}

\noindent In Exercises~\ref{c10.5.2a} -- \ref{c10.5.2d} find a basis 
consisting of the eigenvectors for the given matrix supplemented by 
generalized eigenvectors.  Choose the generalized eigenvectors with 
lowest index possible.
\begin{exercise}  \label{c10.5.2a}
$A=\mattwo{1}{-1}{1}{3}$.
\end{exercise}
\begin{exercise} \label{c10.5.2b}
$B=\left(\begin{array}{rrr} -2 & 0 & -2 \\-1 & 1 & -2 \\ 0 & 1 & -1 \end{array}
\right)$.
\end{exercise}
\begin{exercise} \label{c10.5.2c}
$C=\left(\begin{array}{rrr} -6 & 31 & -14 \\-1 & 6 & -2 \\ 0 & 2 & 1\end{array}
\right)$.
\end{exercise}
\begin{exercise} \label{c10.5.2d}
$D=\left(\begin{array}{rrr} 5 & 1 & 0 \\-3 & 1 & 1 \\ -12 & -4 & 0\end{array}
\right)$.
\end{exercise}


\CEXER
\noindent In Exercises~\ref{c10.5.3A} -- \ref{c10.5.3B}, use \Matlab to find 
the eigenvalues and their algebraic and geometric multiplicities for the given 
matrix.
\begin{exercise} \label{c10.5.3A}
\begin{equation*}
A=\left(\begin{array}{rrrr} 2 & 3 & -21 & -3 \\2 & 7 & -41 & -5 \\ 
0 & 1 & -5 & -1 \\ 0 & 0 & 4 & 4 \end{array}
\right).
\end{equation*}
\end{exercise}
\begin{exercise} \label{c10.5.3B}
\begin{equation*}
B=\left(\begin{array}{rrrrr} 179 & -230 & 0 & 10 & -30 \\
144 & -185 & 0 & 8 & -24 \\ 30 & -39 & -1 & 3 & -9 \\ 192 & -245 & 0 & 9 & -30 
\\ 40 & -51 & 0 & 2 & -7\end{array}\right).
\end{equation*}
\end{exercise}


\section{The Jordan Normal Form Theorem}
\label{S:JNF}

The question that we discussed in Sections~\ref{S:RDM} and \ref{S:CSE} is: 
Up to similarity, what is the simplest form that a matrix can have?  
We have seen that if $A$ has real distinct eigenvalues, then $A$ is real 
diagonalizable.  That is, $A$ is similar to a diagonal matrix whose 
diagonal entries are the real eigenvalues of $A$.  Similarly, if $A$ has 
distinct real and complex eigenvalues, then $A$ is complex diagonalizable; 
that is, $A$ is similar either to a diagonal matrix whose diagonal entries 
are the real and complex eigenvalues of $A$ or to a real block diagonal 
matrix.

In this section we address the question of simplest form when a
matrix has multiple eigenvalues.  In much of this discussion we assume
that $A$ is an $n\times n$ matrix with only real eigenvalues.  
Lemma~\ref{L:eigenv-diag} shows that if the eigenvectors of 
$A$ form a basis, then $A$ is diagonalizable. Indeed, for $A$ to be
diagonalizable, there must be a basis of eigenvectors of $A$.  It 
follows that if $A$ is not diagonalizable, then $A$ must have fewer
than $n$ linearly independent 
eigenvectors\index{eigenvector!linearly independent}.  

The prototypical examples of matrices having fewer eigenvectors than 
eigenvalues are the matrices $J_n(\lambda)$ for $\lambda$ real (see
\Ref{E:JnR}) and $\widehat{J}_n(\lambda)$ for $\lambda$ complex (see
\Ref{E:JnC}).
\begin{Def} 
A matrix is in {\em Jordan normal form\/} if it is block diagonal and 
the matrix in each block on the diagonal is a Jordan block, that is, 
$J_\ell(\lambda)$ for some integer $\ell$ and some real or complex number 
$\lambda$.  

A matrix is in {\em real Jordan normal form\/} if it is block diagonal 
and the matrix in each block on the diagonal is a real Jordan block, that is, 
either $J_\ell(\lambda)$ for some integer $\ell$ and some real number 
$\lambda$ or $\widehat{J}_\ell(\lambda)$ for some integer $\ell$ and some 
complex number $\lambda$. 
\end{Def} \index{Jordan block}
\index{Jordan normal form}


The main theorem about Jordan normal form is:
\begin{thm}[Jordan normal form] \label{T:Jordan}
Let $A$ be an $n\times n$ matrix.  Then $A$ is 
similar\index{similar} to a Jordan normal
form matrix and to a real Jordan normal form matrix.
\end{thm}

This theorem is proved by constructing a basis ${\cal V}$ for $\R^n$ so 
that the matrix $S\inv AS$ is in Jordan normal form, where $S$ is the
matrix whose columns consists of vectors in ${\cal V}$.  The algorithm for 
finding the basis ${\cal V}$ is complicated and is found in 
Appendix~\ref{S:Jordan}.  In this section we construct ${\cal V}$ only in the 
special and simpler case where each eigenvalue of $A$ is real and is 
associated with exactly one Jordan block.

More precisely, let $\lambda_1,\ldots,\lambda_s$ be the distinct 
eigenvalues of $A$ and let 
\[
A_j = A-\lambda_jI_n.
\]   
The eigenvectors corresponding to $\lambda_j$ are the vectors in the
null space\index{null space} of $A_j$ and the 
generalized eigenvectors are the vectors in 
the null space of $A_j^k$ for some $k$.  The dimension of the null space 
of $A_j$ is precisely the number of Jordan blocks\index{Jordan block} 
of $A$ associated to 
the eigenvalue $\lambda_j$.  So the assumption that we make here is 
\[
{\rm nullity}(A_j) = 1
\]
for $j = 1,\ldots,s$.

Let $k_j$ be the integer whose existence is specified by
Lemma~\ref{L:Jordan}.  Since, by assumption, there is only one Jordan block
associated with the eigenvalue $\lambda_j$, it follows that $k_j$ is the
algebraic multiplicity of the eigenvalue $\lambda_j$.

To find a basis in which the matrix $A$ is in Jordan normal 
form\index{Jordan normal form!basis for}, we proceed as follows.  First, let 
$w_{jk_j}$ be a vector in 
\[
\nulls(A_j^{k_j})\setmin\nulls(A_j^{k_j-1}).  
\]  
Define the vectors $w_{ji}$ by 
\begin{eqnarray*}
w_{j,k_j-1} & = & A_jw_{j,k_j} \\
& \vdots &  \\
w_{j,1} & = & A_jw_{j,2}.
\end{eqnarray*}
Second, when $\lambda_j$ is real, let the $k_j$ vectors $v_{ji}=w_{ji}$, and
when $\lambda_j$ is complex, let the $2k_j$ vectors $v_{ji}$ be defined by 
\begin{eqnarray*}
v_{j,2i-1} & = & \RE(w_{ji})\\
v_{j,2i} & = & \IM(w_{ji}). 
\end{eqnarray*}
Let ${\cal V}$ be the set of vectors $v_{ji}\in\R^n$.   We will show in
Appendix~\ref{S:Jordan} that the set ${\cal V}$ consists of $n$ vectors and 
is a basis of $\R^n$.  Let $S$ be the matrix whose columns are the
vectors in ${\cal V}$.  Then $S\inv AS$ is in Jordan normal form.

\subsubsection*{The Cayley Hamilton Theorem}

As a corollary of the Jordan normal form theorem, we prove the Cayley 
Hamilton theorem which states that a {\em square matrix satisfies its 
characteristic polynomial\/}.  More precisely:
\begin{thm}[Cayley Hamilton] \label{T:CH} \index{Cayley Hamilton theorem}
Let $A$ be a square matrix\index{matrix!square} and let $p_A(\lambda)$ be 
its characteristic polynomial\index{characteristic polynomial}.  Then
\[
p_A(A) = 0.
\]
\end{thm}

\begin{proof}  Let $A$ be an $n\times n$ matrix.  The characteristic polynomial of 
$A$ is
\[
p_A(\lambda)=\det(A-\lambda I_n).
\]
Suppose that $B=P\inv AP$ is a matrix similar to $A$.  
Theorem~\ref{T:similareigen} states that $p_B=p_A$.  Therefore
\[
p_B(B)= p_A(P\inv AP) = P\inv p_A(A)P.
\]
So if the Cayley Hamilton theorem holds for a matrix similar to $A$, 
then it is valid for the matrix $A$.  Moreover, using the Jordan normal form 
theorem, we may assume that $A$ is in Jordan normal form.  

Suppose that $A$ is block diagonal, that is 
\[
A = \mattwo{A_1}{0}{0}{A_2},
\]
where $A_1$ and $A_2$ are square matrices.  Then 
\[
p_A(\lambda) = p_{A_1}(\lambda)p_{A_2}(\lambda).
\]
This observation follows directly from Lemma~\ref{L:detblockdiag}.  Since
\[
A^k = \mattwoc{A_1^k}{0}{0}{A_2^k},
\]
it follows that 
\[
p_A(A) = \mattwoc{p_A(A_1)}{0}{0}{p_A(A_2)}
 = \mattwoc{p_{A_1}(A_1)p_{A_2}(A_1)}{0}{0}{p_{A_1}(A_2)p_{A_2}(A_2)}.
\]
It now follows from this calculation that if the Cayley Hamilton theorem is 
valid for Jordan blocks, then $p_{A_1}(A_1)=0=p_{A_2}(A_2)$.  So $p_A(A)=0$ 
and the Cayley Hamilton theorem is valid for all matrices.

A direct calculation shows that Jordan blocks satisfy the 
Cayley Hamilton theorem.  To begin, suppose that the eigenvalue of the 
Jordan block is real.  Note that the characteristic polynomial of 
the Jordan block $J_n(\lambda_0)$ in \Ref{E:JnR} is $(\lambda-\lambda_0)^n$.
Indeed, $J_n(\lambda_0)-\lambda_0I_n$ is strictly upper triangular and 
$(J_n(\lambda_0)-\lambda_0I_n)^n=0$.  If $\lambda_0$ is complex, then either
repeat this calculation using the complex Jordan form or show by direct 
calculation that $(A-\lambda_0I_n)(A-\overline{\lambda_0}I_n)$ is strictly 
upper triangular when $A=\widehat{J}_n(\lambda_0)$ is the real Jordan form of 
the Jordan block in \Ref{E:JnC}.  \end{proof}




\subsection*{An Example}

Consider the $4\times 4$ matrix
\begin{equation*} \label{e:Aexamp}
A=\left(\begin{array}{rrrr}      -147  &  -106      &   -66     &   -488\\
         604   &      432      &   271     &   1992\\
         621   &      448       &  279     &   2063\\
        -169    &    -122      &   -76     &   -562\end{array}\right).
\end{equation*}
Using \Matlab we can compute the characteristic polynomial of $A$
by typing\index{\computer!poly}
\begin{verbatim}
poly(A)
\end{verbatim}
The output is
\begin{verbatim}
ans =
     1.0000   -2.0000  -15.0000   -0.0000   -0.0000
\end{verbatim}
Note that since $A$ is a matrix of integers we know that the coefficients 
of the characteristic polynomial of $A$ must be integers.   Thus the 
characteristic polynomial is exactly:
\[
p_A(\lambda) = \lambda^4-2\lambda^3-15\lambda^2 =
	\lambda^2(\lambda-5)(\lambda+3).
\]
So $\lambda_1=0$ is an eigenvalue of $A$ with 
algebraic multiplicity\index{multiplicity!algebraic} two
and $\lambda_2=5$ and $\lambda_3=-3$ are simple eigenvalues of 
multiplicity one.

We can find eigenvectors of $A$ corresponding to the simple
eigenvalues by typing
\begin{verbatim}
v2 = null(A-5*eye(4));
v3 = null(A+3*eye(4));
\end{verbatim}
At this stage we do not know how many linearly independent
eigenvectors have eigenvalue $0$.  There are either one or two linearly
independent eigenvectors and we determine which by typing {\tt null(A)} 
and obtaining
\begin{verbatim}
ans =
  -0.1818
   0.6365
   0.7273
  -0.1818
\end{verbatim}
So \Matlab tells us that there is just one linearly independent
eigenvector having $0$ as an eigenvalue.  There must be a generalized
eigenvector\index{eigenvector!generalized}
in $V_0$.  Indeed, the null space of $A^2$ is two dimensional
and this fact can be checked by typing
\begin{verbatim}
null2 = null(A^2)
\end{verbatim}
obtaining
\begin{verbatim}
null2 =
    0.2193   -0.2236
   -0.5149   -0.8216
   -0.8139    0.4935
    0.1561    0.1774
\end{verbatim}
Choose one of these vectors, say the first vector, to be $v_{12}$ by typing
\begin{verbatim}
v12 = null2(:,1);
\end{verbatim}
Since the algebraic multiplicity\index{multiplicity!algebraic}
of the eigenvalue $0$ is two, we choose the 
fourth basis vector be $v_{11}=Av_{12}$.  In \Matlab we type 
\begin{verbatim}
v11 = A*v12
\end{verbatim}
obtaining
\begin{verbatim}
v11 =
   -0.1263
    0.4420
    0.5051
   -0.1263
\end{verbatim}
Since {\tt v11} is nonzero, we have found a basis for $V_0$.
We can now put the matrix $A$ in Jordan normal 
form\index{Jordan normal form}
by setting 
\begin{verbatim}
S = [v11 v12 v2 v3];
J = inv(S)*A*S
\end{verbatim}
to obtain
\begin{verbatim}
J = 
   -0.0000    1.0000    0.0000   -0.0000
    0.0000    0.0000    0.0000   -0.0000
   -0.0000   -0.0000    5.0000    0.0000
    0.0000   -0.0000   -0.0000   -3.0000
\end{verbatim}

We have only discussed a Jordan normal form example when the eigenvalues
are real and multiple.  The case when the eigenvalues are complex and 
multiple\index{eigenvalue!complex!multiple} first occurs when $n=4$. 
A sample complex Jordan block when the 
matrix has algebraic multiplicity two eigenvalues $\sigma\pm i\tau$ of 
geometric multiplicity one is
\[
\left(\begin{array}{rrrr} \sigma & -\tau & 1 & 0 \\ 
\tau & \sigma & 0 & 1 \\ 0 & 0 & \sigma & -\tau \\
0 & 0 & \tau & \sigma \end{array}\right).
\]

\subsection*{Numerical Difficulties}

When a matrix has multiple eigenvalues, then numerical difficulties
can arise when using the \Matlab command {\tt eig(A)}, as we now explain.
\index{\computer!eig}

Let $p(\lambda)=\lambda^2$.  Solving $p(\lambda)=0$ is very easy --- in theory
--- as $\lambda=0$ is a double root of $p$.  Suppose, however, that 
we want to solve $p(\lambda)=0$ numerically.  Then, numerical errors 
will lead to solving the equation  
\[
\lambda^2 = \epsilon
\]
where $\epsilon$ is a small number.  Note that if $\epsilon>0$, the 
solutions are $\pm\sqrt{\epsilon}$; while if $\epsilon<0$, the solutions 
are $\pm i\sqrt{|\epsilon|}$.  Since numerical errors are machine 
dependent, $\epsilon$ can be of either sign.  The numerical process of
finding double roots of a characteristic polynomial (that is, double 
eigenvalues of a matrix) is similar to numerically solving the equation 
$\lambda^2=0$, as we shall see.

For example, on a
{\em Sun SPARCstation 10\/} using \Matlab version 4.2c, the eigenvalues 
of the $4\times 4$ matrix $A$ in \Ref{e:Aexamp} (in {\tt format long}) 
obtained using {\tt eig(A)} are:
\begin{verbatim}
ans = 
  5.00000000001021                    
 -0.00000000000007 + 0.00000023858927i
 -0.00000000000007 - 0.00000023858927i
 -3.00000000000993       
\end{verbatim}
That is, \Matlab computes two complex conjugate eigenvalues
\[
\pm 0.00000023858927i
\]
which corresponds to an $\epsilon$ of {\tt -5.692483975913288e-14}.
On a {\em IBM\/} compatible $486$ computer using \Matlab version 4.2
the same computation yields eigenvalues
\begin{verbatim}
ans=
 4.99999999999164
 0.00000057761008
-0.00000057760735
-2.99999999999434
\end{verbatim}
That is, on this computer \Matlab computes two real, near zero, 
eigenvalues 
\[ 
\pm 0.00000057761
\]
that corresponds to an $\epsilon$ of {\tt 3.336333121e-13}. These
errors are within round off error\index{round off error} in 
double precision\index{double precision} computation.

A consequence of these kinds of error, however, is that when a matrix
has multiple eigenvalues, we cannot use the command {\tt [V,D] = eig(A)} 
with confidence. On the {\em Sun SPARCstation\/}, this command yields 
a matrix 
\begin{verbatim}
V = 
  -0.1652             0.0000 - 0.1818i   0.0000 + 0.1818i  -0.1642          
   0.6726            -0.0001 + 0.6364i  -0.0001 - 0.6364i   0.6704          
   0.6962            -0.0001 + 0.7273i  -0.0001 - 0.7273i   0.6978          
  -0.1888             0.0000 - 0.1818i   0.0000 + 0.1818i  -0.1915          
\end{verbatim}
that suggests that $A$ has two complex eigenvectors corresponding 
to the `complex' pair of near zero eigenvalues.  The {\em IBM\/} 
compatible yields the matrix
\begin{verbatim}
V = 
   -0.1652    0.1818   -0.1818   -0.1642
    0.6726   -0.6364    0.6364    0.6704
    0.6962   -0.7273    0.7273    0.6978
   -0.1888    0.1818   -0.1818   -0.1915
\end{verbatim}
indicating that \Matlab has found two real eigenvectors corresponding 
to the near zero real eigenvalues.  Note that the two eigenvectors
corresponding to the eigenvalues $5$ and $-3$ are correct on both 
computers.     

\EXER

\TEXER


\begin{exercise} \label{c10.5.2}
Write two different $4\times 4$ Jordan normal form matrices all
of whose eigenvalues equal $2$ for which the geometric
multiplicity is two.
\end{exercise}

\begin{exercise} \label{c10.5.2A}
How many different $6\times 6$ Jordan form matrices have all eigenvalues 
equal to $3$? (We say that two Jordan form matrices are the same 
if they have the same number and type of Jordan block, though not
necessarily in the same order along the diagonal.)
\end{exercise}

\begin{exercise}  \label{c10.5.2B}
A $5\times 5$ matrix $A$ has three eigenvalues equal
to $4$ and two eigenvalues equal to $-3$.  List the possible Jordan normal 
forms for $A$ (up to similarity).  Suppose that you can ask your computer to
compute the nullity of precisely two matrices.  Can you devise a strategy for
determining the Jordan normal form of $A$?  Explain your answer.
\end{exercise}

\begin{exercise}  \label{c10.5.2C}
An $8\times 8$ real matrix $A$ has three eigenvalues equal to $2$, two 
eigenvalues equal to $1+i$, and one zero eigenvalue.  List the possible 
Jordan normal forms for $A$ (up to similarity).  Suppose that you can ask 
your computer to compute the nullity of precisely two matrices.  Can you 
devise a strategy for determining the Jordan normal form of $A$?  Explain 
your answer.
\end{exercise}

\noindent In Exercises~\ref{c10.5.3a} -- \ref{c10.5.4c} find the 
Jordan normal forms for the given matrix.
\begin{exercise} \label{c10.5.3a}
$A = \mattwo{2}{4}{1}{1}$. 
\end{exercise}
\begin{exercise} \label{c10.5.3b}
$B = \mattwo{9}{25}{-4}{-11}$.
\end{exercise}
\begin{exercise} \label{c10.5.4}
$C = \left(\begin{array}{rrr} -5 & -8 & -9 \\  5 & 9 & 9 \\
 -1 & -2 & -1 \end{array}\right)$.
\end{exercise}
\begin{exercise} \label{c10.5.4a}
$D = \left(\begin{array}{rrr} 0 & 1 & 0 \\  0 & 0 & 1 \\
1 & 1 & -1 \end{array}\right)$.
\end{exercise}
\begin{exercise} \label{c10.5.4b}
$E = \left(\begin{array}{rrr} 2 & 0 & -1 \\  2 & 1 & -1 \\
1 & 0 & 0 \end{array}\right)$.
\end{exercise}
\begin{exercise} \label{c10.5.4c}
$F = \left(\begin{array}{rrr} 3 & -1 & 2 \\  -1 & 2 & -1 \\
-1 & 1 & 0 \end{array}\right)$.
\end{exercise}

\begin{exercise}  \label{c10.5.5A}
Compute $e^{tJ}$ where $J=\left(\begin{array}{rrr} 2 & 0 & 0 \\  0 & -1 & 1 \\
0 & 0 & -1 \end{array}\right)$.
\end{exercise}

\begin{exercise}  \label{c10.5.5B}
Compute $e^{tJ}$ where $J=\left(\begin{array}{rrrrr} 2 & 1 & 0 & 0 & 0\\  
0 & 2 & 0 & 0 & 0 \\ 0 & 0 & 3 & 1 & 0\\ 0 & 0 & 0 & 3 & 1\\
0 & 0 & 0 & 0 & 3 \end{array}\right)$.
\end{exercise}

\begin{exercise} \label{c10.5.5}
An $n\times n$ matrix $N$ is 
{\em nilpotent\/}\index{nilpotent}\index{matrix!nilpotent} 
if $N^k=0$ for some positive integer $k$.
\begin{itemize}
\item[(a)]  Show that the matrix $N$ defined in \Ref{e:Ndef} is nilpotent.
\item[(b)]  Show that all eigenvalues of a nilpotent matrix equal zero.
\item[(c)]  Show that any matrix similar to a nilpotent matrix is also 
	nilpotent.
\item[(d)]  Let $N$ be a matrix all of whose eigenvalues are zero.  Use 
	the Jordan normal form theorem to show that $N$ is nilpotent.
\end{itemize}
\end{exercise}

\begin{exercise} \label{c10.5.5C}
Let $A$ be a $3\times 3$ matrix.  Use the Cayley-Hamilton theorem to show that
$A\inv$ is a linear combination of $I_3,A,A^2$.  That is, there exist real 
scalars $a,b,c$ such that 
\[
A\inv = aI_3 + bA + cA^2.
\]
\end{exercise}

\CEXER

\noindent In Exercises~\ref{E:jnfma} -- \ref{E:jnfme}, (a) determine the real
Jordan normal form for the given matrix $A$, and (b) find the matrix $S$ so 
that $S\inv AS$ is in real Jordan normal form.  
\begin{exercise}  \label{E:jnfma}
\begin{equation*}
A = \left(\begin{array}{rrrr} -3 & -4 & -2 & 0\\
-9 & -39 & -16 & -7\\ 18 & 64 & 27 & 10 \\ 15 & 86 & 34 & 18
\end{array}\right). 
\end{equation*}
\end{exercise}
\begin{exercise}  \label{E:jnfmb}
\begin{equation*}
A =\left(\begin{array}{rrrr} 9 & 45 & 18 & 8\\
0 & -4 & -1 & -1\\ -16 & -69 & -29 & -12 \\ 25 & 123 & 49 & 23
\end{array}\right). 
\end{equation*}
\end{exercise}
\begin{exercise}  \label{E:jnfmc}
\begin{equation*}
A = \left(\begin{array}{rrrr} -5 & -13 & 17 & 42\\
-10 & -57 & 66 & 187\\ -4 & -23 & 26 & 77 \\ -1 & -9 & 9 & 32
\end{array}\right).  
\end{equation*}
\end{exercise}
\begin{exercise}  \label{E:jnfmd}
\begin{equation*}
A = \left(\begin{array}{rrrr} 1 & 0 & -9 & 18 \\
12 & -7 & -26 & 77\\ 5 & -2 & -13 & 32 \\ 2 & -1 & -4 & 11
\end{array}\right). 
\end{equation*}
\end{exercise}
\begin{exercise} \label{E:jnfme}
\begin{equation*}
A = \left(\begin{array}{rrrr} 
    -1  &  -1  &   1   &  0\\
    -3  &   1  &   1   &  0\\
    -3  &   2  &  -1   &  1\\
    -3  &   2  &   0   &  0
 \end{array}\right). 
\end{equation*}
\end{exercise}
\begin{exercise} \label{E:jnfmf}
\begin{equation*}
A = \left(\begin{array}{rrrrr} 
     0   &  0  &  -1  &   2   &  2\\
     1   & -2  &   0   &  2   &  2\\
     1   & -1  &  -1   &  2   &  2\\
     0   &  0  &   0   &  1   &  2\\
     0   &  0  &   0   & -1   &  3
 \end{array}\right). 
\end{equation*}
\end{exercise}



\section{Markov Matrix Theory}
\label{S:TransitionTheory} \index{matrix!Markov}\index{matrix!Markov}

In this appendix we use the Jordan normal form theorem to study the asymptotic
dynamics of transition matrices\index{matrix!transition} such as those of 
Markov chains\index{Markov chain} introduced in 
Section~\ref{S:TransitionApplied}.

The basic result is the following theorem.
\begin{thm} \label{T:convergeto0}
Let $A$ be an $n\times n$ matrix and assume that all eigenvalues $\lambda$ of 
$A$ satisfy $|\lambda|<1$.  Then for every vector $v_0\in\R^n$
\begin{equation}  \label{E:convergeto0}
\lim_{k\to\infty} A^kv_0 = 0.
\end{equation}
\end{thm}

\begin{proof}  Suppose that $A$ and $B$ are similar matrices; that is, $B=SAS\inv$
for some invertible matrix $S$.  Then $B^k = SA^kS\inv$ and for any vector 
$v_0\in\R^n$ \Ref{E:convergeto0} is valid if and only if
\[
\lim_{k\to\infty} B^kv_0 = 0.
\]
Thus, when proving this theorem, we may assume that $A$ is in Jordan normal
form.

Suppose that $A$ is in block diagonal form; that is, suppose
\[
A=\mattwo{C}{0}{0}{D},
\]
where $C$ is an $\ell\times\ell$ matrix and $D$ is a $(n-\ell)\times (n-\ell)$
matrix.  Then 
\[
A^k = \mattwo{C^k}{0}{0}{D^k}.
\]
So for every vector $v_0=(w_0,u_0)\in\R^\ell\times\R^{n-\ell}$ 
\Ref{E:convergeto0} is valid if and only if 
\[
\lim_{k\to\infty} C^kv_0 = 0 \AND \lim_{k\to\infty} D^kv_0 = 0.
\]
So, when proving this theorem, we may assume that $A$ is a Jordan block.

Consider the case of a simple Jordan block\index{Jordan block}.  
Suppose that $n=1$ and that 
$A=(\lambda)$ where $\lambda$ is either real or complex.  Then 
\[
A^kv_0 = \lambda^kv_0.
\]
It follows that \Ref{E:convergeto0} is valid precisely when $|\lambda|<1$.  
Next, suppose that $A$ is a nontrivial Jordan block.  For example, let 
\[
A=\mattwo{\lambda}{1}{0}{\lambda} = \lambda I_2+N
\]
where $N^2=0$.  It follows by induction that 
\[
A^kv_0 = \lambda^kv_0 + k\lambda^{k-1}Nv_0 = \lambda^kv_0 + 
k\lambda^k\frac{1}{\lambda}Nv_0.
\]
Thus \Ref{E:convergeto0} is valid precisely when $|\lambda|<1$.  The reason for this convergence is as follows.  The first term converges to $0$ as 
before but the second term is the product of three terms $k$, $\lambda^k$,
and $\frac{1}{\lambda}Nv_0$.  The first increases to infinity, the second
decreases to zero, and the third is constant independent of $k$.  In fact,
geometric decay\index{geometric decay} 
($\lambda^k$, when $|\lambda|<1$) always beats 
polynomial growth\index{polynomial growth}.  Indeed,
\begin{equation}  \label{E:PG}
\lim_{m\to\infty}m^j\lambda^m = 0
\end{equation}
for any integer $j$.  This fact can be proved using l'H\^{o}spital's rule 
and induction.

So we see that when $A$ has a nontrivial Jordan block, convergence is 
subtler than when $A$ has only simple Jordan blocks, as initially the 
vectors $Av_0$ grow in magnitude.  For example, suppose that $\lambda=0.75$ 
and $v_0=(1,0)^t$.  Then $A^8v_0 = (0.901,0.075)^t$ is the first vector in 
the sequence $A^kv_0$ whose norm is less than $1$; that is, $A^8v_0$ is the 
first vector in the sequence closer to the origin than $v_0$.  

It is also true that \Ref{E:convergeto0} is valid for any Jordan block 
$A$ and for all $v_0$ precisely when $|\lambda|<1$.  To verify this fact 
we use the binomial theorem\index{binomial theorem}.  
We can write a nontrivial Jordan block as
$\lambda I_n+N$ where $N^{k+1}=0$ for some integer $k$.  We just discussed 
the case $k=1$.  In this case
\[
(\lambda I_n+N)^m = \lambda^mI_n+m\lambda^{m-1}N + 
\left(\begin{array}{@{}c@{}} m\\2\end{array}\right)\lambda^{m-2}N^2
+\cdots+
\left(\begin{array}{@{}c@{}} m\\k \end{array}\right)\lambda^{m-k}N^k,
\]
where
\[
\left(\begin{array}{@{}c@{}} m \\ j\end{array}\right)
 = \frac{m!}{j!(m-j)!}=\frac{m(m-1)\cdots(m-j+1)}{j!}.
\]
To verify that 
\[
\lim_{m\to\infty}(\lambda I_n+N)^m = 0
\]
we need only verify that each term
\[
\lim_{m\to\infty}\left(\begin{array}{@{}c@{}} m\\j \end{array}\right)
\lambda^{m-j}N^j = 0
\]
Such terms are the product of three terms 
\[
m(m-1)\cdots(m-j+1) \AND \lambda^m \AND \frac{1}{j!\lambda^j}N^j.
\]
The first term has polynomial growth to infinity dominated by $m^j$, the 
second term decreases to zero geometrically, and the third term is 
constant independent of $m$. The desired convergence to zero follows from 
\Ref{E:PG}.  \end{proof}

\begin{Def}  The $n\times n$ matrix $A$ has a {\em dominant\/} eigenvalue 
$\lambda_0>0$ if $\lambda_0$ is a simple eigenvalue and all other eigenvalues
$\lambda$ of $A$ satisfy $|\lambda|<\lambda_0$.
\end{Def}\index{eigenvalue!dominant}


\begin{thm}  \label{T:Markovdom}
Let $P$ be a Markov matrix. Then $1$ is a dominant eigenvalue of $P$.
\end{thm}\index{matrix!Markov}\index{matrix!Markov}

\begin{proof}  Recall from Chapter~\ref{chap:matrices}, Definition~\ref{D:Markov} 
that a Markov matrix is a square matrix $P$ whose entries are nonnegative, 
whose rows sum to $1$, and for which a power $P^k$ that has all positive 
entries.  To prove this theorem we must show that all eigenvalues $\lambda$ 
of $P$ satisfy $|\lambda|\leq 1$ and that $1$ is a simple eigenvalue of $P$.

Let $\lambda$ be an eigenvalue of $P$ and let $v=(v_1,\ldots,v_n)^t$ be an 
eigenvector corresponding to the eigenvalue $\lambda$.  We prove that 
$|\lambda|\leq 1$.  Choose $j$ so that $|v_j|\ge|v_i|$ for all $i$.  Since 
$Pv=\lambda v$, we can equate the $j^{th}$ coordinates of both sides of 
this equality, obtaining
\[
p_{j1}v_1 + \cdots + p_{jn}v_n = \lambda v_j.
\]
Therefore,
\[
 |\lambda| |v_j| = |p_{j1}v_1 + \cdots + p_{jn}v_n| \leq 
p_{j1}|v_1| + \cdots + p_{jn}|v_n|,
\]
since the $p_{ij}$ are nonnegative.  It follows that 
\[
|\lambda| |v_j| \leq (p_{j1}+\cdots+p_{jn})|v_j| =|v_j|,
\]
since $|v_i|\le|v_j|$ and rows of $P$ sum to $1$.  Since $|v_j|>0$, it 
follows that $\lambda\leq 1$.

Next we show that $1$ is a simple eigenvalue of $P$.  Recall, or just 
calculate directly, that the vector $(1,\ldots,1)^t$ is an eigenvector of $P$ 
with eigenvalue $1$.  Now let $v=(v_1,\ldots,v_n)^t$ be an eigenvector of $P$ 
with eigenvalue $1$.  Let $Q=P^k$ so that all entries of $Q$ are positive. 
Observe that $v$ is an eigenvector of $Q$ with eigenvalue $1$, and hence that 
all rows of $Q$ also sum to $1$.

To show that $1$ is a simple eigenvalue of $Q$, and therefore of $P$, we must 
show that all coordinates of $v$ are equal.  Using the previous estimates 
(with $\lambda=1$), we obtain 
\begin{equation}  \label{E:ineqM}
|v_j|= |q_{j1}v_1 + \cdots + q_{jn}v_n| \leq  q_{j1}|v_1| + \cdots + 
q_{jn}|v_n| \leq |v_j|.
\end{equation}
Hence 
\[
|q_{j1}v_1 + \cdots + q_{jn}v_n| =  q_{j1}|v_1| + \cdots + q_{jn}|v_n|.
\]
This equality is valid only if all of the $v_i$ are nonnegative or all are 
nonpositive.  Without loss of generality, we assume that all $v_i\geq 0$.
It follows from \Ref{E:ineqM} that 
\[
v_j= q_{j1}v_1 + \cdots + q_{jn}v_n.
\]
Since $q_{ji}>0$, this inequality can hold only if all of the $v_i$ are
equal.  \end{proof}


\begin{thm} \label{T:convergetoeig}
(a)  Let $Q$ be an $n\times n$ matrix with dominant eigenvalue 
$\lambda>0$ and associated eigenvector $v$.  Let $v_0$ be any vector in 
$\R^n$.  Then
\[
\lim_{k\to\infty}\frac{1}{\lambda^k}Q^kv_0 = cv,
\]
for some scalar $c$.

(b)  Let $P$ be a Markov matrix and $v_0$ a nonzero vector in $\R^n$
with all entries nonnegative.  Then 
\[
\lim_{k\to\infty}(P^t)^kv_0 = V
\]
where $V$ is the eigenvector of $P^t$ with eigenvalue $1$ such that the 
sum of the entries in $V$ is equal to the sum of the entries in $v_0$.
\end{thm} \index{matrix!Markov}\index{matrix!Markov}

\begin{proof}  (a) After a similarity transformation, if needed, 
we can assume that $Q$ is in 
Jordan normal form.  More precisely, we can assume that 
\[
\frac{1}{\lambda}Q = \mattwo{1}{0}{0}{A}
\]
where $A$ is an $(n-1)\times (n-1)$ matrix with all eigenvalues $\mu$
satisfying $|\mu|<1$.  Suppose $v_0=(c_0,w_0)\in\R\times\R^{n-1}$.  It 
follows from Theorem~\ref{T:convergeto0} that 
\[
\lim_{k\to\infty}\frac{1}{\lambda^k}Q^kv_0 = 
\lim_{k\to\infty}(\frac{1}{\lambda}Q)^kv_0 =
\lim_{k\to\infty}\mattwo{c_0}{0}{0}{A^kw_0} = c_0e_1.
\]
Since $e_1$ is the eigenvector of $Q$ with eigenvalue $\lambda$ Part (a) 
is proved.

(b)   Theorem~\ref{T:Markovdom} states that a Markov matrix has a dominant 
eigenvalue equal to $1$.  The Jordan normal form theorem implies that the 
eigenvalues of $P^t$ are equal to the eigenvalues of $P$ with the same 
algebraic and geometric multiplicities.  It follows that $1$ is also a 
dominant eigenvalue of $P^t$.  It follows from Part (a) that
\[
\lim_{k\to\infty}(P^t)^kv_0 = cV
\]
for some scalar $c$.  But Theorem~\ref{T:Markov} in 
Chapter~\ref{chap:matrices} implies that the sum of the
entries in $v_0$ equals the sum of the entries in $cV$ which, by assumption
equals the sum of the entries in $V$.  Thus, $c=1$.   \end{proof}





\EXER

\TEXER

\begin{exercise} \label{c10.6.1}
Let $A$ be an $n\times n$ matrix.   Suppose that 
\[
\lim_{k\to\infty} A^kv_0 = 0.
\]
for every vector $v_0\in\R^n$.  Then the eigenvalues $\lambda$ of $A$ all
satisfy $|\lambda|<1$.
\end{exercise}






\section{Proof of Jordan Normal Form}
\label{S:Jordan} \index{Jordan normal form}

We prove the Jordan normal form theorem under the assumption that the 
eigenvalues of $A$ are all real.  The proof for matrices having both real and 
complex eigenvalues proceeds along similar lines.

Let $A$ be an $n\times n$ matrix, let $\lambda_1,\ldots,\lambda_s$ be the
distinct eigenvalues of $A$, and let $A_j = A-\lambda_jI_n$.

\begin{lemma}  \label{L:commute}
The linear mappings $A_i$ and $A_j$ commute.
\end{lemma}

\begin{proof} Just compute
\[
A_iA_j = (A-\lambda_iI_n)(A-\lambda_jI_n)= A^2-\lambda_iA-\lambda_jA+
\lambda_i\lambda_jI_n,
\]
and
\[
A_jA_i = (A-\lambda_jI_n)(A-\lambda_iI_n)= A^2-\lambda_jA-\lambda_iA+
\lambda_j\lambda_iI_n.
\]
So $A_iA_j=A_jA_i$, as claimed.  \end{proof}

Let $V_j$ be the generalized eigenspace corresponding to eigenvalue 
$\lambda_j$. 

\begin{lemma}  \label{L:Ajinvertible}
$A_i:V_j\to V_j$ is invertible when $i\neq j$.
\end{lemma}

\begin{proof}  Recall from Lemma~\ref{L:Jordan} that $V_j=\nulls(A_j^k)$ for some 
$k\ge 1$.  Suppose that $v\in V_j$.  We first verify that $A_iv$ is also in 
$V_j$.  Using Lemma~\ref{L:commute}, just compute 
\[
A_j^kA_iv = A_iA_j^kv = A_i0 = 0.
\]
Therefore, $A_iv\in\nulls(A_j^k)=V_j$.
 
Let $B$ be the linear mapping $A_i|V_j$.  It follows from
Chapter~\ref{C:LMCC}, Theorem~\ref{T:nsr} that
\[
{\rm nullity}(B) +\dim{\rm range}(B) = \dim(V_j).
\]
Now $w\in\nulls(B)$ if $w\in V_j$ and $A_iw=0$.  Since
$A_iw = (A-\lambda_iI_n)w = 0$, it follows that $Aw = \lambda_iw$.  Hence 
\[
A_jw = (A-\lambda_jI_n)w = (\lambda_i-\lambda_j)w
\]
and
\[
A_j^kw = (\lambda_i-\lambda_j)^kw.
\]
Since $\lambda_i\neq\lambda_j$, it follows that $A_j^kw=0$ only when $w=0$.
Hence the nullity of $B$ is zero.  We conclude that
\[
\dim{\rm range}(B) = \dim(V_j).
\]
Thus, $B$ is invertible, since the domain and range of $B$ are the same
space.  \end{proof}

\begin{lemma}  \label{L:independentVj}
Nonzero vectors taken from different generalized eigenspaces $V_j$ are 
linearly independent.  More precisely, if $w_j\in V_j$ and 
\[
w = w_1 + \cdots + w_s = 0,
\]
then $w_j=0$.  
\end{lemma}

\begin{proof} Let $V_j=\nulls(A_j^{k_j})$ for some integer $k_j$.  Let
$C=A_2^{k_2}\compose\cdots\compose A_s^{k_s}$. Then 
\[
0 = Cw = Cw_1,
\]
since $A_j^{k_j}w_j=0$ for $j=2,\ldots,s$.   But Lemma~\ref{L:Ajinvertible} 
implies that $C|V_1$ is invertible.  Therefore, $w_1=0$.  Similarly, all of 
the remaining $w_j$ have to vanish.  \end{proof}

\begin{lemma}  \label{L:spanVj}
Every vector in $\R^n$ is a linear combination of vectors in the generalized 
eigenspaces $V_j$.
\end{lemma}

\begin{proof}  Let $W$ be the subspace
of $\R^n$ consisting of all vectors of the form $z_1+\cdots +z_s$ where 
$z_j\in V_j$.  We need to verify that $W=\R^n$.  Suppose that $W$ is a 
proper subspace.  Then choose a basis $w_1,\ldots,w_t$ of $W$ and extend
this set to a basis ${\cal W}$ of $\R^n$.  In this basis the matrix
$[A]_{\cal W}$ has block form, that is,
\[
[A]_{\cal W} = \mattwo{A_{11}}{A_{12}}{0}{A_{22}},
\]
where $A_{22}$ is an $(n-t)\times(n-t)$ matrix.  The eigenvalues of $A_{22}$ 
are eigenvalues of $A$.  Since all of the distinct eigenvalues and 
eigenvectors of $A$ are accounted for in $W$ (that is, in $A_{11}$), we have 
a contradiction.  So $W=\R^n$, as claimed.  \end{proof}

\begin{lemma}  \label{L:basisunion}
Let ${\cal V}_j$ be a basis for the generalized eigenspaces $V_j$ and let 
${\cal V}$ be the union of the sets ${\cal V}_j$.  Then ${\cal V}$ is a basis
for $\R^n$.
\end{lemma}

\begin{proof}  We first show that the vectors in ${\cal V}$ span $\R^n$.  It follows 
from Lemma~\ref{L:spanVj} that every vector in $\R^n$ is a linear combination
of vectors in $V_j$.  But each vector in $V_j$ is a linear combination of
vectors in ${\cal V}_j$.  Hence, the vectors in ${\cal V}$ span $\R^n$.

Second, we show that the vectors in ${\cal V}$ are linearly independent. 
Suppose that a linear combination of vectors in ${\cal V}$ sums to zero.  
We can write this sum as 
\[
w_1 + \cdots + w_s = 0,
\]
where $w_j$ is the linear combination of vectors in ${\cal V}_j$. 
Lemma~\ref{L:independentVj} implies that each $w_j=0$.  Since ${\cal V}_j$ is
a basis for $V_j$, it follows that the coefficients in the linear
combinations $w_j$ must all be zero.  Hence, the vectors in ${\cal V}$ are 
linearly independent.

Finally, it follows from Theorem~\ref{basis=span+indep} of 
Chapter~\ref{C:vectorspaces} that ${\cal V}$ is a basis.  \end{proof}

\begin{lemma} \label{L:diagVj}
In the basis ${\cal V}$ of $\R^n$ guaranteed by Lemma~\ref{L:basisunion}, the 
matrix $[A]_{\cal V}$ is block diagonal, that is,
\[
[A]_{\cal V} = \left(\begin{array}{ccc} A_{11} & 0 & 0  \\ 0 & \ddots & 0 \\
0 & 0 & A_{ss} \end{array}\right),
\]
where all of the eigenvalues of $A_{jj}$ equal $\lambda_j$.
\end{lemma}

\begin{proof}  It follows from Lemma~\ref{L:commute} that $A:V_j\to V_j$.  Suppose
that $v_j\in {\cal V}_j$.   Then $Av_j$ is in $V_j$ and $Av_j$ is a linear
combination of vectors in ${\cal V}_j$.   The block diagonalization of 
$[A]_{\cal V}$ follows.  Since $V_j=\nulls(A_j^{k_j})$, it follows that all
eigenvalues of $A_{jj}$ equal $\lambda_j$.   \end{proof}

Lemma~\ref{L:diagVj} implies that to prove the Jordan normal form theorem, 
we must find a basis in which the matrix $A_{jj}$ is in Jordan normal form.  
So, without loss of generality, we may assume that all eigenvalues of $A$ 
equal $\lambda_0$, and then find a basis in which $A$ is in Jordan normal 
form.  Moreover, we can replace $A$ by the matrix $A-\lambda_0I_n$, a
matrix all of whose eigenvalues are zero.  So, without loss of generality, we 
assume that $A$ is an $n\times n$ matrix all of whose eigenvalues are zero.  
We now sketch the remainder of the proof of Theorem~\ref{T:Jordan}.

Let $k$ be the smallest integer such that $\R^n = \nulls(A^k)$ and let 
\[
s=\dim\nulls(A^k)-\dim\nulls(A^{k-1})>0.
\]
Let $z_1,\ldots,z_{n-s}$ be 
a basis for $\nulls(A^{k-1})$ and extend this set to a basis for 
$\nulls(A^k)$ by adjoining the linearly independent vectors $w_1,\ldots,w_s$.  
Let 
\[
W_k=\Span\{w_1,\ldots,w_s\}.
\]
It follows that $W_k\cap\nulls(A^{k-1})=\{0\}$.  

We claim that the $ks$ vectors ${\cal W}=\{w_{j\ell}=A^\ell(w_j)\}$ where 
$0\le\ell\le {k-1}$ and $1\le j\le s$ are linearly independent.  We can write 
any linear combination of the vectors in ${\cal W}$ as $y_k+\cdots+y_1$, 
where $y_j\in A^{k-j}(W_k)$.  Suppose that 
\[
y_k+\cdots+y_1=0.
\]
Then $A^{k-1}(y_k+\cdots+y_1)= A^{k-1}y_k=0$.  Therefore, $y_k$ is in $W_k$ 
and in $\nulls(A^{k-1})$.  Hence, $y_k=0$.  Similarly, 
$A^{k-2}(y_{k-1}+\cdots+y_1)= A^{k-2}y_{k-1}=0$.  But $y_{k-1}=A\hat{y}_k$ 
where $\hat{y}_k\in W_k$ and $\hat{y}_k\in\nulls(A^{k-1})$.  Hence, 
$\hat{y}_k=0$ and $y_{k-1}=0$.  Similarly, all of the $y_j=0$.  It follows 
from $y_j=0$ that a linear combination of the vectors 
$A^{k-j}(w_1),\ldots,A^{k-j}(w_s)$ is zero; that is
\[
0 = \beta_1A^{k-j}(w_1) + \cdots + \beta_sA^{k-j}(w_s) =
A^{k-j}(\beta_1w_1+\cdots+\beta_sw_s).
\]
Applying  $A^{j-1}$ to this expression, we see that 
\[
\beta_1w_1+\cdots+\beta_sw_s
\]
is in $W_k$ and in the $\nulls(A^{k-1})$.  Hence, 
\[
\beta_1w_1+\cdots+\beta_sw_s = 0.
\]
Since the $w_j$ are linearly independent, each $\beta_j=0$, thus verifying 
the claim.

Next, we find the largest integer $m$ so that 
\[
t=\dim\nulls(A^m)-\dim\nulls(A^{m-1})>0.
\]
Proceed as above.  Choose a basis for $\nulls(A^{m-1})$ and extend to a basis 
for $\nulls(A^m)$ by adjoining the vectors $x_1,\ldots,x_t$.  Adjoin the $mt$ 
vectors $A^\ell x_j$ to the set ${\cal V}$ and verify that these vectors are 
all linearly independent.  And repeat the process.  Eventually, we arrive at 
a basis for $\R^n=\nulls(A^k)$.  

In this basis the matrix $[A]_{\cal V}$ is block diagonal; indeed, each of 
the blocks is a Jordan block, since 
\[
A(w_{j\ell}) = \left\{\begin{array}{cl}w_{j(\ell-1)} & 0<\ell\le k-1\\ 0 & 
\ell=1 \end{array}\right. .
\]
Note the resemblance with \Ref{e:Ndef}.


\end{document}
