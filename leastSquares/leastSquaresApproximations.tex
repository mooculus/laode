\documentclass{ximera}

\input{../preamble.tex}

\title{Least Squares Approximations}

\begin{document}
\begin{abstract}
\end{abstract}
\maketitle

  \label{S:LSA}

Let $W\subset\R^n$ be a subspace and $b\in\R^n$ be a vector.  In this
section we solve a basic geometric problem and investigate some of its
consequences.  The problem is:
\begin{quote}
Find the vector $\tilde{w}\in W$ that is the nearest vector to $b$ in $W$.
\end{quote}

\begin{definition} \rm \label{D:least_squares}
The vector $\tilde{w}$ in the subspace $W$ of $\R^n$ that is the nearest to the 
vector $b$ in $\R^n$ is called the \em{least squares approximation} of 
$b$ in $W$.
\end{definition} 

\begin{figure}[htb]
        \centerline{%
        \includegraphics[width=2.5in]{../figures/nearest.pdf}}
        \caption{The vector $\tilde{w}$ is the least squares approximation to the vector $b$ by a vector in $W$.}
        \label{F:nearest}
\end{figure}


The distance between two vectors\index{distance!between vectors}
$v$ and $w$ is $||v-w||$.  Hence the least squares approximation 
can be rephrased as follows: find a vector $\tilde{w}\in W$ such that
\begin{equation}  \label{E:leastsq}
||b-\tilde{w}||\leq ||b-w|| \quad \forall w\in W.
\end{equation}
Condition \eqref{E:leastsq} is also called the
{\em least squares approximation}\index{least squares!approximation}.
In order to see where this name comes from, \eqref{E:leastsq} can be 
rewritten in the equivalent form
\begin{equation} \label{E:LS_inequality}
||b-\tilde{w}||^2\leq ||b-w||^2 \quad \forall w\in W.
\end{equation}
The form \eqref{E:LS_inequality} means that the sum of the squares of the
components of the vectors $b - w$ is minimal at $w = \tilde{w}$.

Recall from \eqref{dotprod=0} that two vectors $z_1,z_2\in\R^n$ are perpendicular 
or equivalently {\em orthogonal} if $z_1\cdot z_2 = 0$.  Before continuing, we state and prove 
\begin{lemma}[The Law of Pythagorus] \index{Law of Pythagorus}  \label{L:LP}
The vectors $z_1,z_2\in\R^n$ are orthogonal if and only if  
\begin{equation} \label{E:LP}
||z_1+z_2||^2 = ||z_1||^2 + ||z_2||^2.
\end{equation}
\end{lemma}

\begin{proof}
To verify \eqref{E:LP} calculate 
\begin{align*}
  ||z_1+z_2||^2&=(z_1+z_2)\cdot(z_1+z_2) \\
  &=z_1\cdot z_1 +2z_1\cdot z_2+z_2\cdot z_2 \\
  &=||z_1||^2 + 2z_1\cdot z_2 +||z_2||^2.
\end{align*}
It follows that $z_1$ and $z_2$ satisfy \eqref{E:LP} if and only if $z_1\cdot z_2 = 0$  
if and only if $z_1$ and $z_2$ are orthogonal.
\end{proof}

Using \eqref{E:leastsq} and \eqref{E:LP}, we can rephrase the minimum distance 
problem as follows.
\begin{lemma}  \label{L:orthoLSA}
The vector $\tilde{w}\in W$ is the closest vector to $b\in\R^n$ if the vector 
$b - \tilde{w}$ is orthogonal to every vector in $W$. See Figure~\ref{F:nearest}.
\end{lemma}

\begin{proof}  Write $b-w=z_1+z_2$ where $z_1=b-\tilde{w}$ and $z_2=\tilde{w}-w$.  By 
assumption, $b-\tilde{w}$ is orthogonal to every vector in $W$; so $z_1$ and 
$z_2\in W$ are orthogonal.  It follows from \eqref{E:LP} that
\[
||b-w||^2 = ||b-\tilde{w}||^2 + ||\tilde{w}-w||^2.
\]
Since $||\tilde{w}-w||^2\ge 0$, \eqref{E:LS_inequality} is valid, and $\tilde{w}$ is the 
vector in $W$ that is nearest to $b$. 
\end{proof}

\subsection*{Least Squares Distance to a Line}
\index{least squares!distance to a line}
\index{distance!to a line}

Suppose $W$ is as simple a subspace as possible; that is, suppose $W$ is one
dimensional with basis vector $w$.  Since $W$ is one dimensional, a vector
$\tilde{w}\in W$ must be a multiple of $w$; that is,
$\tilde{w} = \alpha w$ for $\alpha\in\R$.  Suppose that we can find a scalar $a$ 
so that $b - \alpha w$ is orthogonal to every vector in $W$.  Then it follows from
Lemma~\ref{L:orthoLSA} that $\tilde{w}$ is the closest vector in $W$ to $b$.
To find $\alpha$, calculate
\[
0 = (b- \alpha w)\cdot w = b\cdot w - \alpha w\cdot w.
\]
Then
\[
\alpha = \frac{b\cdot w}{||w||^2}
\]
and
\begin{equation}  \label{E:singleortho}
\tilde{w} = \frac{b\cdot w}{||w||^2} w.
\end{equation}
Observe that $||w||^2\not=0$ since $w$ is a basis vector.

For example, if $b = (1,2,-1,3)\in\R^4$ and $w=(0,1,2,3)$.  Then the vector
$\tilde{w}$ in the space spanned by $w$ that is nearest to $b$ is
\[
\tilde{w} = \frac{9}{14}w
\]
since $b \cdot w = 9$ and $||w||^2 = 14$.

\subsection*{Least Squares Distance to a Subspace}
\index{least squares!distance to a subspace}
\index{distance!to a subspace}

\ignore{
Similarly, using Lemma~\ref{L:orthoLSA} we can solve the general least
squares problem by solving a system of linear equations.  Let
$w_1,\ldots,w_k$ be a basis for $W$ and suppose that
\[
\tilde{w} = \alpha_1w_1 + \cdots + \alpha_kw_k
\]
for some scalars $\alpha_i$.  We now show how to find these scalars.
}
Similarly, we solve the general least squares problem by solving a 
system of linear equations.


\begin{theorem}  \label{T:nearestvector}
Let $b\in\R^n$ be a vector, let $\{w_1,\ldots,w_k\}$ be a basis\index{basis} 
for the subspace\index{subspace} $W\subset\R^n$, and let $\WW = (w_1|\cdots|w_k)$ 
be the $n\times k$ matrix whose columns are the basis vectors of $W$.  Suppose
\begin{equation} \label{e:w_0_in_basis}
\tilde{w} = \alpha_1w_1 + \cdots + \alpha_kw_k
\end{equation}
is the vector in $W$ nearest to $b$. Then
\begin{equation}  \label{E:nearestvector}
\left(\begin{array}{c} \alpha_1 \\ \vdots \\ \alpha_k \end{array}\right) =
(\WW^t\WW)\inv \WW^t b.
\end{equation}
\end{theorem}

\begin{proof} Observe that the vector $b - \tilde{w}$ is orthogonal to every vector in $W$
precisely when $b-\tilde{w}$ is orthogonal to each basis vector $w_j$.  It
follows from Lemma~\ref{L:orthoLSA} that $\tilde{w}$ is the closest vector to $b$
in $W$ if
\[
(b - \tilde{w})\cdot w_j = 0
\]
for every $j$.  That is, if
\[
\tilde{w} \cdot w_j = b \cdot w_j
\]
for every $j$.  These equations can be rewritten as a system of equations in
terms of the $\alpha_i$, as follows:
\begin{equation}  \label{E:dots}
 \begin{array}{ccc}
w_1\cdot w_1\alpha_1 + \cdots + w_1\cdot w_k\alpha_k & = & w_1\cdot b\\
 & \vdots &  \\
w_k\cdot w_1\alpha_1 + \cdots + w_k\cdot w_k\alpha_k & = & w_k\cdot b.
\end{array}
\end{equation}

Note that if $u,v\in\R^n$ are column vectors, then $u\cdot v= u^tv$. Therefore,
we can rewrite \eqref{E:dots} as
\[
\WW^t\WW \left(\begin{array}{c} \alpha_1\\ \vdots \\ \alpha_k \end{array}\right) =
\WW^t b,
\]
where $\WW$ is the matrix whose columns are the $w_j$ and $b$ is viewed as a
column vector.  Note that the matrix $\WW^t\WW$ is a $k\times k$ matrix.

We claim that $\WW^t\WW$ is invertible.  To verify this claim, it suffices to
show that the null space\index{null space}
of $\WW^t\WW$ is zero; that is, if $\WW^t\WW z = 0$ for some
$z\in\R^k$, then we show $z=0$.  First, calculate
\[
||\WW z||^2 = \WW z\cdot \WW z = (\WW z)^t\WW z = z^t\WW^t\WW z= z^t0 = 0.
\]
It follows that $\WW z=0$.  Now if we let $z=(z_1,\ldots,z_k)^t$, then the
equation $\WW z=0$ may be rewritten as
\[
z_1w_1 + \cdots + z_kw_k = 0.
\]
Since the $w_j$ are linearly independent, it follows that $z_1 = \cdots = z_k =0$   
and $z=0$.  Since $\WW^t\WW $ is invertible, \eqref{E:nearestvector} is
valid, and the theorem is proved. 
\end{proof}

\begin{corollary} \label{C:nearestvector}
Let $b$ be a vector in $\R^n$, let $W$ be a subspace of $\R^n$, and
let $w_1,\ldots,w_k$ in $\R^n$ be a basis for $W$.
Let $\WW $ be the $n\times k$ matrix $(w_1|\cdots| w_k)$.  Then 
\begin{equation} \label{C:w_0_formula}
\tilde{w} =  \WW (\WW^t\WW )\inv \WW^t b \in W
\end{equation}
 is the least squares approximation to $b$ in $W$.  The distance of 
 $b$ to $W$ is $|| b - \tilde{w} ||$.
\end{corollary}

\begin{proof}
Define scalars $\alpha_1,\ldots,\alpha_k$ by \eqref{E:nearestvector}.  It follows 
from Theorem~\ref{T:nearestvector} that the least squares approximation is
\[ 
\tilde{w} = \WW \left(\begin{array}{c} \alpha_1 \\ \vdots \\ \alpha_k \end{array}\right) 
= \WW (\WW^t \WW )\inv \WW^t b, 
\]
as claimed.
\end{proof}

\includeexercises

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../linearAlgebra"
%%% End:
