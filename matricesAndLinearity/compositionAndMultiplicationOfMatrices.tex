\documentclass{ximera}

\input{../preamble.tex}

\title{Composition and Multiplication of Matrices}

\begin{document}
\begin{abstract}
\end{abstract}
\maketitle

 \label{S:4.6}
\index{composition} \index{matrix!multiplication}

The {\em composition\/} of two matrix mappings leads to another
matrix mapping from which the concept of multiplication of two
matrices follows.  Matrix multiplication can be introduced by
formula, but then the idea is unmotivated and one is left to
wonder why matrix multiplication is defined in such a seemingly
awkward way.

We begin with the example of $2\times 2$ matrices.  Suppose that
\[
A= \left(\begin{array}{rr} 2 & 1\\ 1 & -1\end{array}\right)
\AND
B= \left(\begin{array}{rr} 0 & 3\\ -1 & 4\end{array}\right).
\]
We have seen that the mappings
\[
x\mapsto Ax \AND x\mapsto Bx
\]
map $2$-vectors to $2$-vectors.  So we can ask what happens when
we compose these mappings.  In symbols, we compute
\[
L_A\compose L_B (x) = L_A(L_B(x))= A(Bx).
\]
In coordinates, let $x=(x_1, x_2)$ and compute
\begin{eqnarray*}
A(Bx) & = & A \left(\begin{array}{c} 3x_2 \\ -x_1+4x_2
\end{array}\right)\\
 & = & \left(\begin{array}{c} -x_1+10x_2  \\ x_1-x_2
\end{array}\right).
\end{eqnarray*}
It follows that we can rewrite $A(Bx)$ using multiplication of a
matrix times a vector as
\[
A(Bx) = \left(\begin{array}{rr} -1 & 10 \\ 1 & -1
\end{array}\right)
        \left(\begin{array}{c} x_1 \\ x_2 \end{array} \right).
\]
In particular, $L_A\compose L_B$ is again a linear mapping,
namely $L_C$, where
\[
C =\left(\begin{array}{rr} -1 & 10 \\ 1 & -1 \end{array}\right).
\]
With this computation in mind, we define the product
\[
AB =
\left(\begin{array}{rr} 2 & 1\\ 1 & -1\end{array}\right)
\left(\begin{array}{rr} 0 & 3\\ -1 & 4\end{array}\right)
= \left(\begin{array}{rr} -1 & 10 \\ 1 & -1 \end{array}\right).
\]

Using the same approach we can derive a formula for matrix
multiplication of $2\times 2$ matrices.  Suppose
\[
A= \left(\begin{array}{rr} a_{11} & a_{12}\\ a_{21} &
a_{22}\end{array}\right)
\AND
B= \left(\begin{array}{rr} b_{11} & b_{12}\\ b_{21} &
b_{22}\end{array}\right).
\]
Then
\begin{eqnarray*}
A(Bx) & = & A \left(\begin{array}{c} b_{11}x_1+b_{12}x_2 \\
b_{21}x_1+b_{22}x_2 \end{array}\right)\\
 & = & \left(\begin{array}{c} a_{11}(b_{11}x_1+b_{12}x_2)
+a_{12}(b_{21}x_1+b_{22}x_2)  \\
a_{21}(b_{11}x_1+b_{12}x_2) +
a_{22}(b_{21}x_1+b_{22}x_2) \end{array}\right) \\
 & = & \left(\begin{array}{c} (a_{11}b_{11}+a_{12}b_{21})x_1+
(a_{11}b_{12}+a_{12}b_{22})x_2  \\
(a_{21}b_{11}+a_{22}b_{21})x_1+
(a_{21}b_{12}+a_{22}b_{22})x_2 \end{array}\right) \\
& = & \left(\begin{array}{rr} a_{11}b_{11}+a_{12}b_{21} &
a_{11}b_{12}+a_{12}b_{22}\\
a_{21}b_{11}+a_{22}b_{21} & a_{21}b_{12}+a_{22}b_{22}
\end{array}\right)
        \left(\begin{array}{c} x_1 \\ x_2 \end{array} \right).
\end{eqnarray*}
Hence, for $2\times 2$ matrices, we see that composition of
matrix mappings defines the matrix multiplication\index{matrix!multiplication}
\begin{equation*}
\left(\begin{array}{rr} a_{11} & a_{12}\\ a_{21} &
a_{22}\end{array}\right)
\left(\begin{array}{rr} b_{11} & b_{12}\\ b_{21} &
b_{22}\end{array}\right)
\end{equation*}
to be
\begin{equation}  \label{2x2mult}
\left(\begin{array}{rr} a_{11}b_{11}+a_{12}b_{21} &
a_{11}b_{12}+a_{12}b_{22}
\\ a_{21}b_{11}+a_{22}b_{21} & a_{21}b_{12}+a_{22}b_{22}
\end{array}\right).
\end{equation}

Formula \eqref{2x2mult} may seem a bit formidable, but it does
have structure.  Suppose $A$ and $B$ are $2\times 2$ matrices,
then the entry of
\[
C=AB
\]
in the $i^{th}$ row, $j^{th}$ column may be written as
\[
a_{i1}b_{1j} + a_{i2}b_{2j} = \sum_{k=1}^2 a_{ik}b_{kj}.
\]
We shall see that an analog of this formula is available for
matrix multiplications of all sizes.  But to derive this
formula, it is easier to develop matrix multiplication
abstractly.


\begin{lemma}  \label{complin}
Let $L_1:\R^n\to\R^m$ and $L_2:\R^p\to\R^n$ be linear mappings.
Then $L=L_1\compose L_2:\R^p\to\R^m$ is a linear mapping.
\end{lemma}

\begin{proof}  Compute
\begin{eqnarray*}
L(x+y) & = & L_1\compose L_2(x+y)\\
 & = & L_1(L_2(x)+L_2(y)) \\
 & = & L_1(L_2(x)) + L_1(L_2(y))\\
 & = & L_1\compose L_2(x) + L_1\compose L_2(y)\\
 & = & L(x) + L(y).
\end{eqnarray*}
Similarly, compute $L_1\compose L_2(cx) = cL_1\compose L_2(x)$.
\end{proof}

We apply Lemma~\ref{complin} in the following way.  Let $A$ be
an $m\times n$ matrix and let $B$ be an $n\times p$
matrix.  Then $L_A:\R^n\to\R^m$ and $L_B:\R^p\to\R^n$ are linear
mappings, and the mapping $L=L_A\compose L_B:\R^p\to\R^m$ is
defined and linear.  Theorem~\ref{lin-matrices} implies that
there is an $m\times p$ matrix $C$ such that $L=L_C$.
Abstractly, we define the {\em matrix product\/} \index{matrix!product}
$AB$ to be $C$.
\begin{quote}
{\em Note that the matrix product $AB$ is defined only when the number of
columns of $A$ is equal to the number of rows of $B$.}
\end{quote}

\subsubsection*{Calculating the Product of Two Matrices}

Next we discuss how to calculate the product of matrices; this
discussion generalizes our discussion of the product of $2\times 2$
matrices.  Lemma~\ref{columnsA} tells how to compute $C=AB$.  The $j^{th}$
column of the matrix product is just
\[
Ce_j = A(Be_j),
\]
where $B_j\equiv Be_j$ is the $j^{th}$ column of the matrix $B$.  Therefore,
\begin{equation}  \label{E:matprod}
C = (AB_1|\cdots|AB_p).
\end{equation}
Indeed, the $(i,j)^{th}$ entry of $C$ is the $i^{th}$ entry of $AB_j$,
that is, the $i^{th}$ entry of
\[
A\left(\begin{array}{c} b_{1j}\\ \vdots\\
b_{nj}\end{array}\right)
=
\left(\begin{array}{c} a_{11}b_{1j} + \cdots + a_{1n}b_{nj} \\
\vdots \\ a_{m1}b_{1j} + \cdots + a_{mn}b_{nj}
\end{array}\right).
\]
It follows that the entry $c_{ij}$ of $C$ in the $i^{th}$ row and
$j^{th}$ column is
\begin{equation} \label{multij}
c_{ij} = a_{i1}b_{1j} + a_{i2}b_{2j} + \cdots + a_{in}b_{nj} =
\sum_{k=1}^n a_{ik}b_{kj}.
\end{equation}

We can interpret \eqref{multij} in the following way.  To calculate $c_{ij}$:
multiply the entries of the $i^{th}$ row of $A$ with the corresponding
entries in the $j^{th}$ column of $B$ and add the results.  This
interpretation reinforces the
idea that for the matrix product $AB$ to be defined, the number of columns
in $A$ must equal the number of rows in $B$.

For example, we now perform the following multiplication:
\begin{eqnarray*}
& & \left(\begin{array}{rrr} 2 & 3 & 1 \\ 3 & -1 & 2 \end{array}\right)
\left(\begin{array}{rr} 1 & -2 \\ 3 & 1 \\ -1 & 4 \end{array}\right)\\
& = & \mattwoc{2\cdot 1+3\cdot 3 + 1\cdot(-1)}{2\cdot(-2)+3\cdot 1+1\cdot 4}
{3\cdot 1 + (-1)\cdot 3+2\cdot(-1)}{3\cdot(-2)+(-1)\cdot 1+2\cdot 4}\\
& = & \mattwo{10}{3}{-2}{1}.
\end{eqnarray*}

\subsubsection*{Some Special Matrix Products}

Let $A$ be an $m\times n$ matrix.  Then
\begin{eqnarray*}
OA & = & O \\
AO & = & O \\
AI_n & = & A \\
I_mA & = & A
\end{eqnarray*}
The first two equalities are easily checked using \eqref{multij}.
It is not significantly more difficult to verify the last two
equalities using \eqref{multij}, but we shall verify these
equalities using the language of linear mappings, as follows:
\[
L_{AI_n}(x)=L_A\compose L_{I_n}(x)=L_A(x),
\]
since $L_{I_n}(x)=x$ is the identity map.  Therefore $AI_n=A$.
A similar proof verifies that $I_mA=A$.  Although the
verification of these equalities using the notions of linear
mappings may appear to be a case of overkill, the next section
contains results where these notions truly simplify the discussion.



\includeexercises



\end{document}
