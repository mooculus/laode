\documentclass{ximera}

\input{../preamble.tex}

\title{Linearity}

\begin{document}
\begin{abstract}
\end{abstract}
\maketitle

  \label{S:linearity}

We begin by recalling the vector operations of addition and
scalar multiplication.  Given two $n$ vectors, vector addition
\index{vector!addition} is defined by
\[
\vect{x}{n}+\vect{y}{n}=\left(\begin{array}{c} x_1+y_1 \\ \vdots \\
x_n+y_n\end{array}\right).
\]
Multiplication of a scalar \index{scalar multiplication} times a vector
is defined by
\[
c\vect{x}{n} = \vect{cx}{n}.
\]
Using \eqref{Atimesx} we can check that matrix multiplication
satisfies
\begin{eqnarray}
A(x+y) & = & Ax + Ay \label{sum} \\
A(cx) & = & c(Ax). \label{product}
\end{eqnarray}
Using \Matlab we can also verify that the identities \eqref{sum}
and \eqref{product} are valid for some particular choices of $x$,
$y$, $c$ and $A$.  For example, let $c=5$ and
\begin{matlabEquation}\label{MATLAB:29}
A = \left(\begin{array}{cccc} 2 & 3 & 4 & 1\\ 1 & 1 & 2 & 3
\end{array}\right), \quad x = \left(\begin{array}{r} 1 \\ 5 \\ 4 \\
3 \end{array}\right), \quad y = \left(\begin{array}{r} 1 \\ -1 \\ -1 \\
4 \end{array}\right).
\end{matlabEquation}
Typing {\tt e3\_3\_3} enters this information into \Matlabp.  Now
type
\begin{verbatim}
z1 = A*(x+y)
z2 = A*x + A*y
\end{verbatim}
and compare {\tt z1} and {\tt z2}.  The fact that they are both
equal to
\[
\vectwo{35}{33}
\]
verifies \eqref{sum} in this case.  Similarly, type
\begin{verbatim}
w1 = A*(c*x)
w2 = c*(A*x)
\end{verbatim}
and compare {\tt w1} and {\tt w2} to verify \eqref{product}.


The central idea in linear algebra is the notion of
{\em linearity\/}. \index{linear}
\begin{definition} \label{linearity}
A mapping $L:\R^n\to\R^m$ is {\em linear}\index{linear!mapping}
if
\begin{itemize}
\item[(a)]  $L(x+y) = L(x) + L(y)$ for all $x,y\in\R^n$.
\item[(b)]  $L(cx) = cL(x)$ for all $x\in\R^n$ and all scalars
$c\in\R$.
\end{itemize}
\end{definition}

To better understand the meaning of Definition~\ref{linearity}(a,b),
we verify these conditions for the mapping $L:\R^2\to\R^2$ defined by
\begin{equation}  \label{E:mme}
L(x) = (x_1+3x_2,2x_1-x_2),
\end{equation}
where $x=(x_1,x_2)\in\R^2$.  To verify Definition~\ref{linearity}(a), let
$y=(y_1,y_2)\in\R^2$.  Then
\begin{eqnarray*}
L(x+y) & = & L(x_1+y_1,x_2+y_2)\\
& = & ((x_1+y_1)+3(x_2+y_2), 2(x_1+y_1)-(x_2+y_2)) \\
 & = & (x_1+y_1+3x_2+3y_2, 2x_1+2y_1-x_2-y_2).
\end{eqnarray*}
On the other hand,
\begin{eqnarray*}
L(x)+L(y) & = & (x_1+3x_2,2x_1-x_2) + (y_1+3y_2,2y_1-y_2) \\
 & = & (x_1+3x_2+y_1+3y_2, 2x_1-x_2+2y_1-y_2).
\end{eqnarray*}
Hence
\[
L(x+y) = L(x) + L(y)
\]
for every pair of vectors $x$ and $y $ in $\R^2$.

Similarly, to verify Definition~\ref{linearity}(b), let $c\in\R$ be a scalar
and compute
\[
L(cx) = L(cx_1,cx_2)=((cx_1)+3(cx_2),2(cx_1)-(cx_2)).
\]
Then compute
\[
cL(x) = c(x_1+3x_2,2x_1-x_2)= (c(x_1+3x_2), c(2x_1-x_2)),
\]
from which it follows that
\[
L(cx) = cL(x)
\]
for every vector $x\in\R^2$ and every scalar $c\in\R$.  Thus $L$ is a linear 
mapping.

In fact, the mapping \eqref{E:mme} is a matrix mapping and could
have been written in the form
\[
L(x) = \mattwo{1}{3}{2}{-1}x.
\]
Hence the linearity of $L$ could have been checked using identities
\eqref{sum} and \eqref{product}.  Indeed, matrix mappings are always linear
mappings, as we now discuss.

\subsubsection*{Matrix Mappings are Linear Mappings}

Let $A$ be an $m\times n$ matrix and recall that the matrix mapping
$L_A:\R^n\to\R^m$ is defined by $L_A(x)=Ax$.  We may rewrite \eqref{sum} and
\eqref{product} using this notation as
\begin{eqnarray*}
L_A(x+y) & = & L_A(x) + L_A(y) \\
L_A(cx) & = & cL_A(x).
\end{eqnarray*}
Thus all matrix mappings\index{matrix!mappings} are linear
mappings\index{linear!mapping}.  We will show that all linear
mappings are matrix mappings (see Theorem~\ref{lin-matrices}).  But first
we discuss linearity in the simplest context of mappings from $\R\to\R$.

\subsection*{Linear and Nonlinear Mappings of $\R\to\R$}

Note that $1\times 1$ matrices are just scalars $A=(a)$.  It follows from
\eqref{sum} and \eqref{product} that we have shown that the matrix mappings
$L_A(x)=ax$ are all linear, though this point could have been verified
directly.  Before showing that these are all the linear mappings of
$\R\to\R$, we focus on examples of functions of $\R\to\R$ that are
{\em not\/} linear.

\subsubsection*{Examples of Mappings that are Not Linear}

\begin{itemize}
\item   $f(x)=x^2$.  Calculate
\[
f(x+y) = (x+y)^2 = x^2+2xy+y^2
\]
while
\[
f(x)+f(y) = x^2 + y^2.
\]
The two expressions are not equal and $f(x)=x^2$ is not linear.
\item   $f(x)=e^x$.  Calculate
\[
f(x+y) = e^{x+y} = e^x e^y
\]
while
\[
f(x)+f(y) = e^x + e^y.
\]
The two expressions are not equal and $f(x)=e^x$ is not linear.
\item   $f(x) = \sin x$.  Recall that
\[
f(x+y) =\sin(x+y) = \sin x \cos y +\cos x \sin y
\]
while
\[
f(x)+f(y) = \sin x + \sin y.
\]
The two expressions are not equal and $f(x)=\sin x $ is not
linear.
\end{itemize}

\subsubsection*{Linear Functions of One Variable}

Suppose we take the opposite approach and ask what functions of
$\R\to\R$ are linear.  Observe that if $L:\R\to\R$ is linear,
then
\[
L(x) = L(x\cdot 1).
\]
Since we are looking at the special case of linear mappings on
$\R$, we note that $x$ is a real number as well as a vector.
Thus we can use Definition~\ref{linearity}(b) to observe that
\[
L(x\cdot 1)=xL(1).
\]
So if we let $a=L(1)$, then we see that
\[
L(x)=ax.
\]
Thus linear mappings of $\R$ into $\R$ are very special mappings
indeed; they are all scalar multiples of the identity mapping.

\subsection*{All Linear Mappings are Matrix Mappings}

We end this section by proving that every linear mapping is
given by matrix multiplication. But first we state and prove two
lemmas.  There is a standard set of vectors that is used over
and over again in linear algebra, which we now define.

\begin{definition}  \label{D:canonicalbasis}
Let $j$ be an integer between $1$ and $n$.  The $n$-vector $e_j$ is
the vector that has a $1$ in the $j^{th}$ entry and zeros in all
other entries.
\end{definition} \index{$e_j$}

\begin{lemma}  \label{linequal}
Let $L_1:\R^n\to\R^m$ and $L_2:\R^n\to\R^m$ be linear mappings.
Suppose that $L_1(e_j)=L_2(e_j)$ for every $j=1,\ldots,n$.  Then
$L_1=L_2$.
\end{lemma}

\begin{proof}  Let $x=(x_1,\ldots,x_n)$ be a vector in $\R^n$.  Then
\[
x = x_1e_1 + \cdots + x_ne_n.
\]
Linearity of $L_1$ and $L_2$ implies that
\begin{eqnarray*}
L_1(x) & = & x_1L_1(e_1) + \cdots + x_nL_1(e_n) \\
  & = & x_1L_2(e_1) + \cdots + x_nL_2(e_n) \\
  & = & L_2(x).
\end{eqnarray*}
Since $L_1(x)=L_2(x)$ for all $x\in\R^n$, it follows that
$L_1=L_2$.  \end{proof}

\begin{lemma}  \label{columnsA}
Let $A$ be an $m\times n$ matrix.  Then $Ae_j$ is the $j^{th}$
column of $A$.
\end{lemma}

\begin{proof}  Recall the definition of matrix multiplication given in \eqref{Atimesx}.
In that formula, just set $x_i$ equal to zero for all $i\neq j$ and set
$x_j=1$. \end{proof}

\begin{theorem}  \label{lin-matrices}
Let $L:\R^n\to\R^m$ be a linear mapping\index{linear!mapping}.
Then there exists an $m\times n$ matrix $A$ such that $L=L_A$.
\end{theorem}

\begin{proof}
There are two steps to the proof: determine the matrix $A$ and
verify that $L_A=L$.

Let $A$ be the matrix whose $j^{th}$ column is $L(e_j)$.  By
Lemma~\ref{columnsA} $L(e_j) = Ae_j$; that is, $L(e_j) = L_A(e_j)$.
Lemma~\ref{linequal} implies that $L=L_A$.  \end{proof}

Theorem~\ref{lin-matrices} provides a simple way of showing that
\[
L(0) = 0
\]
for any linear map $L$.  Indeed, $L(0)=L_A(0)=A0=0$ for some matrix $A$.  
(This fact can also be proved directly from the definition of linear mapping.)

\subsubsection*{Using Theorem~\protect\ref{lin-matrices} to Find Matrices
Associated to Linear Maps}

The proof of Theorem~\ref{lin-matrices} shows that the $j^{th}$ column of the
matrix $A$ associated to a linear mapping $L$ is $L(e_j)$ viewed as a column
vector.  As an example, let $L:\R^2\to\R^2$ be rotation clockwise through
$90^\circ$.  Geometrically, it is easy to see that
\[
  L(e_1) = L\left(\vectwo{1}{0}\right) = \vectwo{0}{-1}
\]
and
\[
L(e_2) = L\left(\vectwo{0}{1}\right) = \vectwo{1}{0}.
\]
Since we know that rotations are linear maps, it follows that the matrix
$A$ associated to the linear map $L$ is:
\[
A = \mattwo{0}{1}{-1}{0}.
\]
Additional examples of linear mappings whose associated matrices can be found
using Theorem~\ref{lin-matrices} are given in Exercises \ref{c4.3.7} --
\ref{c4.3.10}.



\EXER

\includeexercises




\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../linearAlgebra"
%%% End:
