\documentclass{ximera}

\input{../preamble.tex}

\title{Solving Linear Systems and Inverses}

\begin{document}
\begin{abstract}
\end{abstract}
\maketitle

 \label{S:SLS}

When we solve the simple equation
\[
ax=b,
\]
we do so by dividing by $a$ to obtain
\[
x=\frac{1}{a}b.
\]
This division works as long as $a\neq 0$.

Writing systems of linear equations as
\[
Ax=b
\]
suggests that solutions should have the form
\[
x=\frac{1}{A} b
\]
and the \Matlab command for solving linear systems
\begin{verbatim}
x=A\b
\end{verbatim}
suggests that there is some merit to this analogy.

The following is a better analogy.  Multiplication by $a$ has the
inverse operation: division by $a$; multiplying a number $x$ by
$a$ and then multiplying the result by $a\inv=1/a$ leaves the number
$x$ unchanged (as long as $a\neq 0$).  In this sense we should
write the solution to $ax=b$ as
\[
x=a\inv b.
\]
For systems of equations $Ax=b$ we wish to write solutions as
\[
x=A\inv b.
\]
In this section we consider the questions: What does $A\inv$
mean and when does $A\inv$ exist? (Even in one dimension, we
have seen that the inverse does not always exist, since
$0\inv=\frac{1}{0}$ is undefined.)

\subsection*{Invertibility}

We begin by giving a precise definition of invertibility for square matrices.
\begin{definition} \label{inverse} \index{inverse} \index{invertible}
The $n\times n$ matrix $A$ is {\em invertible\/} if there is an $n\times n$
matrix $B$ such that
\[
AB=I_n \AND BA=I_n.
\]
The matrix $B$ is called an {\em inverse\/} of $A$.  If $A$ is not invertible,
then $A$ is {\em noninvertible\/} or {\em singular\/}. \index{noninvertible}
\index{singular}
\end{definition}

Geometrically, we can see that some matrices are invertible.  For example, 
the matrix
\[
R_{90} = \mattwo{0}{-1}{1}{0}
\]
rotates the plane counterclockwise through $90^\circ$ and is
invertible.  The inverse matrix of $R_{90}$ is the matrix that rotates the
plane clockwise through $90^\circ$.  That matrix is:
\[
R_{-90} = \mattwo{0}{1}{-1}{0}.
\]
This statement can be checked algebraically by verifying that 
$R_{90}R_{-90}=I_2$ and that $R_{-90}R_{90} = I_2$.

Similarly,  
\[
B = \mattwo{5}{3}{2}{1}
\]
is an inverse of 
\[
A = \mattwo{-1}{3}{2}{-5},
\]
as matrix multiplication shows that $AB=I_2$ and $BA=I_2$. In fact, there is 
an elementary formula for finding inverses of $2\times 2$ matrices (when they 
exist); see \eqref{e:formAinv} in Section~\ref{S:det2x2}.

On the other hand, not all matrices are invertible.  For example, the zero 
matrix is noninvertible, since $0B=0$ for any matrix $B$.

\begin{lemma} \label{B=C}
If an $n\times n$ matrix $A$ is invertible, then its inverse is unique
and is denoted by $A\inv$.
\end{lemma}

\begin{proof}
Let $B$ and $C$ be $n\times n$ matrices that are inverses of $A$.  Then
\[
BA=I_n \AND AC=I_n.
\]
We use the associativity of matrix multiplication to
prove that $B=C$.  Compute
\[
B = BI_n = B(AC) = (BA)C = I_nC = C.
\]
\end{proof}

We now show how to compute inverses for products of invertible
matrices.

\begin{proposition} \label{P:invprod} \index{matrix!multiplication}
\index{inverse}
Let $A$ and $B$ be two invertible $n\times n$ matrices.  Then
$AB$ is also invertible and
\[
(AB)\inv = B\inv A\inv.
\]
\end{proposition}

\begin{proof}  Use associativity of matrix multiplication to compute
\[
(AB)(B\inv A\inv) = A(BB\inv)A\inv = AI_nA\inv = AA\inv=I_n.
\]
Similarly,
\[
(B\inv A\inv)(AB) = B\inv(A\inv A)B = B\inv B = I_n.
\]
Therefore $AB$ is invertible with the desired inverse. \end{proof}

\begin{proposition} \label{L:transposeinv} \index{matrix!transpose}
Suppose that $A$ is an invertible $n\times n$ matrix.  Then
$A^t$ is invertible and
\[
(A^t)\inv = (A\inv)^t.
\]
\end{proposition}

\begin{proof}  We must show that $(A\inv)^t$ is the inverse of $A^t$.  Identity
\eqref{e:transposeprod} implies that
\[
(A\inv)^tA^t = (AA\inv)^t = (I_n)^t = I_n,
\]
and
\[
A^t(A\inv)^t = (A\inv A)^t = (I_n)^t = I_n.
\]
Therefore, $(A\inv)^t$ is the inverse of $A^t$, as claimed.  \end{proof}

\subsection*{Invertibility and Unique Solutions}

Next we discuss the implications of invertibility for the
solution of the inhomogeneous linear system:  \index{inhomogeneous}
\begin{equation}  \label{squarematrix}
Ax=b,
\end{equation}
where $A$ is an $n\times n$ matrix and $b\in\R^n$.

\begin{proposition} \label{P:inv=>unique}\index{uniqueness of solutions}
Let $A$ be an invertible $n\times n$ matrix and let $b$ be in $\R^n$.
Then the system of linear equations \eqref{squarematrix} has a unique solution.
\end{proposition}

\begin{proof}  We can solve the linear system \eqref{squarematrix} by setting
\begin{equation}  \label{soln}
x=A\inv b.
\end{equation}
This solution is easily verified by calculating
\[
Ax=A(A\inv b) = (AA\inv)b = I_nb = b.
\]
Next, suppose that $x$ is a solution to \eqref{squarematrix}.  Then
\[
x = I_n x = (A\inv A)x = A\inv(Ax) = A\inv b.
\]
So $A\inv b$ is the only possible solution.  \end{proof}

\begin{corollary} \label{C:inv=>In}
An invertible matrix is row equivalent to $I_n$.
\end{corollary}

\begin{proof}  Let $A$ be an invertible $n\times n$ matrix.
Proposition~\ref{P:inv=>unique} states that the system of linear equations
$Ax=b$ has a unique solution.  Chapter~\ref{lineq}, Corollary~\ref{consistent}
states that $A$ is row equivalent to $I_n$.  \end{proof}

The converse of Corollary~\ref{C:inv=>In} is also valid.

\begin{proposition}  \label{P:row=>inv}
An $n\times n$ matrix $A$ that is row equivalent to $I_n$ is invertible.
\end{proposition}

\begin{proof}  Form the $n\times 2n$ matrix $M=(A|I_n)$.  Since $A$ is row equivalent
to $I_n$, there is a sequence of elementary row operations so that $M$ is
row equivalent to $(I_n|B)$.  Eliminating all columns from the right half
of $M$ except the $j^{th}$ column yields the matrix $(A|e_j)$.  The same
sequence of elementary row operations states that the matrix $(A|e_j)$ is
row equivalent to $(I_n|B_j)$ where $B_j$ is the $j^{th}$ column of $B$.  It
follows that $B_j$ is the solution to the system of linear equations
$Ax=e_j$ and that the matrix product
\[
AB = (AB_1|\cdots|AB_n) = (e_1|\cdots|e_n) = I_n.
\]
So $AB=I_n$.

We claim that $BA=I_n$ and hence that $A$ is invertible.  To verify this claim
form the $n\times 2n$ matrix $N=(I_n|A)$.  Using the same sequence of
elementary row operations again shows that $N$ is row equivalent to $(B|I_n)$.
By construction
the matrix $B$ is row equivalent to $I_n$.  Therefore, there is a unique
solution to the system of linear equations $Bx=e_j$.  Now eliminating all
columns except the $j^{th}$ from the right hand side of the matrix $(B|I_n)$
shows that the solution to the system of linear equations $Bx=e_j$ is just
$A_j$, where $A_j$ is the $j^{th}$ column of $A$.  It follows that
\[
BA = (BA_1|\cdots|BA_n) = (e_1|\cdots|e_n) = I_n.
\]
Hence $BA=I_n$.  \end{proof}

\begin{theorem} \label{invertequiv}
Let $A$ be an $n\times n$ matrix.  Then the following are
equivalent:
\begin{itemize}
\item[(a)]  $A$ is invertible. \index{invertible}
\item[(b)] The equation $Ax=b$ has a unique solution for each
$b\in\R^n$.
\item[(c)]  The only solution to $Ax=0$ is $x=0$.
\item[(d)]  $A$ is row equivalent to $I_n$.
\end{itemize}
\end{theorem}

\begin{proof} $(a) \Rightarrow (b)$ This implication
is just Proposition~\ref{P:inv=>unique}.

$(b) \Rightarrow (c)$ This implication is straightforward --- just
take $b=0$ in \eqref{squarematrix}.

$(c) \Rightarrow (d)$  This implication is just a restatement of
Chapter~\ref{lineq}, Corollary~\ref{consistent}.

$(d) \Rightarrow (a)$.  This implication is just
Proposition~\ref{P:row=>inv}. \end{proof}


\subsection*{A Method for Computing Inverse Matrices}

The proof of Proposition~\ref{P:row=>inv} gives a constructive method
for finding the inverse of any invertible square matrix.

\begin{theorem}  \label{T:AIn}\index{inverse!computation}
Let $A$ be an $n\times n$ matrix that is row equivalent to
$I_n$ and let $M$ be the $n\times 2n$ augmented matrix
\begin{equation}  \label{e:M}
M = (A | I_n).
\end{equation}
Then the matrix $M$ is row equivalent to $(I_n | A\inv)$.
\end{theorem}

\subsubsection*{An Example}

Compute the inverse of the matrix
\[
A = \left(\begin{array}{rrr} 1 & 2 & 0 \\ 0 & 1 & 3 \\ 0 & 0 & 1
\end{array}\right).
\]
Begin by forming the $3\times 6$ matrix
\[
M = \left(\begin{array}{rrr|rrr} 1 & 2 & 0 & 1 & 0 & 0 \\
0 & 1 & 3 & 0 & 1 & 0 \\ 0 & 0 & 1 & 0 & 0 & 1
\end{array}\right).
\]
To put $M$ in row echelon form by row reduction, first subtract
3 times the $3^{rd}$ row from the $2^{nd}$ row, obtaining
\[
\left(\begin{array}{rrr|rrr} 1 & 2 & 0 & 1 & 0 & 0 \\
0 & 1 & 0 & 0 & 1 & -3 \\ 0 & 0 & 1 & 0 & 0 & 1
\end{array}\right).
\]
Second, subtract 2 times the $2^{nd}$ row from the $1^{st}$ row, obtaining
\[
\left(\begin{array}{rrr|rrr} 1 & 0 & 0 & 1 & -2 & 6 \\
0 & 1 & 0 & 0 & 1 & -3 \\ 0 & 0 & 1 & 0 & 0 & 1
\end{array}\right).
\]
Theorem~\ref{T:AIn} implies that
\[
A\inv = \left(\begin{array}{rrr} 1 & -2 & 6 \\ 0 & 1 & -3 \\ 0 & 0 & 1
\end{array}\right),
\]
which can be verified by matrix multiplication.

\subsubsection*{Computing the Inverse Using \Matlab}

There are two ways that we can compute inverses using \Matlab.
Either we can perform the row reduction of \eqref{e:M} directly
or we can use the \Matlab the command {\tt inv}.  We illustrate
both of these methods. First type {\tt e3\_7\_4} to recall the matrix
\begin{matlabEquation}\label{MATLAB:31}
A = \left(\begin{array}{rrr} 1 & 2 & 4 \\ 3 & 1 & 1 \\ 2 & 0 & -1
\end{array}\right).
\end{matlabEquation}

To perform the row reduction of \eqref{e:M} we need to form the matrix
$M$. The \Matlab command for generating an $n\times n$ identity
matrix is {\tt eye(n)}.  Therefore, typing
\begin{verbatim}
M = [A eye(3)]
\end{verbatim} \index{\computer!eye}
in \Matlab yields the result
\begin{verbatim}
M =
     1     2     4     1     0     0
     3     1     1     0     1     0
     2     0    -1     0     0     1
\end{verbatim}
Now row reduce $M$ to reduced echelon form as follows.  Type
\begin{verbatim}
M(3,:) = M(3,:) - 2*M(1,:)
M(2,:) = M(2,:) - 3*M(1,:)
\end{verbatim}
obtaining
\begin{verbatim}
M =
     1     2     4     1     0     0
     0    -5   -11    -3     1     0
     0    -4    -9    -2     0     1
\end{verbatim}
Next type
\begin{verbatim}
M(2,:) = M(2,:)/M(2,2)
M(3,:) = M(3,:) + 4*M(2,:)
M(1,:) = M(1,:) - 2*M(2,:)
\end{verbatim}
to obtain
{\scriptsize
\begin{verbatim}
M =
    1.0000         0   -0.4000   -0.2000    0.4000         0
         0    1.0000    2.2000    0.6000   -0.2000         0
         0         0   -0.2000    0.4000   -0.8000    1.0000
\end{verbatim}
  \normalsize}
Finally, type
\begin{verbatim}
M(3,:) = M(3,:)/M(3,3)
M(2,:) = M(2,:) - M(2,3)*M(3,:)
M(1,:) = M(1,:) - M(1,3)*M(3,:)
\end{verbatim}
to obtain
{\scriptsize\begin{verbatim}
M =
    1.0000         0         0   -1.0000    2.0000   -2.0000
         0    1.0000         0    5.0000   -9.0000   11.0000
         0         0    1.0000   -2.0000    4.0000   -5.0000
\end{verbatim}
\normalsize}
Thus $C=A\inv$ is obtained by extracting the last three columns
of $M$ by typing
\begin{verbatim}
C = M(:,[4 5 6])
\end{verbatim}
which yields
\begin{verbatim}
C =
   -1.0000    2.0000   -2.0000
    5.0000   -9.0000   11.0000
   -2.0000    4.0000   -5.0000
\end{verbatim}
You may check that $C$ is the inverse of $A$ by typing {\tt A*C}
and {\tt C*A}.

In fact, this entire scheme for computing the inverse of a
matrix has been preprogrammed into \Matlab.  Just type
\begin{verbatim}
inv(A)
\end{verbatim} \index{\computer!inv}
to obtain
\begin{verbatim}
ans =
   -1.0000    2.0000   -2.0000
    5.0000   -9.0000   11.0000
   -2.0000    4.0000   -5.0000
\end{verbatim}

We illustrate again this simple method for computing the inverse
of a matrix $A$.  For example, reload the matrix in \eqref{eq:5matrix}
by typing \verb+ e3_1_4+ and obtaining:
\begin{verbatim}
A =
     5    -4     3    -6     2
     2    -4    -2    -1     1
     1     2     1    -5     3
    -2    -1    -2     1    -1
     1    -6     1     1     4
\end{verbatim}
The command {\tt B = inv(A)} stores the inverse of the matrix $A$
in the matrix $B$, and we obtain the result
{\small
\begin{verbatim}
B =
   -0.0712    0.2856   -0.0862   -0.4813   -0.0915
   -0.1169    0.0585    0.0690   -0.2324   -0.0660
    0.1462   -0.3231   -0.0862    0.0405    0.0825
   -0.1289    0.0645   -0.1034   -0.2819    0.0555
   -0.1619    0.0810    0.1724   -0.1679    0.1394
\end{verbatim}
}
This computation also illustrates the fact that even when the matrix
$A$ has integer entries, the inverse of $A$ usually has noninteger
entries.

Let $b=(2,-8,18,-6,-1)$.  Then we may use the inverse $B=A\inv$
to compute the solution of $Ax=b$.  Indeed if we type
\begin{verbatim}
b = [2;-8;18;-6;-1];
x = B*b
\end{verbatim}
then we obtain
\begin{verbatim}
x =
   -1.0000
    2.0000
    1.0000
   -1.0000
    3.0000
\end{verbatim}
as desired (see \eqref{eq:5rhs}).  With this computation we have confirmed
the analytical results of the previous subsections.







\includeexercises





\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../linearAlgebra"
%%% End:
