\documentclass{ximera}

\input{../preamble.tex}

\title{The Wronskian}

\begin{document}
\begin{abstract}
\end{abstract}
\maketitle

  \label{S:wronskian}
\index{Wronskian} 

We return to the homogeneous system of linear differential equations
\arraystart
\begin{equation}  \label{eq:linihsys2}
\frac{dX}{dt}  =  A(t)X
\end{equation}
\arrayfinish
where $A(t)=(a_{ij}(t))$ is an $n\times n$ matrix.  Let $X_1,\ldots,X_n$ be a 
linearly independent set of solutions\index{basis!of solutions} 
to \eqref{eq:linihsys2} and let 
\begin{equation}   \label{E:Y(t)2}
Y(t) = \left(X_1(t)|\cdots |X_n(t)\right) 
\end{equation}
Using existence and uniqueness of solutions to the initial problem for
\eqref{eq:linihsys2}, we showed in Lemma~\ref{L:DEspan} that $Y(t)$ is an 
invertible matrix for every time $t$.  In this section we prove this same 
result by explicitly showing that the determinant\index{determinant} 
of $Y(t)$ is always nonzero.

Define the {\em Wronskian\/}\index{Wronskian} to be
\[
W(t) = \det Y(t).
\]

\begin{theorem}  \label{T:Wronskian}
Let $X_1,\ldots,X_n$ be solutions to \eqref{E:NCCH} such that the vectors
$X_j(0)$ form a basis of $\R^n$.  Let $Y(t)$ be the matrix defined in 
\eqref{E:Y(t)}.  Then
\begin{equation}  \label{L:Wronskian}
W(t) = e^{\int_0^t\trace(A(\tau))d\tau}W(0).
\end{equation}
\end{theorem}

It follows directly from \eqref{L:Wronskian} that the determinant of $Y(t)$ is 
nonzero when the determinant of $Y(0)$ is nonzero.  But $\det Y(0)\neq 0$ 
since the vectors $X_j(0)$ form a basis of $\R^n$. 

We prove Theorem~\ref{T:Wronskian} in two important special cases: linear 
constant coefficient systems and linear nonconstant $2\times 2$ systems.  
The proof for constant coefficient systems is based on Jordan normal forms, 
while the proof for $2\times 2$ systems is based on solving a separable 
differential equation for the Wronskian itself.  It is this latter proof 
that generalizes to a proof of the theorem.

\subsubsection*{Wronskians for Constant Coefficient Systems}
\index{Wronskian!constant coefficient system}

First, we interpret the Wronskian directly in terms of the constant coefficient 
matrix $A$.  Note that $X_j(t)=e^{tA}e_j$ is just the $j^{th}$ column of the 
matrix $e^{tA}$.   It follows that 
\[
W(t) = \det e^{tA}.
\]

\begin{lemma} 
Let $A$ be an $n\times n$ matrix.  Then
\[
\det e^A = e^{\trace(A)}.
\]
\end{lemma}\index{determinant}\index{matrix!exponential}
\index{trace}

\begin{proof}  This result is proved using 
Jordan normal forms\index{Jordan normal form}.  To see why normal
form theory is relevant, suppose that $A$ and $B$ are similar matrices.  Then 
$e^A$ and $e^B$ are similar matrices, and $\trace(A)=\trace(B)$ and 
$\det e^A = \det e^B$.  So if we can show that the lemma is valid for matrices 
in Jordan normal form, then the lemma is valid for all matrices.

Suppose that the matrix $J$ is  a $k\times k$ 
Jordan block\index{Jordan block} matrix associated
to the eigenvalue $\lambda$.  Then $J$ is upper triangular and the 
diagonal entries of $J$ all equal $\lambda$.  
It follows that $\trace(J)=k\lambda$.
It also follows that $e^J$ is an upper triangular matrix whose diagonal 
entries all equal $e^\lambda$.  Hence
\[
\det e^J = \left(e^\lambda\right)^k = e^{k\lambda} = e^{\trace(J)}.
\]
So the lemma is valid for Jordan block matrices.

Next suppose that $A$ is in block diagonal\index{matrix!block diagonal}, 
that is
\[
A=\mattwo{B}{0}{0}{C}.
\]
We claim that if the lemma is valid for matrices $B$ and $C$, then it 
is valid for the matrix $A$.  To see this observe that 
\[
\trace(A) = \trace(B) + \trace(C),
\]
and that 
\[
e^A = \mattwo{e^B}{0}{0}{e^C}.
\]
Hence 
\[
\det e^A = \det e^B \det e^C = e^{\trace(B)}e^{\trace(C)}, 
\]
by assumption.  It follows that 
\[
\det e^A = e^{\trace(B)+\trace(C)} = e^{\trace(A)},
\]
as desired.  By induction,  the lemma is valid for Jordan normal form 
matrices and hence for all matrices.  \end{proof}

\subsubsection*{Wronskians for Planar Systems}
\index{Wronskian!planar system}

In the time dependent case we verify Theorem~\ref{T:Wronskian} only 
for $2\times 2$ systems, as this 
substantially simplifies the discussion.   Let 
\[
A(t) = \mattwo{a_{11}(t)}{a_{12}(t)}{a_{21}(t)}{a_{22}(t)},
\]
and let 
\[
X_1(t) = \vectwo{x_1(t)}{y_1(t)} \AND  X_2(t) = \vectwo{x_2(t)}{y_2(t)}
\]
be solutions of \eqref{E:NCCH}.  It follows that 
\begin{equation}   \label{E:xyderiv}
\begin{array}{rcl}
\dot{x}_1 & = & a_{11}x_1 + a_{12}y_1 \\
\dot{y}_1 & = & a_{21}x_1 + a_{22}y_1 \\
\dot{x}_2 & = & a_{11}x_2 + a_{12}y_2 \\
\dot{y}_2 & = & a_{21}x_2 + a_{22}y_2.
\end{array}
\end{equation}

In this notation 
\[
Y(t) = \mattwo{x_1(t)}{x_2(t)}{y_1(t)}{y_2(t)},
\]
and
\[
W(t) = x_1(t)y_2(t) - x_2(t)y_1(t).
\]
We claim that 
\[
\dot{W} = \trace(A) W.
\]
If so, we can use separation of variables to solve this differential equation 
obtaining
\[
\ln |W| = \int \trace(A(t))dt 
\]
from which the proof of Theorem~\ref{T:Wronskian} follows.

Use the product rule to compute
\[
\dot{W}  =  \dot{x}_1y_2 + x_1\dot{y}_2 - \dot{x}_2y_1 - x_2\dot{y}_1.
\]
Now substitute \eqref{E:xyderiv} to see that 
\[
\dot{W} = (a_{11}x_1 + a_{12}y_1)y_2 + x_1(a_{21}x_2 + a_{22}y_2)
- (a_{11}x_2 + a_{12}y_2)y_1 - x_2(a_{21}x_1 + a_{22}y_1)
\]
from which it follows that 
\[
\dot{W} = (a_{11}+a_{22})W = \trace(A)W,
\]
as claimed.


\EXER

\includeexercises

 


\end{document}
