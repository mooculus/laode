\documentclass{ximera}

\input{../preamble.tex}

\title{The Jordan Normal Form Theorem}

\begin{document}
\begin{abstract}
\end{abstract}
\maketitle


\label{S:JNF}

The question that we discussed in Sections~\ref{S:RDM} and \ref{S:CSE} is: 
Up to similarity, what is the simplest form that a matrix can have?  
We have seen that if $A$ has real distinct eigenvalues, then $A$ is real 
diagonalizable.  That is, $A$ is similar to a diagonal matrix whose 
diagonal entries are the real eigenvalues of $A$.  Similarly, if $A$ has 
distinct real and complex eigenvalues, then $A$ is complex diagonalizable; 
that is, $A$ is similar either to a diagonal matrix whose diagonal entries 
are the real and complex eigenvalues of $A$ or to a real block diagonal 
matrix.

In this section we address the question of simplest form when a
matrix has multiple eigenvalues.  In much of this discussion we assume
that $A$ is an $n\times n$ matrix with only real eigenvalues.  
Lemma~\ref{L:eigenv-diag} shows that if the eigenvectors of 
$A$ form a basis, then $A$ is diagonalizable. Indeed, for $A$ to be
diagonalizable, there must be a basis of eigenvectors of $A$.  It 
follows that if $A$ is not diagonalizable, then $A$ must have fewer
than $n$ linearly independent 
eigenvectors\index{eigenvector!linearly independent}.  

The prototypical examples of matrices having fewer eigenvectors than 
eigenvalues are the matrices $J_n(\lambda)$ for $\lambda$ real (see
\eqref{E:JnR}) and $\widehat{J}_n(\lambda)$ for $\lambda$ complex (see
\eqref{E:JnC}).
\begin{definition} 
A matrix is in {\em Jordan normal form\/} if it is block diagonal and 
the matrix in each block on the diagonal is a Jordan block, that is, 
$J_\ell(\lambda)$ for some integer $\ell$ and some real or complex number 
$\lambda$.  

A matrix is in {\em real Jordan normal form\/} if it is block diagonal 
and the matrix in each block on the diagonal is a real Jordan block, that is, 
either $J_\ell(\lambda)$ for some integer $\ell$ and some real number 
$\lambda$ or $\widehat{J}_\ell(\lambda)$ for some integer $\ell$ and some 
complex number $\lambda$. 
\end{definition} \index{Jordan block}
\index{Jordan normal form}


The main theorem about Jordan normal form is:
\begin{theorem}[Jordan normal form] \label{T:Jordan}
Let $A$ be an $n\times n$ matrix.  Then $A$ is 
similar\index{similar} to a Jordan normal
form matrix and to a real Jordan normal form matrix.
\end{theorem}

This theorem is proved by constructing a basis ${\cal V}$ for $\R^n$ so 
that the matrix $S\inv AS$ is in Jordan normal form, where $S$ is the
matrix whose columns consists of vectors in ${\cal V}$.  The algorithm for 
finding the basis ${\cal V}$ is complicated and is found in 
Appendix~\ref{S:Jordan}.  In this section we construct ${\cal V}$ only in the 
special and simpler case where each eigenvalue of $A$ is real and is 
associated with exactly one Jordan block.

More precisely, let $\lambda_1,\ldots,\lambda_s$ be the distinct 
eigenvalues of $A$ and let 
\[
A_j = A-\lambda_jI_n.
\]   
The eigenvectors corresponding to $\lambda_j$ are the vectors in the
null space\index{null space} of $A_j$ and the 
generalized eigenvectors are the vectors in 
the null space of $A_j^k$ for some $k$.  The dimension of the null space 
of $A_j$ is precisely the number of Jordan blocks\index{Jordan block} 
of $A$ associated to 
the eigenvalue $\lambda_j$.  So the assumption that we make here is 
\[
{\rm nullity}(A_j) = 1
\]
for $j = 1,\ldots,s$.

Let $k_j$ be the integer whose existence is specified by
Lemma~\ref{L:Jordan}.  Since, by assumption, there is only one Jordan block
associated with the eigenvalue $\lambda_j$, it follows that $k_j$ is the
algebraic multiplicity of the eigenvalue $\lambda_j$.

To find a basis in which the matrix $A$ is in Jordan normal 
form\index{Jordan normal form!basis for}, we proceed as follows.  First, let 
$w_{jk_j}$ be a vector in 
\[
\nulls(A_j^{k_j})\setmin\nulls(A_j^{k_j-1}).  
\]  
Define the vectors $w_{ji}$ by 
\begin{eqnarray*}
w_{j,k_j-1} & = & A_jw_{j,k_j} \\
& \vdots &  \\
w_{j,1} & = & A_jw_{j,2}.
\end{eqnarray*}
Second, when $\lambda_j$ is real, let the $k_j$ vectors $v_{ji}=w_{ji}$, and
when $\lambda_j$ is complex, let the $2k_j$ vectors $v_{ji}$ be defined by 
\begin{eqnarray*}
v_{j,2i-1} & = & \RE(w_{ji})\\
v_{j,2i} & = & \IM(w_{ji}). 
\end{eqnarray*}
Let ${\cal V}$ be the set of vectors $v_{ji}\in\R^n$.   We will show in
Appendix~\ref{S:Jordan} that the set ${\cal V}$ consists of $n$ vectors and 
is a basis of $\R^n$.  Let $S$ be the matrix whose columns are the
vectors in ${\cal V}$.  Then $S\inv AS$ is in Jordan normal form.

\subsubsection*{The Cayley Hamilton Theorem}

As a corollary of the Jordan normal form theorem, we prove the Cayley 
Hamilton theorem which states that a {\em square matrix satisfies its 
characteristic polynomial\/}.  More precisely:
\begin{theorem}[Cayley Hamilton] \label{T:CH} \index{Cayley Hamilton theorem}
Let $A$ be a square matrix\index{matrix!square} and let $p_A(\lambda)$ be 
its characteristic polynomial\index{characteristic polynomial}.  Then
\[
p_A(A) = 0.
\]
\end{theorem}

\begin{proof}  Let $A$ be an $n\times n$ matrix.  The characteristic polynomial of 
$A$ is
\[
p_A(\lambda)=\det(A-\lambda I_n).
\]
Suppose that $B=P\inv AP$ is a matrix similar to $A$.  
Theorem~\ref{T:similareigen} states that $p_B=p_A$.  Therefore
\[
p_B(B)= p_A(P\inv AP) = P\inv p_A(A)P.
\]
So if the Cayley Hamilton theorem holds for a matrix similar to $A$, 
then it is valid for the matrix $A$.  Moreover, using the Jordan normal form 
theorem, we may assume that $A$ is in Jordan normal form.  

Suppose that $A$ is block diagonal, that is 
\[
A = \mattwo{A_1}{0}{0}{A_2},
\]
where $A_1$ and $A_2$ are square matrices.  Then 
\[
p_A(\lambda) = p_{A_1}(\lambda)p_{A_2}(\lambda).
\]
This observation follows directly from Lemma~\ref{L:detblockdiag}.  Since
\[
A^k = \mattwoc{A_1^k}{0}{0}{A_2^k},
\]
it follows that 
\begin{align*}
p_A(A) &= \mattwoc{p_A(A_1)}{0}{0}{p_A(A_2)} \\
  &= \mattwoc{p_{A_1}(A_1)p_{A_2}(A_1)}{0}{0}{p_{A_1}(A_2)p_{A_2}(A_2)}.
\end{align*}
It now follows from this calculation that if the Cayley Hamilton theorem is 
valid for Jordan blocks, then $p_{A_1}(A_1)=0=p_{A_2}(A_2)$.  So $p_A(A)=0$ 
and the Cayley Hamilton theorem is valid for all matrices.

A direct calculation shows that Jordan blocks satisfy the 
Cayley Hamilton theorem.  To begin, suppose that the eigenvalue of the 
Jordan block is real.  Note that the characteristic polynomial of 
the Jordan block $J_n(\lambda_0)$ in \eqref{E:JnR} is $(\lambda-\lambda_0)^n$.
Indeed, $J_n(\lambda_0)-\lambda_0I_n$ is strictly upper triangular and 
$(J_n(\lambda_0)-\lambda_0I_n)^n=0$.  If $\lambda_0$ is complex, then either
repeat this calculation using the complex Jordan form or show by direct 
calculation that $(A-\lambda_0I_n)(A-\overline{\lambda_0}I_n)$ is strictly 
upper triangular when $A=\widehat{J}_n(\lambda_0)$ is the real Jordan form of 
the Jordan block in \eqref{E:JnC}.  \end{proof}




\subsection*{An Example}

Consider the $4\times 4$ matrix
\begin{matlabEquation} \label{e:Aexamp}
A=\left(\begin{array}{rrrr}      -147  &  -106      &   -66     &   -488\\
         604   &      432      &   271     &   1992\\
         621   &      448       &  279     &   2063\\
        -169    &    -122      &   -76     &   -562\end{array}\right).
\end{matlabEquation}
Using \Matlab we can compute the characteristic polynomial of $A$
by typing\index{\computer!poly}
\begin{verbatim}
poly(A)
\end{verbatim}
The output is
\begin{verbatim}
ans =
  1.0000   -2.0000  -15.0000   -0.0000   -0.0000
\end{verbatim}
Note that since $A$ is a matrix of integers we know that the coefficients 
of the characteristic polynomial of $A$ must be integers.   Thus the 
characteristic polynomial is exactly:
\[
p_A(\lambda) = \lambda^4-2\lambda^3-15\lambda^2 =
	\lambda^2(\lambda-5)(\lambda+3).
\]
So $\lambda_1=0$ is an eigenvalue of $A$ with 
algebraic multiplicity\index{multiplicity!algebraic} two
and $\lambda_2=5$ and $\lambda_3=-3$ are simple eigenvalues of 
multiplicity one.

We can find eigenvectors of $A$ corresponding to the simple
eigenvalues by typing
\begin{verbatim}
v2 = null(A-5*eye(4));
v3 = null(A+3*eye(4));
\end{verbatim}
At this stage we do not know how many linearly independent
eigenvectors have eigenvalue $0$.  There are either one or two linearly
independent eigenvectors and we determine which by typing {\tt null(A)} 
and obtaining
\begin{verbatim}
ans =
   0.1818
  -0.6365
  -0.7273
   0.1818
\end{verbatim}
So \Matlab tells us that there is just one linearly independent
eigenvector having $0$ as an eigenvalue.  There must be a generalized
eigenvector\index{eigenvector!generalized}
in $V_0$.  Indeed, the null space of $A^2$ is two dimensional
and this fact can be checked by typing
\begin{verbatim}
null2 = null(A^2)
\end{verbatim}
obtaining
\begin{verbatim}
null2 =
    0.3132   -0.0021
    0.2198   -0.9443
   -0.9238   -0.2295
   -0.0158    0.2358
\end{verbatim}
Choose one of these vectors, say the first vector, to be $v_{12}$ by typing
\begin{verbatim}
v12 = null2(:,1);
\end{verbatim}
Since the algebraic multiplicity\index{multiplicity!algebraic}
of the eigenvalue $0$ is two, we choose the 
fourth basis vector be $v_{11}=Av_{12}$.  In \Matlab we type 
\begin{verbatim}
v11 = A*v12
\end{verbatim}
obtaining
\begin{verbatim}
v11 =
   -0.6580
    2.3030
    2.6320
   -0.6580
\end{verbatim}
Since {\tt v11} is nonzero, we have found a basis for $V_0$.
We can now put the matrix $A$ in Jordan normal 
form\index{Jordan normal form}
by setting 
\begin{verbatim}
S = [v11 v12 v2 v3];
J = inv(S)*A*S
\end{verbatim}
to obtain
\begin{verbatim}
J = 
    0.0000    1.0000    0.0000   -0.0000
    0.0000   -0.0000    0.0000   -0.0000
   -0.0000   -0.0000    5.0000   -0.0000
    0.0000    0.0000    0.0000   -3.0000
\end{verbatim}

We have only discussed a Jordan normal form example when the eigenvalues
are real and multiple.  The case when the eigenvalues are complex and 
multiple\index{eigenvalue!complex!multiple} first occurs when $n=4$. 
A sample complex Jordan block when the 
matrix has algebraic multiplicity two eigenvalues $\sigma\pm i\tau$ of 
geometric multiplicity one is
\[
\left(\begin{array}{rrrr} \sigma & -\tau & 1 & 0 \\ 
\tau & \sigma & 0 & 1 \\ 0 & 0 & \sigma & -\tau \\
0 & 0 & \tau & \sigma \end{array}\right).
\]

\subsection*{Numerical Difficulties}

When a matrix has multiple eigenvalues, then numerical difficulties
can arise when using the \Matlab command {\tt eig(A)}, as we now explain.
\index{\computer!eig}

Let $p(\lambda)=\lambda^2$.  Solving $p(\lambda)=0$ is very easy --- in theory
--- as $\lambda=0$ is a double root of $p$.  Suppose, however, that 
we want to solve $p(\lambda)=0$ numerically.  Then, numerical errors 
will lead to solving the equation  
\[
\lambda^2 = \epsilon
\]
where $\epsilon$ is a small number.  Note that if $\epsilon>0$, the 
solutions are $\pm\sqrt{\epsilon}$; while if $\epsilon<0$, the solutions 
are $\pm i\sqrt{|\epsilon|}$.  Since numerical errors are machine 
dependent, $\epsilon$ can be of either sign.  The numerical process of
finding double roots of a characteristic polynomial (that is, double 
eigenvalues of a matrix) is similar to numerically solving the equation 
$\lambda^2=0$, as we shall see.

For example, on a
{\em Sun SPARCstation 10\/} using \Matlab version 4.2c, the eigenvalues 
of the $4\times 4$ matrix $A$ in \eqref{e:Aexamp} (in {\tt format long}) 
obtained using {\tt eig(A)} are:
\begin{verbatim}
ans = 
  5.00000000001021                    
 -0.00000000000007 + 0.00000023858927i
 -0.00000000000007 - 0.00000023858927i
 -3.00000000000993       
\end{verbatim}
That is, \Matlab computes two complex conjugate eigenvalues
\[
\pm 0.00000023858927i
\]
which corresponds to an $\epsilon$ of {\tt -5.692483975913288e-14}.
On a {\em IBM\/} compatible $486$ computer using \Matlab version 4.2
the same computation yields eigenvalues
\begin{verbatim}
ans=
 4.99999999999164
 0.00000057761008
-0.00000057760735
-2.99999999999434
\end{verbatim}
That is, on this computer \Matlab computes two real, near zero, 
eigenvalues 
\[ 
\pm 0.00000057761
\]
that corresponds to an $\epsilon$ of {\tt 3.336333121e-13}. These
errors are within round off error\index{round off error} in 
double precision\index{double precision} computation.

A consequence of these kinds of error, however, is that when a matrix
has multiple eigenvalues, we cannot use the command {\tt [V,D] = eig(A)} 
with confidence. On the {\em Sun SPARCstation\/}, this command yields 
a matrix 

{\scriptsize
\begin{verbatim}
V = 
  -0.1652             0.0000 - 0.1818i   0.0000 + 0.1818i  -0.1642          
   0.6726            -0.0001 + 0.6364i  -0.0001 - 0.6364i   0.6704          
   0.6962            -0.0001 + 0.7273i  -0.0001 - 0.7273i   0.6978          
  -0.1888             0.0000 - 0.1818i   0.0000 + 0.1818i  -0.1915          
\end{verbatim}
\normalsize}

that suggests that $A$ has two complex eigenvectors corresponding 
to the `complex' pair of near zero eigenvalues.  The {\em IBM\/} 
compatible yields the matrix
\begin{verbatim}
V = 
   -0.1652    0.1818   -0.1818   -0.1642
    0.6726   -0.6364    0.6364    0.6704
    0.6962   -0.7273    0.7273    0.6978
   -0.1888    0.1818   -0.1818   -0.1915
\end{verbatim}
indicating that \Matlab has found two real eigenvectors corresponding 
to the near zero real eigenvalues.  Note that the two eigenvectors
corresponding to the eigenvalues $5$ and $-3$ are correct on both 
computers.     



\includeexercises



\end{document}


