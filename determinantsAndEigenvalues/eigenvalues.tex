\documentclass{ximera}

\input{../preamble.tex}

\title{Eigenvalues and Eigenvectors}

\begin{document}
\begin{abstract}
\end{abstract}
\maketitle

 \label{S:eig} 
 
In this section we discuss how to find eigenvalues for an
$n\times n$ matrix $A$.  This discussion parallels the
discussion for $2\times 2$ matrices given in
Section~\ref{S:evchp}.  As we noted in that section, $\lambda$
is a real eigenvalue of $A$ if there exists a nonzero
eigenvector $v$ such that
\begin{equation}  \label{e:eigen}
Av = \lambda v.
\end{equation}
It follows that the matrix $A-\lambda I_n$ is 
singular\index{singular} since
\[
(A-\lambda I_n)v = 0.
\]
Theorem~\ref{T:detandinv} implies that 
\[
\det(A-\lambda I_n) = 0.
\]
With these observations in mind, we can make the following definition.
\begin{definition}   \label{D:charpoly}
Let $A$ be an $n\times n$ matrix.  The {\em characteristic polynomial\/} 
of $A$ is:
\[
p_A(\lambda) = \det(A-\lambda I_n).
\]  \index{characteristic polynomial}
\end{definition}

In Theorem~\ref{T:charpolyn} we show that $p_A(\lambda)$ is indeed a 
polynomial of degree $n$ in $\lambda$.  Note here that the roots of 
\index{characteristic polynomial!roots}
$p_A$ are the {\em eigenvalues\/} of $A$. As we
discussed, the real eigenvalues\index{eigenvalue!real} 
of $A$ are roots of the
characteristic polynomial.  \index{eigenvalue} Conversely, if
$\lambda$ is a real root of $p_A$, then
Theorem~\ref{T:detandinv} states that the matrix $A-\lambda I_n$
is singular and therefore that there exists a nonzero vector $v$
such that \eqref{e:eigen} is satisfied.  Similarly, by using this
extended algebraic definition of eigenvalues we allow the
possibility of complex eigenvalues\index{eigenvalue!complex}.  
The complex analog of
Theorem~\ref{T:detandinv} shows that if $\lambda$ is a complex
eigenvalue, then there exists a nonzero complex $n$-vector $v$
such that \eqref{e:eigen} is satisfied.

\begin{example} \label{E:triangular}
Let $A$ be an $n\times n$ lower triangular matrix.  Then the
diagonal entries are the eigenvalues of $A$.  {\rm We verify
this statement as follows.  \index{matrix!lower triangular}
\[
A-\lambda I_n = \left(\begin{array}{ccc} a_{11}-\lambda &  & 0 \\
 &  \ddots &  \\ (*) & & a_{nn}-\lambda \end{array}\right).
\]
Since the determinant of a triangular matrix is the product of
the diagonal entries, it follows that 
\begin{equation}  \label{e:triangpoly}
p_A(\lambda) = (a_{11}-\lambda)\cdots(a_{nn}-\lambda),
\end{equation}
and hence that the diagonal entries of $A$ are roots of the 
characteristic polynomial. A similar argument works if $A$ is
upper triangular.}
\end{example}  
 
It follows from \eqref{e:triangpoly} that the characteristic
polynomial of a triangular matrix
\index{characteristic polynomial!of triangular matrices}
is a polynomial of degree $n$ and that
\begin{equation}  \label{e:leadingterm}
p_A(\lambda) = (-1)^n \lambda^n + b_{n-1}\lambda^{n-1} + \cdots +b_0.
\end{equation}
for some real constants $b_0, \ldots, b_{n-1}$. In fact, this
statement is true in general.  

\begin{theorem}  \label{T:charpolyn}
Let $A$ be an $n\times n$ matrix.  Then $p_A$ is a polynomial of
degree $n$ of the form \eqref{e:leadingterm}.
\end{theorem} \index{characteristic polynomial}

\begin{proof} Let $C$ be an $n\times n$ matrix whose entries have
the form $c_{ij}+d_{ij}\lambda$.  Then $\det(C)$ is a polynomial
in $\lambda$ of degree at most $n$.  We verify this statement by
induction. It is easily verified when $n=1$, since then
$C=(c+d\lambda)$ for some real numbers $c$ and $d$. Then
$\det(C)=c+d\lambda$ which is a polynomial of degree at most
one.  (It may have degree zero, if $d=0$.) So assume that this
statement is true for $(n-1)\times (n-1)$ matrices. Recall from
\eqref{e:inductdet} that 
\[
\det(C) = (c_{11}+d_{11}\lambda)\det(C_{11})
+\cdots+(-1)^{n+1}(c_{1n}+d_{1n}\lambda)\det(C_{1n}).
\]    
By induction each of the determinants $C_{1j}$ is a polynomial
of degree at most $n-1$.  It follows that multiplication by
$c_{1j}+d_{1j}\lambda$ yields a polynomial of degree at most $n$
in $\lambda$.  Since the sum of polynomials of degree at most
$n$ is a polynomial of degree at most $n$, we have verified our
assertion.

Since $A-\lambda I_n$ is a matrix whose entries have the
desired form, it follows that $p_A(\lambda)$ is a polynomial of
degree at most $n$ in $\lambda$.  To complete the proof of this
theorem we need to show that the coefficient of $\lambda^n$ is
$(-1)^n$.  Again, we verify this statement by induction.  This
statement is easily verified for $1\times 1$ matrices --- we
assume that it is true for $(n-1)\times (n-1)$ matrices.  Again
use \eqref{e:inductdet} to compute
\begin{align*}
\det(A-\lambda I_n) &= (a_{11}-\lambda)\det(B_{11})-a_{12}\det(B_{12}) 
                      +\cdots \\
  &\quad +(-1)^{n+1}a_{1n}\det(B_{1n}).
\end{align*}
where $B_{1j}$ are the cofactor \index{cofactor} matrices of
$A-\lambda I_n$.  Using our previous observation all of the
terms $\det(B_{1j})$ are polynomials of degree at most $n-1$.
Thus, in this expansion, the only term that can contribute a
term of degree $n$ is:
\[
-\lambda\det(B_{11}).
\]
Note that the cofactor matrix $B_{11}$ is the $(n-1)\times
(n-1)$ matrix
\[
B_{11} = A_{11} -\lambda I_{n-1},
\]
where $A_{11}$ is the first cofactor matrix of the matrix $A$.
By induction, $\det(B_{11})$ is a polynomial of degree $n-1$ with
leading term $(-1)^{n-1}\lambda^{n-1}$.  Multiplying this
polynomial by $-\lambda$ yields a polynomial of degree $n$ with
the correct leading term.  \end{proof}

\subsection*{General Properties of Eigenvalues}

The {\em fundamental theorem of algebra\/} \index{fundamental
theorem of algebra} states that every polynomial of degree $n$
has exactly $n$ roots (counting multiplicity).  For example, the 
quadratic formula shows that 
every quadratic polynomial has exactly two roots.  In general, the proof 
of the fundamental theorem is not easy and is certainly beyond the 
limits of this course.  Indeed, the difficulty in proving the {\em
fundamental theorem of algebra\/} is in proving that a
polynomial $p(\lambda)$ of degree $n>0$ has one (complex) root.
Suppose that $\lambda_0$ is a root of $p(\lambda)$; that is,
suppose that $p(\lambda_0)=0$. Then it follows that
\begin{equation}  \label{e:factoring}
p(\lambda) = (\lambda-\lambda_0)q(\lambda)
\end{equation}
for some polynomial $q$ of degree $n-1$.  So once we know that
$p$ has a root, then we can argue by induction to prove that $p$
has $n$ roots.  A linear algebra proof of \eqref{e:factoring} is 
sketched in Exercise~\ref{c8.2.a1}.

Recall that a polynomial need not have any real roots. For
example, the polynomial $p(\lambda)=\lambda^2+1$ has no real
roots, since $p(\lambda)> 0$ for all real $\lambda$.  This
polynomial does have two complex roots $\pm i =\pm\sqrt{-1}$.  

However, a polynomial with real coefficients has either real
roots or complex roots that come in complex conjugate pairs.  To
verify this statement, we need to show that if $\lambda_0$ is a
complex root of $p(\lambda)$, then so is $\overline{\lambda_0}$.
We claim that 
\[
p(\overline{\lambda})=\overline{p(\lambda)}.
\]
To verify this point, suppose that
\[
p(\lambda) = c_n\lambda^n + c_{n-1}\lambda^{n-1} + \cdots + c_0,
\]
where each $c_j\in\R$.  Then
\begin{align*}
\overline{p(\lambda)} 
&=\overline{c_n\lambda^n + c_{n-1}\lambda^{n-1} + \cdots + c_0}  \\
&= c_n\overline{\lambda}^n + c_{n-1}\overline{\lambda}^{n-1} + \cdots + c_0 \\
&= p(\overline{\lambda})
\end{align*}
If $\lambda_0$ is a root of $p(\lambda)$, then
\[
p(\overline{\lambda_0}) = \overline{p(\lambda_0)}=\overline{0}=0.
\]
Hence $\overline{\lambda_0}$ is also a root of $p$.

It follows that 
\begin{theorem}  \label{T:eigens}
Every (real) $n\times n$ matrix $A$ has exactly $n$ eigenvalues
$\lambda_1,\ldots,\lambda_n$.  These eigenvalues are either real
or complex conjugate pairs.  Moreover,
\begin{enumerate}
\item[(a)] $p_A(\lambda) = (\lambda_1-\lambda)\cdots(\lambda_n-\lambda)$,
\item[(b)] $\det(A) = \lambda_1\cdots\lambda_n$.
\end{enumerate}
\end{theorem} \index{eigenvalue}\index{eigenvalue!existence}
\index{determinant}

\begin{proof} Since the characteristic polynomial $p_A$ is a polynomial
of degree $n$ with real coefficients, the first part of the
theorem follows from the preceding discussion. In particular, it follows
from \eqref{e:factoring} that 
\[
p_A(\lambda) = c(\lambda_1-\lambda)\cdots(\lambda_n-\lambda),
\]
for some constant $c$.  Formula \eqref{e:leadingterm} implies that
$c=1$ --- which proves (a).  Since $p_A(\lambda)=\det(A-\lambda I_n)$, 
it follows that $p_A(0)=\det(A)$.  Thus (a) implies 
that $p_A(0)=\lambda_1\cdots\lambda_n$, thus proving (b).  \end{proof}

The eigenvalues of a matrix do not have to be different.  For
example, consider the extreme case of a strictly triangular
matrix $A$.  Example~\ref{E:triangular} shows that all of the
eigenvalues of $A$ are zero. 

We now discuss certain properties of eigenvalues.  
\begin{corollary}  \label{C:eig=0}
Let $A$ be an $n\times n$ matrix. Then $A$ is invertible if and
only if zero is {\em not\/} an eigenvalue of $A$.
\end{corollary}  \index{invertible}

\begin{proof} The proof follows from Theorem~\ref{T:detandinv} and
Theorem~\ref{T:eigens}(b). \end{proof} 

\begin{lemma} 
Let $A$ be a singular $n\times n$ matrix.  Then the null space of $A$
is the span of all eigenvectors whose associated eigenvalue is zero.
\end{lemma} \index{null space}\index{singular}

\begin{proof} An eigenvector $v$ of $A$ has eigenvalue zero if and only
if 
\[
Av=0.
\]
This statement is valid if and only if $v$ is in the null space
of $A$. \end{proof}



\begin{theorem}  \label{T:inveig}
Let $A$ be an invertible $n\times n$ matrix with eigenvalues 
$\lambda_1,\cdots,\lambda_n$.  Then the eigenvalues of 
$A\inv$ are $\lambda_1\inv,\cdots,\lambda_n\inv$.
\end{theorem}  \index{invertible}\index{eigenvalue!of inverse}

\begin{proof}  We claim that  
\[
p_A(\lambda) = (-1)^n \det(A) \lambda^n p_{A\inv}(\frac{1}{\lambda}).
\]
It then follows that $\frac{1}{\lambda}$ is an eigenvalue for
$A\inv$ for each eigenvalue $\lambda$ of $A$.  This makes sense,
since the eigenvalues of $A$ are nonzero. 

Compute:
\begin{eqnarray*}
(-1)^n \det(A) \lambda^n p_{A\inv}(\frac{1}{\lambda}) & = &
 (-\lambda)^n \det(A)\det(A\inv-\frac{1}{\lambda}I_n) \\
& = & \det(-\lambda A)\det(A\inv-\frac{1}{\lambda}I_n)\\
& = & \det(-\lambda A (A\inv-\frac{1}{\lambda}I_n))\\
& = & \det(A-\lambda I_n) \\
& = & p_A(\lambda),
\end{eqnarray*}
which verifies the claim.  \end{proof}

\begin{theorem}  \label{T:similareigen}
Let $A$ and $B$ be similar $n\times n$ matrices.  Then
\[
p_A = p_B,
\]
and hence the eigenvalues of $A$ and $B$ are identical.
\end{theorem}  \index{similar}

\begin{proof}  Since $B$ and $A$ are similar, there exists an 
invertible $n\times n$ matrix $S$ such that $B=S\inv AS$.  It 
follows that 
\begin{align*}
\det(B-\lambda I_n) &= \det(S\inv AS-\lambda I_n) \\
&= \det(S\inv(A-\lambda I_n)S) = \det(A-\lambda I_n),
\end{align*}
which verifies that $p_A=p_B$.  \end{proof}

Recall that the {\em trace\/}\index{trace} of an 
$n\times n$ matrix $A$ is
the sum of the diagonal entries of $A$; that is
\[
{\rm tr}(A) = a_{11} +\cdots+ a_{nn}.
\]
We state without proof the following theorem:
\begin{theorem} \label{T:tracen}
Let $A$ be an $n\times n$ matrix with eigenvalues
$\lambda_1,\ldots,\lambda_n$.  Then
\[
{\rm tr}(A) = \lambda_1 + \cdots + \lambda_n.
\]
\end{theorem}

It follows from Theorem~\ref{T:similareigen} that the traces of
similar matrices are equal.

\begin{definition} \label{D:eigenspace} {\rm
Associated with each eigenvalue $\lambda$ of the square 
matrix $A$  is a vector subspace of $\R^n$. This subspace, called the 
{\em eigenspace} of $\lambda$, is the {\em null space($A-\lambda I_n$)} and 
consists of all eigenvectors associated with the eigenvalue $\lambda$.}
\end{definition}

\subsection*{\Matlab Calculations}

The commands for computing characteristic polynomials and
eigenvalues of square matrices are straightforward in \Matlab.
In particular, for an $n\times n$ matrix $A$, the \Matlab command
 {\tt poly(A)} returns the coefficients of $(-1)^np_A(\lambda)$.

For example, reload the $4\times 4$ matrix $A$ of \eqref{e:A4x4}
by typing {\tt e7\_1\_12}.  The characteristic polynomial of $A$ is
found by typing
\begin{verbatim}
poly(A)
\end{verbatim} \index{\computer!poly}
to obtain
{\small
\begin{verbatim}
ans =
    1.0000   -5.0000   15.0000  -10.0000  -46.0000
\end{verbatim} 
}
Thus the characteristic polynomial of $A$ is:
\[
p_A(\lambda) = \lambda^4 -5\lambda^3+15\lambda^2-10\lambda-46.
\]
The eigenvalues of $A$ are found by typing {\tt eig(A)} and
obtaining
\begin{verbatim}
ans =
  -1.2224 + 0.0000i
   1.6605 + 3.1958i
   1.6605 - 3.1958i
   2.9014 + 0.0000i
\end{verbatim}
Thus $A$ has two real eigenvalues and one complex conjugate pair
of eigenvalues.  Note that \Matlab has preprogrammed not only
the algorithm for finding the characteristic polynomial, but
also numerical routines for finding the roots of the
characteristic polynomial.

The trace of $A$ is found by typing {\tt trace(A)} and obtaining
\begin{verbatim} 
ans =
     5
\end{verbatim} \index{\computer!trace}

Using the \Matlab command {\tt sum} we can verify the statement
of Theorem~\ref{T:tracen}.  Indeed {\tt sum(v)} computes the sum 
of the components of the vector $v$ and typing
\begin{verbatim}
sum(eig(A))
\end{verbatim}
\index{\computer!sum}
we obtain the answer {\tt 5.0000}, as expected.



\includeexercises



\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../linearAlgebra"
%%% End:
