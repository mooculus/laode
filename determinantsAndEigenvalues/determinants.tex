\documentclass{ximera}

\input{../preamble.tex}

\title{Determinants}

\begin{document}
\begin{abstract}
\end{abstract}
\maketitle

 
\label{S:det}
 
There are several equivalent ways to introduce determinants --- none of which 
are easily motivated.  We prefer to define determinants through the properties 
they satisfy rather than by formula.  These properties actually enable us to 
compute determinants of $n\times n$ matrices where $n>3$, which further 
justifies the approach. Later on, we will give an inductive formula 
\eqref{e:inductdet} for computing the determinant. 
 
\begin{definition}  \label{D:determinants}
A {\em determinant function} of a square $n\times n$ matrix $A$ is a real
number $D(A)$ that satisfies three properties:
\begin{itemize} 
\item[(a)]  If $A=(a_{ij})$ is lower 
triangular\index{matrix!lower triangular}, then
 $D(A)$ is the product of the diagonal entries;
that is,
\[
D(A) = a_{11}\cdot\cdots\cdot a_{nn}.
\]
\item[(b)]  $D(A^t)=D(A)$\index{matrix!transpose}.
\item[(c)]  Let $B$ be an $n\times n$ matrix.  
Then
\begin{equation} \label{e:detproduct}
D(AB) = D(A) D(B).
\end{equation}
\end{itemize}
\end{definition} \index{determinant}

\begin{theorem}  \label{T:determinants}
There exists a unique determinant function $\det$ satisfying the three
properties of Definition~\ref{D:determinants}.
\end{theorem}\index{determinant!uniqueness}

We will show that it is possible to compute the determinant of
any $n\times n$ matrix using Definition~\ref{D:determinants}.
Here we present a few examples:

\begin{lemma} \label{L:detbasic}
Let $A$ be an $n\times n$ matrix.
\begin{itemize}
\item[(a)]   Let $c\in\R$ be a scalar.  Then $D(cA) = c^n D(A)$.
\item[(b)] If all of the entries in either a row or a column of $A$ are 
zero, then $D(A)=0$.
\end{itemize}
\end{lemma}

\begin{proof}  (a) Note that Definition~\ref{D:determinants}(a) implies that 
$D(cI_n)=c^n$.  It follows from \eqref{e:detproduct} that
\[
D(cA) = D(cI_n A) = D(cI_n)D(A) = c^n D(A).
\]

(b)  Definition~\ref{D:determinants}(b) implies that it suffices to prove 
this assertion when one row of $A$ is zero.  Suppose that the $i^{th}$ row 
of $A$ is zero.  Let $J$ be an $n\times n$ 
diagonal matrix with a $1$ in every diagonal entry except the $i^{th}$ 
diagonal entry which is $0$.  A matrix calculation shows that $JA=A$. 
It follows from Definition~\ref{D:determinants}(a) that $D(J)=0$ and 
from \eqref{e:detproduct} that $D(A)=0$.  
\end{proof} 



\subsection*{Determinants of $2\times 2$ Matrices}
\index{determinant!of $2\times 2$ matrices} 
 
Before discussing how to compute determinants, we discuss the
special case of $2\times 2$ matrices.  Recall from \eqref{D:determinant} of 
Section~\ref{S:det2x2} that when 
\[
A=\left(\begin{array}{cc} a & b\\c & d \end{array}\right)
\]
we defined 
\begin{equation}  \label{e:determinantn=2}
\det(A)=ad-bc.
\end{equation}
We check that \eqref{e:determinantn=2} satisfies the three
properties in Definition~\ref{D:determinants}.  Observe that when
$A$ is lower triangular, then $b=0$ and $\det(A)=ad$.  So (a) is
satisfied.  It is straightforward to verify (b).  We already
verified (c) in Chapter~\ref{chap:matrices}, Proposition~\ref{propdet}.

It is less obvious perhaps --- but true nonetheless --- that the
three properties of $D(A)$ actually force the determinant of
$2\times 2$ matrices to be given by formula
\eqref{e:determinantn=2}. We begin by showing that
Definition~\ref{D:determinants} implies that 
\begin{equation}  \label{e:detswap}
D\left(\begin{array}{cc} 0 & 1\\1 & 0 \end{array}\right)=-1.
\end{equation}
We verify \eqref{e:detswap} by observing that 
\begin{equation*}
  \left(\begin{array}{cc} 0 & 1\\1 & 0 \end{array}\right)
\end{equation*}
equals
\begin{equation}\label{e:swapdecomp}
\left(\begin{array}{cr} 1 & -1\\0 & 1 \end{array}\right)
\left(\begin{array}{cc} 1 & 0\\1 & 1 \end{array}\right)
\left(\begin{array}{cr} 1 & 0\\0 & -1 \end{array}\right)
\left(\begin{array}{cc} 1 & 1\\0 & 1 \end{array}\right).
\end{equation}
Hence property (c), (a) and (b) imply that
\[
D \left(\begin{array}{cc} 0 & 1\\1 & 0 \end{array}\right) =
1\cdot 1\cdot (-1) \cdot 1 = -1.
\]
It is helpful to interpret the matrices in \eqref{e:swapdecomp} as
elementary row operations\index{elementary row operations}.  
Then \eqref{e:swapdecomp} states that
swapping two rows in a $2\times 2$ matrix is the same as
performing the following row operations in order:
\begin{itemize}
\item        add the $2^{nd}$ row to the  $1^{st}$ row;
\item        multiply the $2^{nd}$ row by $-1$; 
\item        add the $1^{st}$ row to the $2^{nd}$ row; and  
\item        subtract the $2^{nd}$ row from the $1^{st}$ row.
\end{itemize}
 
Suppose that $d\neq 0$.  Then 
\[
A=\left(\begin{array}{cc} a & b\\c & d \end{array}\right) =
\left(\begin{array}{cc} 1 & \frac{b}{d}\\0 & 1 \end{array}\right)
\left(\begin{array}{cc} \frac{ad-bc}{d} & 0\\c & d
\end{array}\right). 
\]
It follows from properties (c), (b) and (a) that
\[
D(A) = \frac{ad-bc}{d}d = ad-bc = \det(A),
\]
as claimed.
 
Now suppose that $d=0$ and note that 
\[
A=\left(\begin{array}{cc} a & b\\c & 0 \end{array}\right) = 
\left(\begin{array}{cc} 0 & 1\\1 & 0 \end{array}\right)
\left(\begin{array}{cc} c & 0\\a & b \end{array}\right).
\]
Using \eqref{e:detswap} we see that 
\[
D(A) = -D \left(\begin{array}{cc} c & 0\\a & b
\end{array}\right) = -bc = \det(A),
\]
as desired. 
 
We have verified that the only possible determinant function for
$2\times 2$ matrices is the determinant function defined by
\eqref{e:determinantn=2}. 
 



\subsection*{Row Operations are Invertible Matrices} 
\index{elementary row operations}

\begin{proposition}  \label{P:ERO}
Let $A$ and $B$ be $m\times n$ matrices where $B$ is obtained from $A$ by
a single elementary row operation.  Then there exists an invertible 
$m\times m$ matrix $R$ such that $B=RA$.
\end{proposition} 

\begin{proof}  First consider multiplying the $j^{th}$ row of $A$ by the
nonzero constant $c$.  Let $R$ be the diagonal matrix whose
$j^{th}$ entry on the diagonal is $c$ and whose other diagonal 
entries are $1$.  Then the matrix $RA$ is just the matrix obtained from 
$A$ by multiplying the $j^{th}$ row of $A$ by $c$.  Note that $R$ is
invertible when $c\neq 0$ and that $R\inv$ is the diagonal
matrix whose $j^{th}$ entry is $\frac{1}{c}$ and whose other
diagonal entries are $1$.  For example
\[
\left(\begin{array}{ccc} 1 & 0 & 0\\ 0 & 1 & 0 \\ 0 & 0 & 2\end{array}\right)
\left(\begin{array}{ccc} a_{11} & a_{12} & a_{13}\\ a_{21} & a_{22} & a_{23}
 \\ a_{31} & a_{32} & a_{33} \end{array}\right) =
\left(\begin{array}{ccc} a_{11} & a_{12} & a_{13}\\ a_{21} & a_{22} & a_{23}
 \\ 2a_{31} & 2a_{32} & 2a_{33} \end{array}\right),
\]
multiplies the $3^{rd}$ row by $2$.

Next we show that the elementary row operation that swaps two
rows may also be thought of as matrix multiplication.  Let
$R=(r_{kl})$ be the matrix that deviates from the identity matrix
by changing in the four entries:
\begin{eqnarray*}
r_{ii} & = & 0 \\
r_{jj} & = & 0\\
r_{ij} & = & 1 \\
r_{ji} & = & 1
\end{eqnarray*}
A calculation shows that $RA$ is the matrix obtained from $A$ by
swapping the $i^{th}$ and $j^{th}$ rows.  For example,
\[
\left(\begin{array}{ccc} 0 & 0 & 1\\ 0 & 1 & 0 \\ 1 & 0 & 0\end{array}\right)
\left(\begin{array}{ccc} a_{11} & a_{12} & a_{13}\\ a_{21} & a_{22} & a_{23}
 \\ a_{31} & a_{32} & a_{33} \end{array}\right) =
\left(\begin{array}{ccc} a_{31} & a_{32} & a_{33}\\ a_{21} & a_{22} & a_{23}
 \\  a_{11} & a_{12} & a_{13} \end{array}\right),
\]
which swaps the $1^{st}$ and $3^{rd}$ rows.  Another calculation
shows that $R^2=I_n$ and hence that $R$ is invertible since
$R\inv=R$.  

Finally, we claim that adding $c$ times the $i^{th}$ row of $A$
to the $j^{th}$ row of $A$ can be viewed as matrix
multiplication.  Let $E_{k\ell}$ be the matrix all of whose
entries are $0$ except for the entry in the $k^{th}$ row and
$\ell^{th}$ column which is $1$.  Then $R=I_n+cE_{ij}$ has the
property that $RA$ is the matrix obtained by adding $c$ times
the $j^{th}$ row of $A$ to the $i^{th}$ row.  We can verify by
multiplication that $R$ is invertible and that
$R\inv=I_n-cE_{ij}$.  More precisely,
\[
(I_n+cE_{ij})(I_n-cE_{ij})=I_n+cE_{ij}-cE_{ij}-c^2E_{ij}^2=I_n,
\]
since $E_{ij}^2 = O$ for $i\not= j$.  For example,
\[
(I_3 + 5E_{12})A  =  \left(\begin{array}{ccc} 1 & 5 & 0\\ 0 & 1 & 0 \\ 0 & 0 & 1\end{array}\right)
\left(\begin{array}{ccc} a_{11} & a_{12} & a_{13}\\ a_{21} & a_{22} & a_{23}
 \\ a_{31} & a_{32} & a_{33} \end{array}\right) 
 \]
 \[
=\left(\begin{array}{ccc} a_{11}+5a_{21} & a_{12}+5a_{22} & a_{13}+5a_{23} \\ 
a_{21} & a_{22} & a_{23} \\ a_{31} & a_{32} & a_{33} \end{array}\right),
\]
adds $5$ times the $2^{nd}$ row to the $1^{st}$ row.   \end{proof}

\subsubsection*{Determinants of Elementary Row Matrices}

\begin{lemma}  \label{L:detelemrowmat}
\begin{itemize}
\item[(a)] The determinant of the matrix that adds a multiple
of one row to another is $1$.
\item[(b)] The determinant of the matrix that multiplies one
row by $c$ is $c$.
\item[(c)] The determinant of a swap matrix is $-1$.
\end{itemize}
\end{lemma}  \index{determinant}
 
\begin{proof} 
(a) The matrix that adds a multiple of one row to another is
triangular (either upper or lower) and has $1$'s on the
diagonal.  Thus property (a) in Definition~\ref{D:determinants}
implies that the determinants of these matrices are equal to $1$.
 
(b) The matrix that multiplies the $i^{th}$ row by $c\neq 0$ is a 
diagonal matrix all of whose diagonal entries are $1$
except for $a_{ii}=c$.  Again property (a) implies that the
determinant of this matrix is $c\neq 0$. 

(c) The matrix that swaps the $i^{th}$ row with the $j^{th}$ row is 
the product of four matrices of types (a) and (b).  To see this 
let $A$ be an $n\times n$ matrix whose $i^{th}$ row vector is 
$a_i$.  Then perform the following four operations in order:

\begin{center}
\begin{tabular}{lllc}
Operation & \multicolumn{2}{c}{Result} & Matrix\\
\hline
Add $r_i$ to $r_j$ & $r_i = a_i$ & $r_j = a_i + a_j$ & $B_1$ \\
Multiply $r_i$ by $-1$  & $r_j = -a_i$ & $r_j = a_i + a_j$ & $B_2$\\
Add $r_j$ to $r_i$  &  $r_i =  a_j$ &  $r_j = a_i + a_j$ &  $B_3$ \\
Subtract  $r_i$ from $r_j$ & $r_i = a_j$ &  $r_j = a_i$ & $B_4$\\ 
\end{tabular}
\end{center}
It follows that the swap matrix equals $B_4 B_3 B_2 B_1$.  Therefore 
\[
\det(\mbox{swap})  = \det(B_4) \det(B_3) \det(B_2) \det(B_1)
\]
\[ 
 =  (1)(-1)(1)(1) = -1.
\]
\end{proof}


\subsection*{Computation of Determinants}
\index{determinant!computation}

We now show how to compute the determinant of any $n\times n$ matrix $A$ 
using elementary row operations and Definition~\ref{D:determinants}.  It 
follows from Proposition~\ref{P:ERO} that every elementary row operation 
on $A$ may be performed by premultiplying $A$ by an elementary row matrix. 

For each matrix $A$ there is a unique 
reduced echelon form\index{echelon form!reduced} matrix
$E$ and a sequence of elementary row matrices $R_1\ldots R_s$
such that \index{elementary row operations}
\begin{equation}  \label{e:rowreduction}
E = R_s\cdots R_1A.
\end{equation}
It follows from Definition~\ref{D:determinants}(c) that we can
compute the determinant of $A$ once we know the determinants of
reduced echelon form matrices and the determinants of elementary
row matrices.  In particular
\begin{equation}  \label{e:detformula}
D(A) = \frac{D(E)}{D(R_1)\cdots D(R_s)}.
\end{equation}

It is easy to compute the determinant of any matrix in reduced echelon 
form using Definition~\ref{D:determinants}(a) since all reduced echelon 
form $n\times n$ matrices are upper triangular.  Lemma~\ref{L:detelemrowmat}  
tells us how to compute the determinants of elementary row matrices.  This 
discussion proves: 
\begin{proposition}
If a determinant function exists for $n\times n$ matrices, then it is unique. 
We call the unique determinant function $\det$. 
\index{determinant!uniqueness}
\end{proposition}

We still need to show that determinant functions exist when $n>2$.  More 
precisely, we know that the reduced echelon form matrix $E$ is uniquely 
defined from $A$ (Chapter~\ref{lineq}, Theorem~\ref{uniquerowechelon}), but 
there is more than one way to perform elementary row operations on $A$ to 
get to $E$.  Thus, we can write $A$ in the form \eqref{e:detformula} in many 
different ways, and these different decompositions might lead to different 
values for $\det A$.  (They don't.)

\subsubsection*{An Example of Determinants by Row Reduction}
\index{row!reduction}

As a practical matter we row reduce a square matrix $A$ by 
premultiplying $A$ by an elementary row matrix $R_j$.  Thus 
\begin{equation} \label{e:pracdet}
\det(A) = \frac{1}{\det(R_j)} \det (R_j A).
\end{equation}
We use this approach to compute the determinant of the 
$4\times 4$ matrix 
\[
A = \left(\begin{array}{rrrr} 0 & 2 & 10 & -2 \\ 1 & 2 & 4 & 0\\
1 & 6 & 1 & -2 \\ 2 & 1 & 1 & 0 \end{array}\right).
\]
The idea is to use \eqref{e:pracdet} to keep track of the 
determinant while row reducing $A$ to upper triangular form. 
For instance, swapping rows changes the sign of the determinant; 
so
\[
\det(A) = -\det\left(\begin{array}{rrrr} 1 & 2 & 4 & 0\\ 0 & 2 & 10 & -2 \\
1 & 6 & 1 & -2 \\ 2 & 1 & 1 & 0 \end{array}\right).
\]
Adding multiples of one row to another leaves the determinant
unchanged; so
\[
\det(A) = -\det\left(\begin{array}{rrrr} 1 & 2 & 4 & 0\\ 0 & 2 & 10 & -2 \\
0 & 4 & -3 & -2 \\ 0 & -3 & -7 & 0 \end{array}\right).
\]
Multiplying a row by a scalar $c$ corresponds to an elementary 
row matrix whose determinant is $c$.  To make sure that we do not 
change the value of $\det(A)$, we have to divide the determinant by 
$c$ as we multiply a row of $A$ by $c$. So as we divide the second 
row of the matrix by $2$, we multiply the whole result by $2$, obtaining   
\[
\det(A) = -2\det\left(\begin{array}{rrrr} 1 & 2 & 4 & 0\\ 0 & 1 & 5 & -1 \\
0 & 4 & -3 & -2 \\ 0 & -3 & -7 & 0 \end{array}\right).
\] 
We continue row reduction by zeroing out the last two entries in
the $2^{nd}$ column, obtaining
\begin{align*}
\det(A) &= -2\det\left(\begin{array}{rrrr} 1 & 2 & 4 & 0\\ 0 & 1 & 5 & -1 \\
0 & 0 & -23 & 2 \\ 0 & 0 & 8 & -3 \end{array}\right) \\
&= 46\det\left(\begin{array}{rrrr} 1 & 2 & 4 & 0\\ 0 & 1 & 5 & -1 \\
0 & 0 & 1 & -\frac{2}{23} \\ 0 & 0 & 8 & -3 \end{array}\right).
\end{align*}
Thus
\[
\det(A) = 46\det\left(\begin{array}{rrrr} 1 & 2 & 4 & 0\\ 0 & 1 & 5 & -1 \\
0 & 0 & 1 & -\frac{2}{23} \\ 0 & 0 & 0 & -\frac{53}{23} \end{array}\right)
= -106.
\]  

\subsubsection*{Determinants and Inverses}
\index{inverse}

We end this subsection with an important observation about the
determinant function.  This observation generalizes to dimension
$n$ Corollary~\ref{C:2x2invert} of Chapter~\ref{chap:matrices}. 
\begin{theorem}  \label{T:detandinv}
An $n\times n$ matrix $A$ is invertible if and only if $\det(A)\neq 0$.
Moreover, if $A\inv$ exists, then 
\begin{equation}  \label{E:detinv}
\det A\inv = \frac{1}{\det A}.
\end{equation}
\end{theorem} \index{invertible}
 
\begin{proof}  If $A$ is invertible, then 
\[
\det(A)\det(A\inv) = \det(AA\inv) = \det(I_n) =1.
\]
Thus $\det(A)\neq 0$ and \eqref{E:detinv} is valid. In particular, the 
determinants of elementary row matrices are nonzero, since they are all
invertible. (This point was proved by direct calculation in
Lemma~\ref{L:detelemrowmat}.)
 
If $A$ is singular, then $A$ is row equivalent to a non-identity
reduced echelon form matrix $E$ whose determinant is zero (since
$E$ is upper triangular and its last diagonal entry is zero).
So it follows from
\eqref{e:rowreduction} that 
\[
0=\det(E) = \det(R_1)\cdots\det(R_s)\det(A)
\]
Since $\det(R_j)\neq 0$, it follows that $\det(A)=0$.  \end{proof}

\begin{corollary}
If the rows of an $n\times n$ matrix $A$ are linearly dependent (for example,
if one row of $A$ is a scalar multiple of another row of $A$), then 
$\det(A)=0$.
\end{corollary}


\subsection*{An Inductive Formula for Determinants} 
\index{determinant!inductive formula for}
 
In this subsection we present an inductive formula for the
determinant --- that is, we assume that the determinant is known
for square $(n-1)\times(n-1)$ matrices and use this formula to
define the determinant for $n\times n$ matrices.  This inductive formula
is called {\em expansion by cofactors\/}.
 
Let $A=(a_{ij})$ be an $n\times n$ matrix.  Let $A_{ij}$ be the
$(n-1)\times(n-1)$ matrix formed from $A$ by deleting the
$i^{th}$ row and the $j^{th}$ column.  The matrices $A_{ij}$ are
called {\em cofactor\/} \index{cofactor} matrices of $A$.  

Inductively we define the determinant of an $n\times n$ matrix $A$ by:
\begin{align}
\det(A) & = \sum^n_{j=1} (-1)^{1+j}a_{1j}\det(A_{1j}) \nonumber
\\  & = 
      a_{11}\det(A_{11})-a_{12}\det(A_{12})+\cdots \nonumber\\
  & \quad +(-1)^{n+1}a_{1n}\det(A_{1n}).
    \label{e:inductdet}
\end{align} \index{determinant}
In Appendix~\ref{A:det} we show that the determinant function defined by 
\eqref{e:inductdet} satisfies all properties of a determinant function.
Formula \eqref{e:inductdet} is also called {\em expansion by cofactors along 
the $1^{st}$ row\/}, since the $a_{1j}$ are taken from the $1^{st}$ row 
of $A$.  Since $\det(A)=\det(A^t)$, it follows that if \eqref{e:inductdet} is 
valid as an inductive definition of determinant, then expansion by cofactors 
along the $1^{st}$ column is also valid.  That is,
\begin{equation}  \label{e:inductdetc}
\det(A) = 
a_{11}\det(A_{11})-a_{21}\det(A_{21})+\cdots
\end{equation} 
\begin{equation*}
\quad +\ (-1)^{n+1}a_{n1}\det(A_{n1}).
\end{equation*}

We now explore some of the consequences of this definition, beginning 
with determinants of small matrices.  For example, 
Definition~\ref{D:determinants}(a) implies that the determinant of a 
$1\times 1$ matrix is just
\[
\det(a) = a.
\]
Therefore, using \eqref{e:inductdet}, the determinant of a $2\times
2$ matrix is:
\begin{equation*}
\det\left(\begin{array}{cc} a_{11} & a_{12}\\a_{21} & a_{22}
\end{array}\right) = a_{11}\det(a_{22}) - a_{12}\det(a_{21})  
\end{equation*}
\begin{equation*}
\qquad = a_{11}a_{22} - a_{12}a_{21},
\end{equation*}
which is just the formula for determinants of $2\times 2$
\index{determinant!of $2\times 2$ matrices}
matrices given in \eqref{e:determinantn=2}. 
 
Similarly, we can now find a formula for the determinant of
$3\times 3$ matrices $A$ as follows.
\begin{align}
\det(A) & = a_{11}
\det \left(\begin{array}{cc} a_{22} & a_{23}\\a_{32} & a_{33}
\end{array}\right) 
- a_{12}
\det \left(\begin{array}{cc} a_{21} & a_{23}\\a_{31} & a_{33}
           \end{array}\right)  \nonumber\\
  &\quad + a_{13}
\det\left(\begin{array}{cc} a_{21} & a_{22}\\a_{31} & a_{32}
\end{array}\right)  \nonumber\\
        & = a_{11}a_{22}a_{33} + a_{12}a_{23}a_{31} + a_{13}a_{21}a_{32}  \nonumber\\
  &\quad - a_{11}a_{23}a_{32} - a_{12}a_{21}a_{33} - a_{13}a_{22}a_{31}. \label{e:det3}
\end{align}  

As an example, compute
\[
\det\left(\begin{array}{rrr} 2 & 1 & 4\\ 1 & -1 & 3\\ 5 & 6 & -2
\end{array}\right) 
\]
using formula \eqref{e:det3} as
\begin{align*}
2(-1)(-2) & + 1\cdot3\cdot5 + 4\cdot6\cdot1 -4(-1)5 -3\cdot6\cdot2
            - (-2)1\cdot1 \\
  &= 4+15+24+20 -36 +2 = 29. 
\end{align*}

There is a visual mnemonic for remembering how to compute the six
terms in formula \eqref{e:det3} for the determinant of 
$3\times 3$ matrices\index{determinant!of $3\times 3$ matrices}.
Write the matrix as a $3\times 5$ array by repeating the first 
two columns, as shown in bold face in Figure~\ref{F:det3}:
\index{determinant!computation}
\begin{figure*}[htb]
           \centerline{%
            \includegraphics[height=2.5in]{../figures/det3.pdf}}
           \caption{Mnemonic for computation of determinants of 
		$3\times 3$ matrices.}
           \label{F:det3}
\end{figure*}
Then add the product of terms connected by solid lines sloping down and 
to the right and subtract the products of terms connected by dashed lines 
sloping up and to the right.  Warning: this nice crisscross algorithm 
for computing determinants of $3\times 3$ matrices does not generalize 
to $n\times n$ matrices.
 
When computing determinants of $n\times n$ matrices when $n>3$,
it is usually more efficient to compute the determinant using row
reduction rather than by using formula \eqref{e:inductdet}.  In the
appendix to this chapter, Section~\ref{A:det}, we verify that formula 
\eqref{e:inductdet} actually satisfies the three properties of a determinant, 
thus completing the proof of Theorem~\ref{T:determinants}.  

An interesting and useful formula for reducing the effort in 
computing determinants is given by the following formula.
\begin{lemma} \label{L:detblockdiag}
Let $A$ be an $n\times n$ matrix of the form
\[
A=\mattwo{B}{0}{C}{D},
\]
where $B$ is a $k\times k$ matrix and $D$ is an $(n-k)\times(n-k)$
matrix.  Then
\[
\det(A)=\det(B)\det(D).
\]
\end{lemma}

\begin{proof} We prove this result using \eqref{e:inductdet} coupled with 
induction. Assume that this lemma is valid or all $(n-1)\times
(n-1)$ matrices of the appropriate form.  Now use
\eqref{e:inductdet} to compute
\begin{eqnarray*}
\det(A) & = & a_{11}\det(A_{11})-a_{12}\det(A_{12}) + \cdots\pm
a_{1n}\det(A_{1n}) \\
& = &  b_{11}\det(A_{11})-b_{12}\det(A_{12}) + \cdots\pm
b_{1k}\det(A_{1k}).
\end{eqnarray*}
Note that the cofactor matrices $A_{1j}$ are obtained from $A$
by deleting the $1^{st}$ row and the $j^{th}$ column.  These
matrices all have the form
\[
A_{1j} = \mattwo{B_{1j}}{0}{C_j}{D},
\]
where $C_j$ is obtained from $C$ by deleting the $j^{th}$
column. By induction on $k$
\[
\det(A_{1j}) = \det(B_{1j})\det(D).
\]
It follows that 
\begin{align*}
  \det(A) & = \left(b_{11}\det(B_{11})-b_{12}\det(B_{12}) + \cdots \right. \\
  &\quad \left. \pm
b_{1k}\det(B_{1k})\right)\det(D) \\
& = \det(B)\det(D),
\end{align*}
as desired.  \end{proof}


\subsection*{Determinants in \Matlab}
\index{determinant!in \protect\Matlab}

The determinant function has been preprogrammed in \Matlab and
is quite easy to use.  For example, typing {\tt e7\_1\_12} will
load the matrix
\begin{matlabEquation}  \label{e:A4x4}
A = \left(\begin{array}{rrrr}
     1   &  2  &   3  &   0\\
     2   &  1  &   4  &   1\\
    -2   & -1  &   0  &   1\\
    -1   &  0  &  -2  &   3  \end{array} \right).
\end{matlabEquation}
To compute the determinant of $A$ just type {\tt det(A)} and
obtain the answer \index{\computer!det}
\begin{verbatim}
ans =
   -46
\end{verbatim}

Alternatively, we can use row reduction techniques in \Matlab to
compute the determinant of $A$ --- just to test the theory that
we have developed.  Note that to compute the determinant we do
not need to row reduce to 
reduced echelon form\index{echelon form!reduced} --- we need only
reduce to an upper triangular matrix.  This can always be done
by successively adding multiples of one row to another --- an
operation that does not change the determinant.  For example,
to clear the entries in the $1^{st}$ column below the $1^{st}$
row, type
\begin{verbatim}
A(2,:) = A(2,:) - 2*A(1,:);
A(3,:) = A(3,:) + 2*A(1,:); 
A(4,:) = A(4,:) + A(1,:)
\end{verbatim}
obtaining 
\begin{verbatim}
A =
     1     2     3     0
     0    -3    -2     1
     0     3     6     1
     0     2     1     3
\end{verbatim}
To clear the $2^{nd}$ column below the $2^{nd}$ row type 
\begin{verbatim}
A(3,:) = A(3,:) + A(2,:);A(4,:)
= A(4,:) - A(4,2)*A(2,:)/A(2,2)\end{verbatim}
obtaining
\begin{verbatim}
A =
    1.0000    2.0000    3.0000         0
         0   -3.0000   -2.0000    1.0000
         0         0    4.0000    2.0000
         0         0   -0.3333    3.6667
\end{verbatim}
Finally, to clear the entry $(4,3)$ type
\begin{verbatim}
A(4,:) = A(4,:) -A(4,3)*A(3,:)/A(3,3)\end{verbatim}
to obtain
\begin{verbatim}
A =
    1.0000    2.0000    3.0000         0
         0   -3.0000   -2.0000    1.0000
         0         0    4.0000    2.0000
         0         0         0    3.8333
\end{verbatim}
To evaluate the determinant of $A$, which is now an upper
triangular matrix, type
\begin{verbatim}
A(1,1)*A(2,2)*A(3,3)*A(4,4)\end{verbatim}
obtaining
\begin{verbatim}
ans =
   -46
\end{verbatim}
as expected.



\includeexercises

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../linearAlgebra"
%%% End:
