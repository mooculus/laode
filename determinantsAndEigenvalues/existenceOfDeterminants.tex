\documentclass{ximera}

\input{../preamble.tex}

\title{*Existence of Determinants}

\begin{document}
\begin{abstract}
\end{abstract}
\maketitle


\label{A:det}

The purpose of this appendix is to verify the inductive
definition of determinant \eqref{e:inductdet}. We have already
shown that if a determinant function exists, then it is unique.
We also know that the determinant function exists for $1\times
1$ matrices. So we assume by induction that the determinant function
exists for $(n-1)\times(n-1)$ matrices and prove that the
inductive definition gives a determinant function for $n\times
n$ matrices.  \index{determinant}

Recall that $A_{ij}$ is the cofactor matrix obtained from $A$ by
deleting the $i^{th}$ row and $j^{th}$ column --- so $A_{ij}$ is
an $(n-1)\times(n-1)$ matrix.  The inductive definition is:
\index{determinant!inductive formula for}  \index{cofactor}
\[
D(A) = a_{11}\det(A_{11})-a_{12}\det(A_{12})+\cdots 
+(-1)^{n+1}a_{1n}\det(A_{1n}).
\]
We use the notation $D(A)$ to remind us that we have not yet
verified that this definition satisfies properties (a)-(c) of
Definition~\ref{D:determinants}.  In this appendix we verify these
properties after assuming that the inductive definition
satisfies properties (a)-(c) for $(n-1)\times (n-1)$ matrices.
For emphasis, we use the notation $\det$ to indicate the
determinant of square matrices of size less than $n$.  Note 
that Lemma~\ref{L:detelemrowmat}, the computation of 
determinants of elementary row operations, can therefore 
be assumed valid for $(n-1)\times (n-1)$ matrices.

We begin with the following two lemmas.

\begin{lemma} \label{L:two_equal}
Let $C$ be an $n\times n$ matrix.  If two rows of  $C$ are equal 
or one row of $C$ is zero, then $D(C)=0$. 
\end{lemma}

\begin{proof}
Suppose that row $i$ of $C$ is zero.  If $i>1$, then each cofactor has a 
zero row and by induction the determinant of the cofactor is $0$.  If row 
$1$ is zero, then the cofactor expansion is $0$ and $D(C)=0$. 

Suppose that row $i$ and row $j$ are equal, where $i >1$ and $j>1$.  Then the result
follows by the induction hypothesis, since each of the cofactors has two equal rows.  
So, we can assume that row $1$ and row $j$ are equal.  If $j>2$, let $\hat{C}$ be 
obtained from $C$ by swapping rows $j$ and $2$.  The cofactors $\hat{C}_{1k}$
are then obtained from the cofactors $C_{1k}$ by swapping rows $j-1$ and $1$.
The induction hypothesis then implies that $\det(\hat{C}_{1k})=-\det(C_{1k})$ and 
$\det(\hat{C}) = -\det(C)$.   Thus, verifying that $\det(C)=0$ reduces to verifying 
the result when rows $1$ and $2$ are equal.

Indeed, the most difficult part of this proof is the calculation that shows that if the 
$1^{st}$ and $2^{nd}$ rows of $C$ are equal, then $D(C)=0$.  This calculation is 
tedious and requires some facility with indexes and summations.  Rather than 
prove this for general $n$, we present the proof for $n=4$.  This case contains 
all of the ideas of the general proof.  

We can assume that 
\[
C = \Matrix{a_1 & a_2 & a_3 & a_4 \\ a_1 & a_2 & a_3 & a_4 \\ 
c_{31} & c_{32} & c_{33} & c_{34} \\ c_{41} & c_{42} & c_{43} & c_{44}}
\]

%\onecolumn

Using the cofactor definition $D(C) = $
\[
\begin{array}{c}
a_1 \det\left(\begin{array}{ccc} a_2 & a_3 & a_4
\\ c_{32} & c_{33} & c_{34} \\ c_{42} & c_{43} & c_{44}
\end{array}\right) 
-a_2\det\left(\begin{array}{ccc} a_1 & a_3 & a_4
\\ c_{31} & c_{33} & c_{34} \\ c_{41} & c_{43} & c_{44}
\end{array}\right) + \\ 
a_3\det\left(\begin{array}{ccc} a_1 & a_2 & a_4
\\ c_{31} & c_{32} & c_{34} \\ c_{41} & c_{42} & c_{44}
\end{array}\right) 
-a_4\det\left(\begin{array}{ccc} a_1 & a_2 & a_3
\\ c_{31} & c_{32} & c_{33} \\ c_{41} & c_{42} & c_{43}
\end{array}\right).
\end{array}
\]
Next we expand each of the four $3\times 3$ matrices along their
$1^{st}$ rows, obtaining $D(C)=$
\[
\small
\begin{array}{l}
a_1\left(a_2\det\mattwo{c_{33}}{c_{34}}{c_{43}}{c_{44}}
-a_3\det\mattwo{c_{32}}{c_{34}}{c_{42}}{c_{44}}
+a_4\det\mattwo{c_{32}}{c_{33}}{c_{42}}{c_{43}}\right)\\ 
-a_2\left(a_1\det\mattwo{c_{33}}{c_{34}}{c_{43}}{c_{44}}
-a_3\det\mattwo{c_{31}}{c_{34}}{c_{41}}{c_{44}}
+a_4\det\mattwo{c_{31}}{c_{33}}{c_{41}}{c_{43}}\right)\\ 
+a_3\left(a_1\det\mattwo{c_{32}}{c_{34}}{c_{42}}{c_{44}}
-a_2\det\mattwo{c_{31}}{c_{34}}{c_{41}}{c_{44}}
+a_4\det\mattwo{c_{31}}{c_{32}}{c_{41}}{c_{42}}\right)\\ 
-a_4\left(a_1\det\mattwo{c_{32}}{c_{33}}{c_{42}}{c_{43}}
-a_2\det\mattwo{c_{31}}{c_{33}}{c_{41}}{c_{43}}
+a_3\det\mattwo{c_{31}}{c_{32}}{c_{41}}{c_{42}}\right)
\end{array}
\]
Combining the $2\times 2$ determinants leads to $D(C)=0$.
\end{proof}

\begin{lemma} \label{L:EB}
Let $E$ be an $n\times n$ elementary row matrix and let $B$ be an $n\times
n$ matrix.   Then
\begin{equation} \label{e:proddetE}
D(EB) = D(E) D(B)
\end{equation} 
\end{lemma}

\begin{proof}  
We recall that the three elementary row operations are 
generated by two: (I) multiply row $i$ by a nonzero scalar $c$ and (II) 
add row $i$ to row $j$.  The remaining elementary row operations are 
obtained as follows. Adding $c$ times row $i$ to row $j$ is the composition 
of multiplying row $i$ by $c$, adding row $i$ to row $j$, and multiplying row 
$i$ by $1/c$.  For $2\times 2$ matrices swapping rows $1$ and $2$ was 
written in terms of four other elementary row operations in \eqref{e:swapdecomp}.   
This observation works in general, as follows.  Consider the sequence 
of row operations:
\begin{itemize}
\item	add row $j$ to row $i$
\item multiply row $j$ by $-1$
\item add row $i$ to row $j$
\item subtract row $j$ from row $i$
\end{itemize}
We can write swapping rows $i$ and $j$ schematically as:
\[
\Matrix{r_i \\ r_j} \to \Matrix{r_i + r_j \\ r_j} \to  \Matrix{r_i + r_j \\ -r_j}
\to   \Matrix{r_i + r_j \\ r_i} \to  \Matrix{r_j \\ r_i}
\]

Thus, we need to verify \eqref{e:proddetE} for two types of
elementary row operation: multiply row $i$ by $c\neq 0$ and add 
row $j$ to row $i$. \index{elementary row operations}

\noindent (I) Suppose that $E$ multiplies the $i^{th}$ row by a
nonzero scalar $c$.  If $i>1$, then the cofactor matrix
$(EA)_{1j}$ is obtained from the cofactor matrix $A_{1j}$ by
multiplying the $(i-1)^{st}$ row by $c$.  By induction,
$\det(EA)_{1j}= c\det(A_{1j})$ and $D(EA)=cD(A)$.  On the other
hand, $D(E)=\det(E_{11})=c$.  So \eqref{e:proddetE} is verified in
this instance.  If $i=1$, then the $1^{st}$ row of $EA$ is
$(ca_{11},\ldots,ca_{1n})$ from which it is easy to use the cofactor 
formula to verify \eqref{e:proddetE}.

\noindent (II) Next suppose that $E$ adds row $i$ to row $j$.  If  
$i,j > 1$, then the result follows from the induction hypothesis 
since the new cofactors are obtained from the old cofactors by 
adding row $i-1$ to row $j-1$. 

If $j=1$, then 
\begin{eqnarray*}
  D(EB) & = & (b_{11} + b_{i1})\det(B_{11}) +\cdots \\
   & & +\ (-1)^{n+1}(b_{1n}+ b_{in})\det(B_{1n})\\
& = &
\left[b_{11}\det(B_{11})+\cdots+(-1)^{n+1}b_{1n}\det(B_{1n})\right]  \\
& & +\  \left[b_{i1}\det(B_{11})+\cdots+(-1)^{n+1}b_{i1}\det(B_{1n})\right]\\
& = & D(B) + D(C)
\end{eqnarray*}
where the $1^{st}$ and $i^{th}$ rows of $C$ are equal.  The fact that $D(C)=0$ 
follows from Lemma~\ref{L:two_equal}.

If $i=1$, then the cofactors are unchanged.  It follows by direct calculation of the 
cofactor expansion that $D(EB) = D(B) + D(C)$ where the $1^{st}$ and $i^{th}$ 
rows of $C$ are equal.  Again, the fact that $D(C)=0$ follows from 
Lemma~\ref{L:two_equal}.
\end{proof}

{\bf Property (a)} is verified for $D(A)$ using cofactors since if $A$ is lower
triangular, then
\[
D(A) = a_{11}\det(A_{11}) 
\]
and 
\[
\det(A_{11}) = a_{22}\cdots a_{nn}
\]
by the induction hypothesis.

{\bf Property (c)} ($D(AB)=D(A)D(B)$) is proved separately 
for $A$ singular and $A$ nonsigular.  In either case, row 
reduction implies that $A = E_s\cdots E_1 R$ where $R$ is 
in reduced echelon form. 

If $A$ is singular, the the bottom row of $R$ is zero and together 
Lemmas~\ref{L:two_equal} and \ref{L:EB} imply that $D(A)=0$.
On the other hand these lemmas also imply that 
\[
D(AB) = D(E_s\cdots E_1 RB)  = D(E_s\cdots E_1) D(RB) 
\]
and direct calculation shows that the bottom row of $RB$ is 
also zero.  Hence $D(RB)=0$ and property (c) is valid.

Next suppose now that $A$ is nonsingular.  It follows that 
\[
AB = E_s\cdots E_1B.
\]
Using \eqref{e:proddetE} we see that
\begin{align*}
D(AB) &=  D(E_s)\cdots D(E_1)D(B) \\
&= D(E_s\cdots E_1)D(B) = D(A)D(B),
\end{align*}
as desired.  

Before verifying property (b) we prove the following:
\begin{lemma}
Let $E$ be an elementary row operation matrix.  Then $D(E^t) = D(E)$.   
An $n\times n$ matrix $A$ is singular if and only if $A^t$ is singular.
\end{lemma}

\begin{proof}
The two generators of elementary row operations are: multiply row $i$ 
by $c$ and add row $i$ to row $j$. The first matrix is diagonal; so 
$E^t = E$.  Denote the second matrix by $F_{ij}$.  It follows that 
$F_{ij}^t = F_{ji}$.   We claim that $D(F_{ij})=1$ for all $i,j$ and hence 
that $D(E^t) = D(E)$ for all $E$. If $i<j$, then $F_{ij}$ is lower triangular 
with $1^s$ on the diagonal.  Hence $D(F_{ij})=1$.  If $1<j<i$, then 
$D(F_{ij}^t) = D(F_{ji}) = 1$ by induction.  If $j=1$, then $D(F_{i1}^t) = 1$ 
by direct calculation. 

If $A$ is singular, then $A=E_s\cdots E_1R$, where $R$ is in reduced 
echelon form and its bottom row is zero. Hence $R$ is singular. It follows 
that $D(A) = 0$. Note that 
\[
A^t = R^t E_1^t\cdots E_s^t
\] 
Here we use the fact that $(BC)^t= C^tB^t$ that was discussed in \eqref{e:transposeprod}.  
By counting pivots in $R$, we see that the column space and the row space 
of $R$ have the same dimensions.  Therefore, the dimension of the row 
space of $R^t$ equals the dimension of the column space of $R^t$ equals 
the dimension of the row space of $R$, and all of these are less than $n$. 
Hence $R^t$ is singular.  Therefore, there exists a nonzero $n$-vector $w$ such that 
$R^tw=0$.  It follows that $v = (E_s^t)^{-1}\cdots (E_1^t)^{-1} w$ satisfies 
$A^tw = R^tv = 0$ and $A^t$ is singular.
\end{proof}


{\bf Property (b)} We prove $D(A^t)=D(A)$ in two steps.  Write   
\begin{equation}  \label{e:Adecomp}
A=E_s\cdots E_1 R,
\end{equation}
where the $E_j$ are elementary row matrices and $R$ is in reduced echelon form.  It follows that 
\begin{equation}  \label{e:Atdecomp}
A^t = R^t E_1^t \cdots E_s^t.  
\end{equation}
If $A$ is invertible, then $R=I_n$ and $D(A^t) = D(A)$.  If $A$ is singular, then $A^t$ is also singular and  $D(A)=0=D(A^t)$. 

We have now completed the proof that a determinant function exists. 
\end{document}
