\documentclass{ximera}

\input{./preamble.tex}

\title{Similar Matrices}

\begin{document}
\begin{abstract}
\end{abstract}
\maketitle

 \label{S:6.5}

In Section~\ref{S:LNFPS} we discussed solutions to differential equations
$\dot{X}=CX$ for three classes of matrices $C$.  See Table~\ref{T:3sys}.
We stated that in a certain sense every $2\times 2$ matrix can be
thought of as a member of one of these families.  In this section we
show that every $2\times 2$ matrix is similar to one of the matrices in that
table (see Theorem~\ref{T:putinform}), where similarity is defined as follows.

\begin{Def}  \label{D:similar}
The $n\times n$ matrices $B$ and $C$ are {\em similar\/} if
there exists an invertible $n\times n$ matrix $P$ such that
\[
C = P\inv BP.
\]
\end{Def} \index{similar}\index{similar!matrices} \index{invertible}

Our present interest in similar matrices stems from the fact that if we
know the solutions to the system of differential equations $\dot{Y}=CY$ in
closed form, then we know the solutions to the system of differential
equations $\dot{X}=BX$ in closed form.  More precisely,
\begin{lemma}  \label{L:simsoln}
Suppose that $B$ and $C=P\inv BP$ are similar matrices.  If
$Y(t)$ is a solution to the system of differential equations
$\dot{Y}=CY$, then $X(t)=PY(t)$ is a solution to the system of 
differential equations $\dot{X}=BX$.
\end{lemma}

\begin{proof}   Since the entries in the matrix $P$ are constants, it follows that
\[
\frac{dX}{dt} = P\frac{dY}{dt}.
\]
Since $Y(t)$ is a solution to the $\dot{Y}=CY$ equation, it follows that
\[
\frac{dX}{dt} = PCY.
\]
Since $Y=P\inv X$ and $PCP\inv = B$,
\[
\frac{dX}{dt} = PCP\inv X = BX.
\]
Thus $X(t)$ is a solution to $\dot{X}=BX$, as claimed.  \end{proof}


\subsection*{Invariants of Similarity}

\begin{lemma}  \label{L:simdettr}
Let $A$ and $B$ be similar $2\times 2$ matrices.  Then
\begin{eqnarray*}
p_A(\lambda) & = & p_B(\lambda),\\
\det(A) & = & \det(B),\\
\trace(A) & = & \trace(B),
\end{eqnarray*} \index{characteristic polynomial}\index{trace}
and the eigenvalues of $A$ and $B$ are equal.
\end{lemma}

\begin{proof}
The determinant\index{determinant} is a function on $2\times 2$ matrices
that has several important properties.  Recall, in particular, from
Chapter~\ref{chap:matrices}, Theorem~\ref{propdet} that for any pair of
$2\times 2$ matrices $A$ and $B$:
\begin{equation} \label{e:detprod}
\det(AB) =  \det(A)\det(B),
\end{equation}
and for any invertible $2\times 2$ matrix $P$
\begin{equation}  \label{e:detinv}
\det(P\inv)  =  \frac{1}{\det(P)}.
\end{equation}

Let $P$ be an invertible $2\times 2$ matrix so that $B=P\inv AP$.
Using \Ref{e:detprod} and \Ref{e:detinv} we see that
\begin{eqnarray*}
p_B(\lambda) & = & \det(B-\lambda I_2) \\
 & = & \det(P\inv AP-\lambda I_2) \\
& = & \det(P\inv(A-\lambda I_2)P) \\
& = & \det(A-\lambda I_2) \\
& = & p_A(\lambda).
\end{eqnarray*}
Hence the eigenvalues of $A$ and $B$ are the same.  It follows
from \Ref{e:treigen} and \Ref{e:deteigen} of Section~\ref{S:evchp}
that the determinants and traces of $A$ and $B$ are equal.   \end{proof}

For example, if
\[
A = \mattwo{-1}{0}{0}{1} \AND  P = \mattwo{1}{2}{1}{1},
\]
then
\[
P\inv = \mattwo{-1}{2}{1}{-1}
\]
and
\[
P\inv AP = \mattwo{3}{4}{-2}{-3}.
\]
A calculation shows that
\[
\det(P\inv AP)=-1=\det(A) \AND {\rm tr}(P\inv AP)=0={\rm tr}(A),
\]
as stated in Lemma~\ref{L:simdettr}.


\subsubsection*{Similarity and Matrix Exponentials}

We introduce similarity at this juncture for the following reason:
if $C$ is a matrix that is similar to $B$, then $e^C$ can be computed
from $e^B$.  More precisely:

\begin{lemma} \label{L:similarexp}
Let $C$ and $B$ be $n\times n$ similar matrices, and let $P$ be
an invertible $n\times n$ matrix such that
\[
C=P\inv BP.
\]
Then
\begin{equation}  \label{e:similarexp}
e^C = P\inv e^BP.
\end{equation}
\end{lemma} \index{similar} \index{invertible}

\begin{proof} Note that for all powers of $k$ we have
\[
(P\inv BP)^k = P\inv B^kP.
\]
Next verify \Ref{e:similarexp} by computing
\[
e^C =\sum^{\infty}_{k=0} \frac{1}{k!}C^k
 =  \sum^{\infty}_{k=0} \frac{1}{k!}(P\inv BP)^k
=  \sum^{\infty}_{k=0} \frac{1}{k!}P\inv B^kP
= P\inv\left(\sum^{\infty}_{k=0} \frac{1}{k!}B^k\right)P
= P\inv e^B P.
\]
\end{proof}


\subsection*{Classification of $2\times 2$ Matrices}
\index{normal form}

We now classify all $2\times 2$ matrices up to similarity.

\begin{thm}  \label{T:putinform}
Let $C$ and $P=(v_1|v_2)$ be $2\times 2$ matrices where the vectors
$v_1$ and $v_2$ are specified below.
\begin{itemize}
\item[(a)]	Suppose that $C$ has two linearly independent
real eigenvectors $v_1$ and $v_2$ with real eigenvalues $\lambda_1$
and $\lambda_2$.  Then
\[
P\inv CP = \mattwoc{\lambda_1}{0}{0}{\lambda_2}.
\]

\item[(b)]	Suppose that $C$ has no real eigenvectors and
complex conjugate eigenvalues $\sigma\pm i\tau$ where
$\tau\neq 0$.  Then
\[
P\inv CP = \mattwo{\sigma}{-\tau}{\tau}{\sigma},
\]
where $v_1 + iv_2$ is an eigenvector of $C$ associated with the
eigenvalue $\lambda_1=\sigma-i\tau$.

\item[(c)]	Suppose that $C$ has exactly one linearly
independent real eigenvector $v_1$ with real eigenvalue $\lambda_1$.
Then
\[
P\inv CP = \mattwoc{\lambda_1}{1}{0}{\lambda_1},
\]
where  $v_2$ is a generalized eigenvector of $C$ that satisfies
\begin{equation}  \label{e:Cw=lw+v}
(C-\lambda_1 I_2) v_2 =  v_1.
\end{equation}

\end{itemize}
\end{thm}

\begin{proof}
The strategy in the proof of this theorem is to determine the
$1^{st}$ and $2^{nd}$ columns of $P\inv CP$ by computing (in each case)
$P\inv CPe_j$ for $j=1$ and $j=2$.  Note from the definition of $P$
that
\[
Pe_1 = v_1 \AND Pe_2 = v_2.
\]
In addition, if $P$ is invertible, then
\[
P\inv v_1 = e_1 \AND P\inv v_2 = e_2.
\]
Note that if $v_1$ and $v_2$ are linearly independent, then $P$ is invertible.

(a) \quad Since $v_1$ and $v_2$ are assumed to be linearly independent,
$P$ is invertible.  So we can compute
\[
P\inv CPe_1 = P\inv C v_1 = \lambda P\inv v_1 = \lambda e_1.
\]
It follows that the $1^{st}$ column of $P\inv CP$	is
\[
\vectwoc{\lambda_1}{0}.
\]
Similarly, the $2^{nd}$ column of $P\inv CP$ is
\[
\vectwoc{0}{\lambda_2}
\]
thus verifying (a).

(b) \quad  Lemma~\ref{L:rievind} implies that $v_1$ and $v_2$ are linearly
independent and hence that $P$ is invertible.  Using \Ref{e:complexcoord},
with $\tau$ replaced by $-\tau$, $v$ replaced by $v_1$, and $w$ replaced by
$w_1$, we calculate
\[
P\inv CPe_1 = P\inv Cv_1 = \sigma P\inv v_1 + \tau P\inv v_2
= \sigma e_1 + \tau e_2,
\]
and
\[
P\inv CPe_2 = P\inv Cv_2 = -\tau P\inv v_1 + \sigma P\inv v_2
= -\tau e_1 + \sigma e_2.
\]
Thus the columns of $P\inv CP$ are
\[
\vectwo{\sigma}{\tau} \AND \vectwo{-\tau}{\sigma},
\]
as desired.


(c) \quad   Let $v_1$ be an eigenvector and assume that $v_2$ is a
generalized eigenvector satisfying \Ref{e:Cw=lw+v}.  By
Lemma~\ref{L:geneig2} the vectors $v_1$ and $v_2$ exist and are linearly
independent.

For this choice of $v_1$ and $v_2$, compute
\[
P\inv CPe_1 = P\inv Cv_1 = \lambda_1 P\inv v_1 = \lambda_1 e_1,
\]
and
\[
P\inv CPe_2 = P\inv Cv_2 = P\inv v_1+\lambda_1 P\inv v_2 = e_1+\lambda_1 e_2.
\]
Thus the two columns of $P\inv CP$ are:
\[
\vectwoc{\lambda_1}{0} \AND \vectwoc{1}{\lambda_1}.
\]
  \end{proof}


\subsection*{Closed Form Solutions Using Similarity}
\index{closed form solution}

We now use Lemma~\ref{L:simsoln}, Theorem~\ref{T:putinform}, and the
explicit solutions to the normal form equations Table~\ref{T:3sys}
to find solutions for $\dot{X}=CX$ where $C$ is any $2\times 2$ matrix.
The idea behind the use of similarity to solve systems of ODEs is to
transform a given system into another normal form system whose solution is
already known.  This method is very much like the technique of change of
variables used when finding indefinite integrals in calculus.

We suppose that we are given a system of differential equations $\dot{X}=CX$
and use Theorem~\ref{T:putinform} to transform $C$ by similarity to one of
the normal form matrices listed in that theorem.  We then solve the
transformed equation, as we did in Section~\ref{S:LNFPS} (see
Table~\ref{T:3sys}), and use Lemma~\ref{L:simsoln} to transform the solution
back to the given system.

For example, suppose that $C$ has a complex eigenvalue $\sigma-i\tau$ with
corresponding eigenvector $v+iw$.  Then Theorem~\ref{T:putinform} states that
\[
B = P\inv CP = \mattwo{\sigma}{-\tau}{\tau}{\sigma},
\]
where $P=(v|w)$ is an invertible matrix.  Using Table~\ref{T:3sys} the
general solution to the system of equations $\dot{Y}=BY$ is:
\[
Y(t) = e^{\sigma t}
\mattwo{\cos(\tau t)}{-\sin(\tau t)}{\sin(\tau t)}{\cos(\tau t)}
\vectwo{\alpha}{\beta}.
\]
Lemma~\ref{L:simsoln} states that
\[
X(t) = PY(t)
\]
is the general solution to the $\dot{X}=CX$ system.  Moreover, we can solve
the initial value problem by solving
\[
X_0 = PY(0) = P\vectwo{\alpha}{\beta}
\]
for $\alpha$ and $\beta$.  In particular,
\[
\vectwo{\alpha}{\beta} = P\inv X_0.
\]
Putting these steps together implies that
\begin{equation} \label{e:exp0ev}
X(t) = e^{\sigma t}
P\mattwo{\cos(\tau t)}{-\sin(\tau t)}{\sin(\tau t)}{\cos(\tau t)}P\inv X_0
\end{equation}
is the solution to the initial value problem.

\subsubsection*{The Example with Complex Eigenvalues Revisited}
\index{eigenvalue!complex}

Recall the example in \Ref{e:complexexample}
\[
\frac{dX}{dt} = \mattwo{-1}{2}{-5}{-3} X,
\]
with initial values
\[
X_0=\vectwo{1}{1}.
\]
This linear system has a complex eigenvalue $\sigma-i\tau=-2-3i$ with
corresponding eigenvector
\[
v+iw = \vectwoc{2}{-1-3i}.
\]
Thus the matrix $P$ that transforms $C$ into normal form is
\[
P = \mattwo{2}{0}{-1}{-3} \AND P\inv = \frac{1}{6}\mattwo{3}{0}{-1}{-2}.
\]
It follows from \Ref{e:exp0ev} that the solution to the initial value problem
is
\begin{eqnarray*}
X(t) & =  &
e^{-2t}P\mattwo{\cos(3t)}{-\sin(3t)}{\sin(3t)}{\cos(3t)}P\inv X_0 \\ & = &
\frac{1}{6}e^{-2t}\mattwo{2}{0}{-1}{-3}
\mattwo{\cos(3t)}{-\sin(3t)}{\sin(3t)}{\cos(3t)}
\mattwo{3}{0}{-1}{-2}\vectwo{1}{1}.
\end{eqnarray*}
A calculation gives
\begin{eqnarray*}
X(t) & = & \frac{1}{2}e^{-2t}\mattwo{2}{0}{-1}{-3}
\mattwo{\cos(3t)}{-\sin(3t)}{\sin(3t)}{\cos(3t)}\vectwo{1}{-1}  \\
& = & e^{-2t}
\vectwoc{\cos(3t)+\sin(3t)}{\cos(3t)-2\sin(3t)}.
\end{eqnarray*}
Thus the solution to \Ref{e:complexexample} that we have found using 
similarity of matrices is identical to the solution \Ref{e:complexexampleans}
that we found by the direct method.

Solving systems with either distinct real eigenvalues or equal eigenvalues
works in a similar fashion.



\EXER

\TEXER

\begin{exercise} \label{c6.5.1}
Suppose that the matrices $A$ and $B$ are similar and the matrices
$B$ and $C$ are similar.  Show that $A$ and $C$ are also similar
matrices.
\end{exercise}

\begin{exercise} \label{c6.5.2}
Use \Ref{e:trAB=trBA} in Chapter~\ref{chap:matrices} to verify that the
traces of similar matrices are equal.
\end{exercise}

\noindent In Exercises~\ref{c6.5.3a} -- \ref{c6.5.3b} determine whether
or not the given matrices are similar, and why.
\begin{exercise} \label{c6.5.3a}
$A = \mattwo{1}{2}{3}{4} \AND B = \mattwo{2}{-2}{-3}{8}$.
\end{exercise}
\begin{exercise} \label{c6.5.3b}
$C = \mattwo{2}{2}{2}{2} \AND D = \mattwo{4}{-2}{-2}{4}$.
\end{exercise}

\begin{exercise} \label{c6.5.4}
Let $B=P\inv AP$ so that $A$ and $B$ are similar matrices.  Suppose
that $v$ is an eigenvector of $B$ with eigenvalue $\lambda$.  Show
that $Pv$ is an eigenvector of $A$ with eigenvalue $\lambda$.
\end{exercise}

\begin{exercise} \label{c6.5.5}
Which $n\times n$ matrices are similar to $I_n$?
\end{exercise}

\begin{exercise} \label{c6.5.6}
Compute $e^A$ where
\[
A = \mattwo{3}{-1}{1}{1}.
\]
Check your answer using \Matlabp.
\end{exercise}


\end{document}
