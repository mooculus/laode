\documentclass{ximera}

\input{../preamble.tex}

\title{Formulas for Matrix Exponentials}

\begin{document}
\begin{abstract}
\end{abstract}
\maketitle


\label{S:6.6}
\index{closed form solution}\index{matrix!exponential!computation}

We now complete our discussion of exact solutions to planar linear systems of
ODEs $\dot{X}=CX$.  There are three different methods for finding closed form
solutions to systems of ODEs.  We have discussed two of these methods.  In
the first we find solutions by the direct method, that is, we find two linear
independent solutions whose linear combinations form the space of solutions.
See Section~\ref{S:TDM}.  In the second method we use similarity and normal
form equations (whose solutions are obtained using matrix exponentials) to
find closed form solutions.  See Sections~\ref{S:LNFPS} and \ref{S:6.5}.
In this section we present a third method based on computable formulas for
matrix exponentials derived using the Cayley Hamilton theorem.

\subsection*{A Formula for the Matrix Exponential}

For $2\times 2$ matrices $C$ with eigenvalues $\lambda_1$ and $\lambda_2$
there is a simple formula for the matrix exponential $e^{tC}$ whose
derivation depends on the Cayley Hamilton theorem. \index{Cayley Hamilton
theorem}  When the eigenvalues $\lambda_1$ and $\lambda_2$ of $C$ are
distinct, the formula is
\begin{equation}  \label{E:exdist}
e^{tC} = \frac{1}{\lambda_2-\lambda_1}\left(e^{\lambda_1 t}(C-\lambda_2I_2) -
e^{\lambda_2 t}(C-\lambda_1I_2)\right).
\end{equation}
When the eigenvalues are equal, the formula is
\begin{equation}  \label{E:exeq}
e^{tC} = e^{\lambda_1 t}(I_2 + tN)
\end{equation}
where $N = C - \lambda_1I_2$.

Note that when computing the matrix exponential using either \Ref{E:exdist}
or \Ref{E:exeq}, it is not necessary to compute the eigenvectors of $C$.
This is a substantial simplification.  But it is with the use of formula
\Ref{E:exeq} that the greatest simplification occurs.

\subsubsection*{The Example with Equal Eigenvalues Revisited}

Let us reconsider the system of differential equations \Ref{e:shearexample}
\[
\frac{dX}{dt} = \mattwo{1}{-1}{9}{-5} X = CX
\]
with initial value
\[
X_0 = \vectwo{2}{3}.
\]
The eigenvalues of $C$ are real and equal to $\lambda_1=-2$.

We may write
\[
C = \lambda_1 I_2 + N = -2I_2+N,
\]
where
\[
N = \mattwo{3}{-1}{9}{-3}.
\]
It follows from \Ref{E:exeq} that
\begin{equation}  \label{e:solntob}
e^{tC} =  e^{-2t}\left(I_2+t\mattwo{3}{-1}{9}{-3}\right)
= e^{-2t}\mattwoc{1+3t}{-t}{9t}{1-3t}.
\end{equation}
Hence the solution to the initial value problem is:
\[
X(t) = e^{tC}X_0 = e^{-2t}\mattwoc{1+3t}{-t}{9t}{1-3t}\vectwo{2}{3}
 = e^{-2t}\vectwo{2+3t}{3+9t}.
\]

\subsection*{The Cayley Hamilton Theorem} \index{Cayley Hamilton theorem}

The Cayley Hamilton theorem states that a matrix satisfies its own
characteristic polynomial.  More precisely:
\begin{theorem}[Cayley Hamilton Theorem] \label{T:CH2}
Let $A$ be a $2\times 2$ matrix and let
\[
p_A(\lambda) = \lambda^2 + a\lambda + b
\]
be the characteristic polynomial\index{characteristic polynomial} of $A$.  Then
\[
p_A(A) = A^2 + aA + bI_2 = 0.
\]
\end{theorem}

\begin{proof}  Suppose $B=P\inv AP$ and $A$ are similar matrices.  We claim that
if $p_A(A)=0$, then $p_B(B)=0$.  To verify this claim, recall from
Lemma~\ref{L:simdettr} that $p_A=p_B$ and calculate
\[
p_B(B) = p_A(P\inv AP) = (P\inv AP)^2 + aP\inv AP + bI_2 = P\inv p_A(A)P= 0.
\]
Theorem~\ref{T:putinform} classifies $2\times 2$ matrices up to similarity.
Thus, we need only verify this theorem for the matrices
\[
C =  \mattwoc{\lambda_1}{0}{0}{\lambda_2} \qquad
D =  \mattwo{\sigma}{-\tau}{\tau}{\sigma} \qquad
E =  \mattwoc{\lambda_1}{1}{0}{\lambda_1},
\]
that is, we need to verify that
\[
p_C(C) = 0 \qquad p_D(D)=0 \qquad p_E(E)=0.
\]

Using the fact that $p_A(\lambda)=\lambda^2-\trace(A)\lambda+\det(A)$, we see
that
\begin{eqnarray*}
p_C(\lambda) & = & (\lambda-\lambda_1)(\lambda-\lambda_2) \\
p_D(\lambda) & = & \lambda^2 - 2\sigma \lambda + (\sigma^2+\tau^2) \\
p_E(\lambda) & = & (\lambda-\lambda_1)^2.
\end{eqnarray*}
It now follows that
\[
p_C(C) = (C-\lambda_1I_2)(C-\lambda_2I_2) =
\mattwoc{0}{0}{0}{\lambda_2-\lambda_1}\mattwoc{\lambda_1-\lambda_2}{0}{0}{0}
=0,
\]
\[
p_D(D) =
\mattwoc{\sigma^2-\tau^2}{-2\sigma\tau}{2\sigma\tau}{\sigma^2-\tau^2}
-2\sigma \mattwo{\sigma}{-\tau}{\tau}{\sigma} +
(\sigma^2+\tau^2)\mattwo{1}{0}{0}{1} = 0,
\]
\[
p_E(E) = (E-\lambda_1I_2)^2 = \mattwo{0}{1}{0}{0}^2 = 0.
\]
\end{proof}



\subsection*{Verification of \protect\Ref{E:exdist}}

Let $C$ be a $2\times 2$ matrix with eigenvalues $\lambda_1\neq\lambda_2$.
Then the characteristic polynomial of $C$ is:
\[
p_C(\lambda) = (\lambda-\lambda_1)(\lambda-\lambda_2).
\]

We begin our verification of \Ref{E:exdist} by showing that
\begin{equation}  \label{E:Inpart}
I_2 = a_1 (A - \lambda_2I_2) + a_2 (A - \lambda_1I_2),
\end{equation}
where
\begin{equation}  \label{E:pfcoeff}
a_1 = \frac{1}{\lambda_2-\lambda_1} \AND a_2 = \frac{1}{\lambda_1-\lambda_2}.
\end{equation}
Using partial fractions we can write
\begin{equation}  \label{E:partfrac}
\frac{1}{p_C(\lambda)} = \frac{a_1}{\lambda-\lambda_1} +
\frac{a_2}{\lambda-\lambda_2},
\end{equation}
where $a_j$ is as in \Ref{E:pfcoeff}.
Multiplying \Ref{E:partfrac} by $p_C(\lambda)$ yields
\[
1 = a_1 (\lambda-\lambda_2) + a_2 (\lambda-\lambda_1).
\]
Now let $v_j$ be the eigenvector of $C$ corresponding to the eigenvalue
$\lambda_j$ and compute
\[
(a_1 (C - \lambda_2I_2) + a_2 (C - \lambda_1I_2))v_1 =
a_1(C - \lambda_2I_2)v_1 = a_1(\lambda_1-\lambda_2)v_1 = v_1.
\]
Similarly,
\[
(a_1 (C - \lambda_2I_2) + a_2 (C - \lambda_1I_2))v_2 = v_2.
\]
Since $v_1$ and $v_2$ form a basis, \Ref{E:Inpart} holds by linearity.

We use the Cayley Hamilton theorem to show that
\begin{equation}  \label{E:almost}
\begin{array}{rcl}
(C - \lambda_2I_2)e^{tC} & = & e^{\lambda_1 t}(C - \lambda_2I_2)\\
(C - \lambda_1I_2)e^{tC} & = & e^{\lambda_2 t}(C - \lambda_1I_2)
\end{array}
\end{equation}
Assuming that \Ref{E:almost} is valid, we see that
\[
e^{tC} = (a_1 (C - \lambda_2I_2) + a_2 (C - \lambda_1I_2))e^{tC}
\]
by \Ref{E:Inpart} and that
\[
e^{tC} = a_1 e^{\lambda_1 t}(C - \lambda_2I_2) +
a_2 e^{\lambda_2 t}(C - \lambda_1I_2)
\]
by \Ref{E:almost}.  Thus we have verified formula \Ref{E:exdist}.
To validate \Ref{E:almost}, calculate
\[
e^{tC} = e^{tC - \lambda_1 tI_2 + \lambda_1 tI_2} =
e^{\lambda_1t}e^{t(C - \lambda_1 I_2)}
\]
using Proposition~\ref{P:expAB} and the fact that $I_2$ commutes with every
$2\times 2$ matrix.  Second, compute
\begin{eqnarray*}
(C - \lambda_2I_2)e^{tC} & = &
e^{\lambda_1 t}(C - \lambda_2I_2)e^{t(C - \lambda_1 I_2)} \\
& = &  e^{\lambda_1 t}(C - \lambda_2I_2)(I_2 + t(C - \lambda_1 I_2) + \cdots)\\
& = & e^{\lambda_1 t}(C - \lambda_2I_2),
\end{eqnarray*}
since every other term in the infinite series contains a factor of
\[
p_C(C) = (C - \lambda_2I_2)(C - \lambda_1 I_2)
\]
which vanishes by the Cayley Hamilton theorem.  The second equation in
\Ref{E:almost} is proved similarly by interchanging the roles of $\lambda_1$
and $\lambda_2$.

\subsection*{Verification of \protect\Ref{E:exeq}}

The verification of \Ref{E:exeq} is less complicated.  Since $C$ is assumed
to have a double eigenvalue $\lambda_1$, it follows that
\[
N = C - \lambda_1 I_2
\]
has zero as a double eigenvalue.  Hence, the characteristic polynomial
$p_N(\lambda) = \lambda^2$ and the Cayley Hamilton theorem implies that
$N^2=0$.  Therefore,
\[
e^{tC} = e^{t(C-\lambda_1 I_2)+\lambda_1 tI_2} =
e^{\lambda_1 t}e^{tN} = e^{\lambda_1 t}(I_2+tN),
\]
as desired.




\EXER

\TEXER

\begin{exercise} \label{c6.6.2}
Solve the initial value problem
\[
\frac{dX}{dt} = \mattwo{0}{1}{-2}{3} X
\]
where $X(0)=(2,1)^t$.
\end{exercise}

\begin{exercise} \label{c6.6.3}
Find all solutions to the linear system of ODEs
\[
\frac{dX}{dt} = \mattwo{-2}{4}{-1}{1} X.
\]
\end{exercise}

\begin{exercise} \label{c6.6.4}
Solve the initial value problem
\[
\frac{dX}{dt} =  \mattwo{2}{1}{-2}{0}X
\]
where $X(0)=(1,1)^t$.
\end{exercise}

\begin{exercise}  \label{c6.CH}
Let $A$ be a $2\times 2$ matrix.  Show that
\[
A^2 = \trace(A)A-\det(A)I_2.
\]
\end{exercise}



\end{document}
