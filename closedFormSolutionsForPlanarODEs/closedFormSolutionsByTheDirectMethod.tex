\documentclass{ximera}

\input{../preamble.tex}

\title{Closed Form Solutions by the Direct Method}

\begin{document}
\begin{abstract}
\end{abstract}
\maketitle


\label{S:TDM}


In Section~\ref{S:IVPR} we showed in detail how solutions to planar systems
of constant coefficient differential equations with distinct real eigenvalues
are found.  This method was just reviewed in Section~\ref{S:6.1} where we saw
that the crucial step in solving these systems of differential equations is
the step where we find two linearly independent solutions.   In this section
we discuss how to find these two linearly independent solutions when the
eigenvalues of the coefficient matrix are either complex or real and equal.

By finding these two linearly independent solutions we will find both
the {\em general\/} solution of the system of differential equations
$\dot{X}=CX$ and a method for solving the initial value
problem\index{initial value problem}
\begin{equation}  \label{e:eqnA}
\begin{array}{rcl}
\dps\frac{dX}{dt} & = & CX\\
X(0) & = & X_0.
\end{array}
\end{equation}
We assume that $C$ is a $2\times 2$ matrix with eigenvalues $\lambda_1$ and
$\lambda_2$.  When needed, we denote the associated eigenvectors by $v_1$ and
$v_2$.

\subsection*{Real Distinct Eigenvalues}
\index{eigenvalue!real}

We have discussed the case when $\lambda_1\neq\lambda_2\in\R$ on several
occasions.  For completeness we repeat the result.  The general solution is:
\begin{equation}  \label{E:RD2}
X(t) = \alpha_1 e^{\lambda_1 t}v_1 + \alpha_2 e^{\lambda_2 t}v_2.
\end{equation}
The initial value problem is solved by finding real numbers $\alpha_1$ and
$\alpha_2$ such that
\[
X_0 = \alpha_1 v_1 + \alpha_2 v_2.
\]
See Section~\ref{S:IVPR} for a detailed discussion with examples.

\subsection*{Complex Conjugate Eigenvalues}
\index{eigenvalue!complex}

Suppose that the eigenvalues of $C$ are complex, that is, suppose that
$\lambda_1= \sigma+i\tau$ with $\tau\neq 0$ is an eigenvalue of $C$ with
eigenvector $v_1=v+iw$, where $v,w\in\R^2$.  We claim that
$X_1(t)$ and $X_2(t)$, where
\begin{equation}  \label{E:CC1}
\begin{array}{rcl}
X_1(t) & = & e^{\sigma t}(\cos(\tau t)v -\sin(\tau t)w)\\
X_2(t) & = & e^{\sigma t}(\sin(\tau t)v +\cos(\tau t)w),
\end{array}
\end{equation}
are solutions to \Ref{e:eqnA} and that the general
solution\index{general solution} to \Ref{e:eqnA} is:
\begin{equation}  \label{E:CC2}
X(t) = \alpha_1 X_1(t) + \alpha_2 X_2(t),
\end{equation}
where $\alpha_1, \alpha_2$ are real scalars.

There are several difficulties in deriving \Ref{E:CC1} and \Ref{E:CC2}; these
difficulties are related to using complex numbers as opposed to real numbers.
In particular, in the derivation of \Ref{E:CC1} we need to define the
exponential of a complex number, and we begin by discussing this issue.

\subsubsection*{Euler's Formula}\index{Euler's formula}

We find complex exponentials by using Euler's celebrated formula:
\begin{equation}  \label{E:Euler}
e^{i\theta} = \cos\theta + i\sin\theta
\end{equation}
for any real number $\theta$.  A justification of this formula is
given in Exercise~\ref{c6.6.05}.   Euler's formula allows us to differentiate
complex exponentials, obtaining the expected result:
\begin{eqnarray*}
\frac{d}{dt}e^{i\tau t} & = &
\frac{d}{dt}(\cos(\tau t) + i\sin(\tau t))\\
& = & \tau(-\sin(\tau t) + i\cos(\tau t)) \\
& = & i\tau(\cos(\tau t) + i\sin(\tau t))\\
& = & i\tau e^{i\tau t}.
\end{eqnarray*}

Euler's formula also implies that
\begin{equation}  \label{E:ecis}
e^{\lambda t} = e^{\sigma t + i\tau t} = e^{\sigma t}e^{i\tau t} =
e^{\sigma t}(\cos(\tau t) + i\sin(\tau t)),
\end{equation}
where $\lambda = \sigma+i\tau$.  Most importantly, we note that
\begin{equation}  \label{E:eldiff}
\frac{d}{dt}e^{\lambda t} = \lambda e^{\lambda t}.
\end{equation}
We use \Ref{E:ecis} and the product rule for differentiation to verify
\Ref{E:eldiff} as follows:
\begin{eqnarray*}
\frac{d}{dt}e^{\lambda t} & =  &
\frac{d}{dt}\left(e^{\sigma t}e^{i\tau t}\right)\\
& = & \left(\sigma e^{\sigma t}\right)e^{i\tau t}
+ e^{\sigma t}\left(i\tau e^{i\tau t}\right)\\
& = & (\sigma+i\tau)e^{\sigma t + i\tau t} \\
& = & \lambda e^{\lambda t}.
\end{eqnarray*}

\subsubsection*{Verification that \protect\Ref{E:CC2} is the General Solution}

A complex vector-valued function $X(t)=X_1(t)+iX_2(t)\in\C^n$
\index{complex valued solution} consists of
a {\em real part\/} $X_1(t)\in\R^n$ and an {\em imaginary part\/}
\index{complex valued solution!real part}
\index{complex valued solution!imaginary part}
$X_2(t)\in\R^n$.  For such functions $X(t)$ we define
\[
\dot{X} = \dot{X}_1+i\dot{X}_2
\]
and
\[
CX = CX_1 + iCX_2.
\]
To say that $X(t)$ is a solution to $\dot{X}=CX$ means that
\begin{equation}  \label{E:X1X2}
\dot{X}_1+i\dot{X}_2 = \dot{X} = CX = CX_1 + iCX_2.
\end{equation}

\begin{lemma}  \label{L:RIsoln}
The complex vector-valued function $X(t)$ is a solution to $\dot{X}=CX$ if
and only if the real and imaginary parts are real vector-valued solutions
to $\dot{X}=CX$.
\end{lemma}

\begin{proof} Equating the real and imaginary parts of \Ref{E:X1X2} implies that
\[
\dot{X}_1 = CX_1 \AND \dot{X}_2 = CX_2.
\]
\end{proof}

It follows from Lemma~\ref{L:RIsoln} that finding one complex-valued solution
to a linear differential equation provides us with two real-valued solutions.
Identity \Ref{E:eldiff} implies that
\[
X(t) = e^{\lambda_1 t}v_1
\]
is a complex-valued solution to \Ref{e:eqnA}.  Using Euler's formula we
compute the real and imaginary parts of $X(t)$, as follows.
\begin{eqnarray*}
X(t) & = & e^{(\sigma+i\tau)t}(v+iw) \\
& = & e^{\sigma t} (\cos(\tau t)+i\sin(\tau t))(v+iw)\\
& = & e^{\sigma t}(\cos(\tau t)v-\sin(\tau t)w)+
ie^{\sigma t}(\sin(\tau t)v+\cos(\tau t)w).
\end{eqnarray*}
Since the real and imaginary parts of $X(t)$ are solutions to $\dot{X}=CX$, it
follows that the real-valued functions $X_1(t)$ and $X_2(t)$ defined in
\Ref{E:CC1} are indeed solutions.

Returning to the case where $C$ is a $2\times 2$ matrix, we see that if
$X_1(0)=v$ and $X_2(0)=w$ are linearly independent, then
Corollary~\ref{C:indsoln} implies that \Ref{E:CC2} is the general solution to
$\dot{X}=CX$.  The linear independence of $v$ and $w$ is verified using the
following lemma.

\begin{lemma}  \label{L:rievind}
Let $\lambda_1=\sigma+i\tau$ with $\tau\neq 0$ be a
complex eigenvalue\index{eigenvalue!complex} of the
$2\times 2$ matrix $C$ with eigenvector\index{eigenvector}
$v_1=v+iw$ where $v,w\in\R^2$.  Then
\begin{equation}  \label{e:complexcoord}
\begin{array}{rcl}
Cv & = & \sigma v - \tau w \\
Cw & = & \tau v + \sigma w.
\end{array}
\end{equation}
and $v$ and $w$ are linearly independent\index{linearly!independent} vectors.
\end{lemma}

\begin{proof}   By assumption $Cv_1=\lambda_1v_1$, that is,
\begin{equation}  \label{E:viw}
C (v+iw) = (\sigma+i\tau)(v+iw) = (\sigma v - \tau w) + i(\tau v + \sigma w).
\end{equation}
Equating real and imaginary parts of \Ref{E:viw} leads to the system of
equations \Ref{e:complexcoord}.  Note that if $w=0$, then $v\neq 0$ and
$\tau v = 0$.  Hence $\tau=0$, contradicting the assumption that
$\tau\neq 0$.  So $w\neq 0$.

Note also that if $v$ and $w$ are linearly dependent, then $v=\alpha w$.
It then follows from the previous equation that
\[
Cw = (\tau\alpha+\sigma)w.
\]
Hence $w$ is a real eigenvector; but the eigenvalues of $C$ are not real and
$C$ has no real eigenvectors.   \end{proof}


\subsubsection*{An Example with Complex Eigenvalues}

Consider an example of an initial value problem for a linear system with
complex eigenvalues.  Let
\begin{equation}  \label{e:complexexample}
\frac{dX}{dt} = \mattwo{-1}{2}{-5}{-3} X = CX,
\end{equation}
and
\[
X_0=\vectwo{1}{1}.
\]

The characteristic polynomial\index{characteristic polynomial} for the
matrix $C$ is:
\[
p_C(\lambda) = \lambda^2 +4\lambda + 13,
\]
whose roots are $\lambda_1 = -2+3i$ and $\lambda_2 = -2-3i$. So
\[
\sigma = -2 \AND \tau = 3.
\]
An eigenvector corresponding to the eigenvalue $\lambda_1$ is
\[
v_1 = \vectwoc{2}{-1+3i} = \vectwo{2}{-1}+i\vectwo{0}{3}=v+iw.
\]
It follows from \Ref{E:CC1} that
\[
\begin{array}{rcl}
X_1(t) & = & e^{-2t}(\cos(3t)v -\sin(3t)w)\\
X_2(t) & = & e^{-2t}(\sin(3t)v +\cos(3t)w),
\end{array}
\]
are solutions to \Ref{e:complexexample} and $X=\alpha_1X_1+\alpha_2X_2$ is
the general solution to \Ref{e:complexexample}.  To solve the initial value
problem we need to find $\alpha_1,\alpha_2$ such that
\[
X_0 = X(0) = \alpha_1X_1(0) + \alpha_2X_2(0) = \alpha_1 v + \alpha_2 w,
\]
that is,
\[
\vectwo{1}{1} = \alpha_1\vectwo{2}{-1}  + \alpha_2\vectwo{0}{3}.
\]
Therefore, $\alpha_1 = \frac{1}{2}$ and $\alpha_2=\frac{1}{2}$ and
\begin{equation}  \label{e:complexexampleans}
X(t) =  e^{-2t}\vectwoc{\cos(3t)+\sin(3t)}{\cos(3t)-2\sin(3t)}.
\end{equation}

\subsection*{Real and Equal Eigenvalues}

There are two types of $2\times 2$ matrices that have real and equal
\index{eigenvalue!real!equal}
eigenvalues --- those that are scalar multiples of the identity and those
that are not.  An example of a $2\times 2$ matrix that has real and equal
eigenvalues is
\begin{equation}  \label{E:equalex}
A = \mattwoc{\lambda_1}{1}{0}{\lambda_1}, \quad \lambda_1\in\R.
\end{equation}
The characteristic polynomial of $A$ is
\[
p_A(\lambda) = \lambda^2 - \trace(A)\lambda + \det(A) =
\lambda^2 -2\lambda_1\lambda + \lambda_1^2 = (\lambda-\lambda_1)^2.
\]
Thus the eigenvalues of $A$ both equal $\lambda_1$.

\subsubsection*{Only One Linearly Independent Eigenvector}

An important fact about the matrix $A$ in \Ref{E:equalex} is that it has
only one linearly independent eigenvector.  To verify this fact, solve the
system of linear equations
\[
Av = \lambda_1v.
\]
In matrix form this equation is
\[
0 = (A-\lambda_1I_2)v = \mattwo{0}{1}{0}{0}v.
\]
A quick calculation shows that all solutions are multiples of
$v_1=e_1=(1,0)^t$.

In fact, this observation is valid for any $2\times 2$ matrix that has
equal eigenvalues and is not a scalar multiple of the identity, as the next
lemma shows.

\begin{lemma}  \label{L:1indeig}
Let $A$ be a $2\times 2$ matrix.  Suppose that $A$ has two linearly
independent eigenvectors both with eigenvalue $\lambda_1$.
Then $A=\lambda_1 I_2$.
\end{lemma}\index{eigenvector!linearly independent}\index{eigenvector!real}

\begin{proof}  Let $v_1$ and $v_2$ be two linearly independent
eigenvectors of $A$, that is, $Av_j = \lambda_1 v_j$.  It follows
from linearity that $Av=\lambda_1 v$ for any linear combination
$v=\alpha_1v_1+\alpha_2v_2$.  Since $v_1$ and $v_2$ are linearly
independent and $\dim(\R^2)=2$, it follows that $\{v_1,v_2\}$ is
a basis of $\R^2$.  Thus, every vector $v\in\R^2$ is a linear
combination of $v_1$ and $v_2$.  Therefore, $A$ is $\lambda_1$ times
the identity matrix.  \end{proof}

\subsubsection*{Generalized Eigenvectors}

Suppose that $C$ has exactly one linearly independent real eigenvector $v_1$
with real eigenvalue $\lambda_1$.  We call $w_1$ a {\em generalized
eigenvector\/}\index{eigenvector!generalized} of $C$ it satisfies the
system of linear equations
\begin{equation} \label{e:Cw=lw+va}
(C-\lambda_1I_2)w_1 = v_1.
\end{equation}

The matrix $A$ in \Ref{E:equalex} has a generalized eigenvector. To verify
this point solve the linear system
\[
(A-\lambda_1I_2)w_1 = \mattwo{0}{1}{0}{0}w_1 = v_1 = \vectwo{1}{0}
\]
for $w_1=e_2$.   Note that for this matrix $A$, $v_1=e_1$ and $w_1=e_2$ are
linearly independent.  The next lemma shows that this observation about
generalized eigenvectors is always valid.

\begin{lemma}  \label{L:geneig2}
Let $C$ be a $2\times 2$ matrix with both eigenvalues equal to $\lambda_1$
and with one linearly independent eigenvector $v_1$.  

\noindent (a)  Let $w_1$ be a generalized eigenvector of $C$, then $v_1$ and 
$w_1$ are linearly independent. 

\noindent (b)  Let $w$ be any vector such that $v_1$ and $w$ are linearly 
independent.  Then $w$ is a nonzero scalar multiple of a generalized 
eigenvector of $C$.
\end{lemma}

\begin{proof}  (a) If $v_1$ and $w_1$ were linearly dependent, then $w_1$ would be 
a multiple of $v_1$ and hence an eigenvector of $C$.  But $C-\lambda_1I_2$
applied to an eigenvector is zero, which is a contradiction.  Therefore, 
$v_1$ and $w_1$ are linearly independent.

(b) Let $w$ be any vector that is linearly independent of
the eigenvector $v_1$.  It follows that  $\{v_1,w\}$ is a basis for
$\R^2$; hence
\begin{equation} \label{E:Cw1}
Cw = \alpha v_1 + \beta w
\end{equation}
for some scalars $\alpha$ and $\beta$.    If $\alpha=0$, then
$w$ is an eigenvector of $C$, contradicting the assumption that $C$ has
only one linearly independent eigenvector.  Therefore, $\alpha\neq 0$.

We claim that $\beta=\lambda_1$ and we prove the claim by showing that
$\beta$ is an eigenvalue of $C$.  Hence $\beta$ must equal $\lambda_1$
since both eigenvalues of $C$ equal $\lambda_1$.  To see that $\beta$
is an eigenvalue, define the nonzero vector
\[
u = \alpha v_1 +(\beta-\lambda_1)w
\]
and compute
\[
Cu = \lambda_1 \alpha v_1 + (\beta-\lambda_1)(\alpha v_1+\beta w) =
\beta u.
\]
So $u$ is an eigenvector of $C$ with eigenvalue $\beta$.
It now follows from \Ref{E:Cw1} that
\[
(C-\lambda_1I_2)w = \alpha v_1.
\]
Therefore, $w_1=\frac{1}{\alpha}w$ is a generalized eigenvector
of $C$.  \end{proof}



\subsubsection*{Independent Solutions to Differential Equations with
Equal Eigenvalues}

In the equal eigenvalue one eigenvector case, we
claim that the general solution\index{general solution} to $\dot{X}=CX$ is:
\begin{equation}  \label{e:exp1eva}
X(t) = e^{\lambda_1 t}\left(\alpha v_1 +\beta (w_1+tv_1)\right),
\end{equation}
where $v_1$ is an eigenvector of $C$ and $w_1$ is the generalized
eigenvector.

Since $v_1$ is an eigenvector of $C$ with eigenvalue $\lambda_1$, the
function $X_1(t)=e^{\lambda_1 t}v_1$ is a solution to $\dot{X}=CX$.  Suppose
that we can show that $X_2(t)=e^{\lambda_1 t}(w_1+tv_1)$ is also a solution
to $\dot{X}=CX$.  Then \Ref{e:exp1eva} is the general solution, since
$X_1(0)=v_1$ and $X_2(0)=w_1$ are linearly independent by 
Lemma~\ref{L:geneig2}(a).  Apply Theorem~\ref{T:solvends}.

To verify that $X_2(t)$ is a solution (that is, that $\dot{X}_2=CX_2$),
calculate
\[
\dot{X}_2(t) = \lambda_1 e^{\lambda_1 t}(w_1+tv_1) + e^{\lambda_1 t}v_1=
e^{\lambda_1 t}(\lambda_1 w_1 + v_1 +t\lambda v_1)
\]
and
\[
CX_2(t) = e^{\lambda_1 t}(Cw_1+tCv_1) = e^{\lambda_1 t}
((v_1+\lambda_1 w_1)+t\lambda_1 v_1),
\]
using \Ref{e:Cw=lw+va}.  Note that $X(0)=\alpha v_1 + \beta w_1$, so $\alpha$
and $\beta$ are found by solving $X_0= \alpha v_1 + \beta w_1$.

\subsubsection*{An Example with Equal Eigenvalues}

Consider the system of differential equations
\begin{equation}  \label{e:shearexample}
\frac{dX}{dt} = \mattwo{1}{-1}{9}{-5} X
\end{equation}
with initial value
\[
X_0 = \vectwo{2}{3}.
\]
The characteristic polynomial for the matrix $C=\mattwo{1}{-1}{9}{-5}$ is
\[
p_C(\lambda) = \lambda^2 + 4\lambda +4 = (\lambda + 2)^2.
\]
Thus $\lambda_1=-2$ is an eigenvalue of multiplicity two.  Since
$C$ is not a multiple of the identity matrix, it must have
precisely one linearly independent eigenvector $v_1$.  This eigenvector is
found by solving the equation
\[
0 = (C-\lambda_1I_2)v_1 = (C+2I_2)v_1 = \mattwo{3}{-1}{9}{-3}v_1
\]
for
\[
v_1=\vectwo{1}{3}.
\]
To find the generalized eigenvector $w_1$, we solve the system of linear
equations
\[
(C-\lambda_1I_2)w_1=(C+2I_2)w_1=\mattwo{3}{-1}{9}{-3}w_1= v_1=\vectwo{1}{3}
\]
by row reducing the augmented matrix
\[
\left(\begin{array}{rr|r} 3 & -1 & 1\\ 9 & -3 & 3 \end{array}\right)
\]
to obtain
\[
w_1 = \vectwo{1}{2}.
\]

We may now apply \Ref{e:exp1eva} to find the general solution to
\Ref{e:shearexample}
\[
X(t) = e^{-2t}\left(\alpha v_1 +\beta (w_1+tv_1)\right).
\]
We solve the initial value problem by solving
\[
\vectwo{2}{3} = X_0 = X(0) = \alpha v_1 +\beta w_1 =
\mattwo{1}{1}{3}{2}\vectwo{\alpha}{\beta}
\]
for $\alpha=-1$ and $\beta=3$.   So the closed form solution to this initial
value problem is
\begin{eqnarray*}
X(t) & = & e^{-2t}\left(-v_1 + 3(w_1+tv_1)\right)\\
& = & e^{-2t}\left(-\vectwo{1}{3}+3\vectwoc{1+t}{2+3t}\right)\\
& = & e^{-2t}\vectwo{2+3t}{3+9t}.
\end{eqnarray*}

There is a simpler method for finding this solution --- a method that
does not require solving for either the eigenvector $v_1$ or the generalized
eigenvector $w_1$ that we will discuss later.  See Section~\ref{S:6.6}.

\EXER

\TEXER

\begin{exercise}  \label{c6.6.05}
Justify Euler's formula \Ref{E:Euler} as follows.  Recall the
Taylor series
\begin{eqnarray*}
e^x & = & 1 + x + \frac{1}{2!}x^2 + \cdots + \frac{1}{n!}x^n + \cdots\\
\cos x & = & 1 - \frac{1}{2!}x^2 + \frac{1}{4!}x^4 + \cdots +
(-1)^n \frac{1}{(2n)!}x^{2n} + \cdots \\
\sin x & = & x - \frac{1}{3!}x^3 + \frac{1}{5!}x^5 + \cdots +
(-1)^n \frac{1}{(2n+1)!}x^{2n+1} + \cdots.
\end{eqnarray*}
Now evaluate the Taylor series $e^{i\theta}$ and separate into real and
imaginary parts.
\end{exercise}

 In modern language De Moivre's formula states that
\[
e^{ni\theta} = \left(e^{i\theta}\right)^n.
\]
In Exercises~\ref{c6.6.1a} -- \ref{c6.6.1b} use De Moivre's formula coupled
with Euler's formula \Ref{E:Euler} to determine trigonometric identities
for the given quantity in terms of $\cos\theta$, $\sin\theta$, $\cos\varphi$,
$\sin\varphi$.
\begin{exercise}  \label{c6.6.1a}
$\cos(\theta+\varphi)$.
\end{exercise}
\begin{exercise}  \label{c6.6.1b}
$\sin(3\theta)$.
\end{exercise}

In Exercises~\ref{c6.6.2a} -- \ref{c6.6.2d} compute the general solution for
the given system of differential equations.
\begin{exercise}  \label{c6.6.2a}
$\dps\frac{dX}{dt} = \mattwo{-1}{-4}{2}{3} X$.
\end{exercise}
\begin{exercise}  \label{c6.6.2b}
$\dps\frac{dX}{dt} = \mattwo{8}{-15}{3}{-4} X$.
\end{exercise}
\begin{exercise}  \label{c6.6.2c}
$\dps\frac{dX}{dt} = \mattwo{5}{-1}{1}{3} X$.
\end{exercise}
\begin{exercise}  \label{c6.6.2d}
$\dps\frac{dX}{dt} = \mattwo{-4}{4}{-1}{0} X$.
\end{exercise}

\end{document}
