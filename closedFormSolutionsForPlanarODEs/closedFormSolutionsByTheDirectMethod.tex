\documentclass{ximera}

\input{../preamble.tex}

\title{Closed Form Solutions by the Direct Method}

\begin{document}
\begin{abstract}
\end{abstract}
\maketitle


\label{S:TDM}


In Section~\ref{S:IVPR} we showed in detail how solutions to planar systems
of constant coefficient differential equations with distinct real eigenvalues
are found.  This method was just reviewed in Section~\ref{S:6.1} where we saw
that the crucial step in solving these systems of differential equations is
the step where we find two linearly independent solutions.   In this section
we discuss how to find these two linearly independent solutions when the
eigenvalues of the coefficient matrix are either complex or real and equal.

By finding these two linearly independent solutions we will find both
the {\em general\/} solution of the system of differential equations
$\dot{X}=CX$ and a method for solving the initial value
problem\index{initial value problem}
\begin{equation}  \label{e:eqnA}
\begin{array}{rcl}
\dps\frac{dX}{dt} & = & CX\\
X(0) & = & X_0.
\end{array}
\end{equation}

The principle results of this section are summarized as follows.  Let 
$C$ be a $2\times 2$ matrix with eigenvalues $\lambda_1$ and
$\lambda_2$, and associated eigenvectors $v_1$ and $v_2$. 
\begin{enumerate}
\item	If the eigenvalues are real and $v_1$ and $v_2$ are linearly independent,
then the general solution to \eqref{e:eqnA} is given by \eqref{E:RD2}.
\item If the eigenvalues are complex, then the general solution to \eqref{e:eqnA} 
is given by \eqref{E:CC1} and \eqref{E:CC2}.
\item If the eigenvalues are equal (and hence real) and there is only one linearly 
independent eigenvector, then the general solution to \eqref{e:eqnA} is given by 
\eqref{e:exp1eva}. 
\end{enumerate}


\subsection*{Real Distinct Eigenvalues}
\index{eigenvalue!real}

We have discussed the case when $\lambda_1\neq\lambda_2\in\R$ on several
occasions.  For completeness we repeat the result.  The general solution is:
\begin{equation}  \label{E:RD2}
X(t) = \alpha_1 e^{\lambda_1 t}v_1 + \alpha_2 e^{\lambda_2 t}v_2.
\end{equation}
The initial value problem is solved by finding real numbers $\alpha_1$ and
$\alpha_2$ such that
\[
X_0 = \alpha_1 v_1 + \alpha_2 v_2.
\]
See Section~\ref{S:IVPR} for a detailed discussion with examples.

\subsection*{Complex Conjugate Eigenvalues}
\index{eigenvalue!complex}

Suppose that the eigenvalues of $C$ are complex, that is, suppose that
$\lambda_1= \sigma+i\tau$ with $\tau\neq 0$ is an eigenvalue of $C$ with
eigenvector $v_1=v+iw$, where $v,w\in\R^2$.  We claim that
$X_1(t)$ and $X_2(t)$, where
\begin{equation}  \label{E:CC1}
\begin{array}{rcl}
X_1(t) & = & e^{\sigma t}(\cos(\tau t)v -\sin(\tau t)w)\\
X_2(t) & = & e^{\sigma t}(\sin(\tau t)v +\cos(\tau t)w),
\end{array}
\end{equation}
are solutions to \eqref{e:eqnA} and that the general
solution\index{general solution} to \eqref{e:eqnA} is:
\begin{equation}  \label{E:CC2}
X(t) = \alpha_1 X_1(t) + \alpha_2 X_2(t),
\end{equation}
where $\alpha_1, \alpha_2$ are real scalars.

\RED{Two basic observations are needed when deriving \eqref{E:CC1} and \eqref{E:CC2}; these
observations use basic properties of the complex exponential function.

The first property is Euler's celebrated formula:\index{Euler's formula}}
\begin{equation}  \label{E:Euler}
e^{i\tau} = \cos\tau + i\sin\tau
\end{equation}
for any real number $\tau$.  A justification of this formula is given in Exercise~\ref{c6.6.05}.   
\RED{The second property is the important feature of exponential functions:
\begin{equation} \label{e:complex_homo}
e^{x+y} = e^x e^y
\end{equation}
for any complex numbers $x, y$.
The two formulas together imply
\begin{equation} \label{e:complex_exp}
e^{\sigma+i\tau} = e^\sigma(\cos \tau + i \sin\tau)
\end{equation} 
}

Euler's formula allows us to differentiate
complex exponentials, obtaining the expected result:
\begin{eqnarray*}
\frac{d}{dt}e^{i\tau t} & = &
\frac{d}{dt}(\cos(\tau t) + i\sin(\tau t))\\
& = & \tau(-\sin(\tau t) + i\cos(\tau t)) \\
& = & i\tau(\cos(\tau t) + i\sin(\tau t))\\
& = & i\tau e^{i\tau t}.
\end{eqnarray*}

Euler's formula also implies that
\begin{equation}  \label{E:ecis}
e^{\lambda t} = e^{\sigma t + i\tau t} = e^{\sigma t}e^{i\tau t} =
e^{\sigma t}(\cos(\tau t) + i\sin(\tau t)),
\end{equation}
where $\lambda = \sigma+i\tau$.  Most importantly, we note that
\begin{equation}  \label{E:eldiff}
\frac{d}{dt}e^{\lambda t} = \lambda e^{\lambda t}.
\end{equation}
We use \eqref{E:ecis} and the product rule for differentiation to verify
\eqref{E:eldiff} as follows:
\begin{eqnarray*}
\frac{d}{dt}e^{\lambda t} & =  &
\frac{d}{dt}\left(e^{\sigma t}e^{i\tau t}\right)\\
& = & \left(\sigma e^{\sigma t}\right)e^{i\tau t}
+ e^{\sigma t}\left(i\tau e^{i\tau t}\right)\\
& = & (\sigma+i\tau)e^{\sigma t + i\tau t} \\
& = & \lambda e^{\lambda t}.
\end{eqnarray*}

\subsubsection*{Verification that \protect\eqref{E:CC2} is the General Solution}

A complex vector-valued function $X(t)=X_1(t)+iX_2(t)\in\C^n$
\index{complex valued solution} consists of
a {\em real part\/} $X_1(t)\in\R^n$ and an {\em imaginary part\/}
\index{complex valued solution!real part}
\index{complex valued solution!imaginary part}
$X_2(t)\in\R^n$.  For such functions $X(t)$ we define
\[
\dot{X} = \dot{X}_1+i\dot{X}_2
\]
and
\[
CX = CX_1 + iCX_2.
\]
To say that $X(t)$ is a solution to $\dot{X}=CX$ means that
\begin{equation}  \label{E:X1X2}
\dot{X}_1+i\dot{X}_2 = \dot{X} = CX = CX_1 + iCX_2.
\end{equation}

\begin{lemma}  \label{L:RIsoln}
The complex vector-valued function $X(t)$ is a solution to $\dot{X}=CX$ if
and only if the real and imaginary parts are real vector-valued solutions
to $\dot{X}=CX$.
\end{lemma}

\begin{proof} 
Equating the real and imaginary parts of \eqref{E:X1X2} implies that
$\dot{X}_1 = CX_1$ and $\dot{X}_2 = CX_2$.
\end{proof}

It follows from Lemma~\ref{L:RIsoln} that finding one complex-valued solution
to a linear differential equation provides us with two real-valued solutions.
Identity \eqref{E:eldiff} implies that
\[
X(t) = e^{\lambda_1 t}v_1
\]
is a complex-valued solution to \eqref{e:eqnA}.  Using Euler's formula we
compute the real and imaginary parts of $X(t)$, as follows.
\begin{eqnarray*}
X(t) & = & e^{(\sigma+i\tau)t}(v+iw) \\
& = & e^{\sigma t} (\cos(\tau t)+i\sin(\tau t))(v+iw)\\
& = & e^{\sigma t}(\cos(\tau t)v-\sin(\tau t)w) \\ 
& & \; + \; ie^{\sigma t}(\sin(\tau t)v+\cos(\tau t)w).
\end{eqnarray*}
Since the real and imaginary parts of $X(t)$ are solutions to $\dot{X}=CX$, it
follows that the real-valued functions $X_1(t)$ and $X_2(t)$ defined in
\eqref{E:CC1} are indeed solutions.

Returning to the case where $C$ is a $2\times 2$ matrix, we see that if
$X_1(0)=v$ and $X_2(0)=w$ are linearly independent, then
Corollary~\ref{C:indsoln} implies that \eqref{E:CC2} is the general solution to
$\dot{X}=CX$.  The linear independence of $v$ and $w$ is verified using the
following lemma.

\begin{lemma}  \label{L:rievind}
Let $\lambda_1=\sigma+i\tau$ with $\tau\neq 0$ be a
complex eigenvalue\index{eigenvalue!complex} of the
$2\times 2$ matrix $C$ with eigenvector\index{eigenvector}
$v_1=v+iw$ where $v,w\in\R^2$.  Then
\begin{equation}  \label{e:complexcoord}
\begin{array}{rcl}
Cv & = & \sigma v - \tau w \\
Cw & = & \tau v + \sigma w.
\end{array}
\end{equation}
and $v$ and $w$ are linearly independent\index{linearly!independent} vectors.
\end{lemma}

\begin{proof}   By assumption $Cv_1=\lambda_1v_1$, that is,
\begin{equation}  \label{E:viw}
\begin{array}{rcl}
C (v+iw) & = & (\sigma+i\tau)(v+iw) \\
& = & (\sigma v - \tau w) + i(\tau v + \sigma w).
\end{array}
\end{equation}
Equating real and imaginary parts of \eqref{E:viw} leads to the system of
equations \eqref{e:complexcoord}.  Note that if $w=0$, then $v\neq 0$ and
$\tau v = 0$.  Hence $\tau=0$, contradicting the assumption that
$\tau\neq 0$.  So $w\neq 0$.

Note also that if $v$ and $w$ are linearly dependent, then $v=\alpha w$.
It then follows from the previous equation that
\[
Cw = (\tau\alpha+\sigma)w.
\]
Hence $w$ is a real eigenvector; but the eigenvalues of $C$ are not real and
$C$ has no real eigenvectors.   \end{proof}


\subsubsection*{An Example with Complex Eigenvalues}

Consider an example of an initial value problem for a linear system with
complex eigenvalues.  Let
\begin{equation}  \label{e:complexexample}
\frac{dX}{dt} = \mattwo{-1}{2}{-5}{-3} X = CX,
\end{equation}
and
\[
X_0=\vectwo{1}{1}.
\]

The characteristic polynomial\index{characteristic polynomial} for the
matrix $C$ is:
\[
p_C(\lambda) = \lambda^2 +4\lambda + 13,
\]
whose roots are $\lambda_1 = -2+3i$ and $\lambda_2 = -2-3i$. So
\[
\sigma = -2 \AND \tau = 3.
\]
An eigenvector corresponding to the eigenvalue $\lambda_1$ is
\[
v_1 = \vectwoc{2}{-1+3i} = \vectwo{2}{-1}+i\vectwo{0}{3}=v+iw.
\]
It follows from \eqref{E:CC1} that
\[
\begin{array}{rcl}
X_1(t) & = & e^{-2t}(\cos(3t)v -\sin(3t)w)\\
X_2(t) & = & e^{-2t}(\sin(3t)v +\cos(3t)w),
\end{array}
\]
are solutions to \eqref{e:complexexample} and $X=\alpha_1X_1+\alpha_2X_2$ is
the general solution to \eqref{e:complexexample}.  To solve the initial value
problem we need to find $\alpha_1,\alpha_2$ such that
\[
X_0 = X(0) = \alpha_1X_1(0) + \alpha_2X_2(0) = \alpha_1 v + \alpha_2 w,
\]
that is,
\[
\vectwo{1}{1} = \alpha_1\vectwo{2}{-1}  + \alpha_2\vectwo{0}{3}.
\]
Therefore, $\alpha_1 = \frac{1}{2}$ and $\alpha_2=\frac{1}{2}$ and
\begin{equation}  \label{e:complexexampleans}
X(t) =  e^{-2t}\vectwoc{\cos(3t)+\sin(3t)}{\cos(3t)-2\sin(3t)}.
\end{equation}

\subsection*{Real and Equal Eigenvalues}

There are two types of $2\times 2$ matrices that have real and equal
\index{eigenvalue!real!equal}
eigenvalues --- those that are scalar multiples of the identity and those
that are not.  An example of a $2\times 2$ matrix that has real and equal
eigenvalues is
\begin{equation}  \label{E:equalex}
A = \mattwoc{\lambda_1}{1}{0}{\lambda_1}, \quad \lambda_1\in\R.
\end{equation}
The characteristic polynomial of $A$ is
\[
p_A(\lambda) = \lambda^2 - \trace(A)\lambda + \det(A) =
\lambda^2 -2\lambda_1\lambda + \lambda_1^2 = (\lambda-\lambda_1)^2.
\]
Thus the eigenvalues of $A$ both equal $\lambda_1$.

\subsubsection*{Only One Linearly Independent Eigenvector}

An important fact about the matrix $A$ in \eqref{E:equalex} is that it has
only one linearly independent eigenvector.  To verify this fact, solve the
system of linear equations
\[
Av = \lambda_1v.
\]
In matrix form this equation is
\[
0 = (A-\lambda_1I_2)v = \mattwo{0}{1}{0}{0}v.
\]
A quick calculation shows that all solutions are multiples of
$v_1=e_1=(1,0)^t$.

In fact, this observation is valid for any $2\times 2$ matrix that has
equal eigenvalues and is not a scalar multiple of the identity, as the next
lemma shows.

\begin{lemma}  \label{L:1indeig}
Let $C$ be a $2\times 2$ matrix.  Suppose that $C$ has two linearly
independent eigenvectors both with eigenvalue $\lambda_1$.
Then $C = \lambda_1 I_2$.
\end{lemma}\index{eigenvector!linearly independent}\index{eigenvector!real}

\begin{proof}  Let $v_1$ and $v_2$ be two linearly independent
eigenvectors of $C$; that is, $Cv_j = \lambda_1 v_j$.  Since $\dim(\R^2) = 2$,
Corollary~\ref{C:dim=n} implies that $\{v_1,v_2\}$ is a basis of $\R^2$.  Hence,
every vector $v$ has the form $v = \alpha_1v_1+\alpha_2v_2$.  Linearity implies 
\[
Cv = C( \alpha_1v_1+\alpha_2v_2) = \alpha_1\lambda_1 v_1+\alpha_2\lambda_1 v_2 = \lambda_1v
\]
Therefore, $Cv = \lambda_1 v$ for every $v\in\R^2$ and hence $C= \lambda_1 I_2$.  
\end{proof}

\subsubsection*{Generalized Eigenvectors}

Suppose that $C$ has exactly one linearly independent real eigenvector 
$v_1$ with a double real eigenvalue $\lambda_1$.  We call $w_1$ a 
{\em generalized eigenvector\/}\index{eigenvector!generalized} of $C$ 
it satisfies the system of linear equations
\begin{equation} \label{e:Cw=lw+va}
(C-\lambda_1I_2)w_1 = v_1.
\end{equation}

The matrix $A$ in \eqref{E:equalex} has a generalized eigenvector. To verify
this point solve the linear system
\[
(C-\lambda_1I_2)w_1 = \mattwo{0}{1}{0}{0}w_1 = v_1 = \vectwo{1}{0}
\]
for $w_1=e_2$.   Note that for this matrix $C$, $v_1 = e_1$ and $w_1 = e_2$ 
are linearly independent.  The next lemma shows that this observation about
generalized eigenvectors is always valid.

\begin{lemma}  \label{L:geneig2}
Let $C$ be a $2\times 2$ matrix with both eigenvalues equal to $\lambda_1$
and with one linearly independent eigenvector $v_1$.  
Let $w_1$ be a generalized eigenvector of $C$, then $v_1$ and 
$w_1$ are linearly independent. 

\ignore{
\noindent (b)  Let $w$ be any vector such that $v_1$ and $w$ are linearly 
independent.  Then $w$ is a nonzero scalar multiple of a generalized 
eigenvector of $C$.}
\end{lemma}

\begin{proof}  If $v_1$ and $w_1$ were linearly dependent, then $w_1$ would be 
a multiple of $v_1$ and hence an eigenvector of $C$.  But $C-\lambda_1I_2$
applied to an eigenvector is zero, which is a contradiction.  Therefore, 
$v_1$ and $w_1$ are linearly independent.
\ignore{
(b) Let $w$ be any vector that is linearly independent of
the eigenvector $v_1$.  It follows that  $\{v_1,w\}$ is a basis for
$\R^2$; hence
\begin{equation} \label{E:Cw1}
Cw = \alpha v_1 + \beta w
\end{equation}
for some scalars $\alpha$ and $\beta$.    If $\alpha=0$, then
$w$ is an eigenvector of $C$, contradicting the assumption that $C$ has
only one linearly independent eigenvector.  Therefore, $\alpha\neq 0$.

We claim that $\beta=\lambda_1$ and we prove the claim by showing that
$\beta$ is an eigenvalue of $C$.  Hence $\beta$ must equal $\lambda_1$
since both eigenvalues of $C$ equal $\lambda_1$.  To see that $\beta$
is an eigenvalue, define the nonzero vector
\[
u = \alpha v_1 +(\beta-\lambda_1)w
\]
and compute
\[
Cu = \lambda_1 \alpha v_1 + (\beta-\lambda_1)(\alpha v_1+\beta w) =
\beta u.
\]
So $u$ is an eigenvector of $C$ with eigenvalue $\beta$.
It now follows from \eqref{E:Cw1} that
\[
(C-\lambda_1I_2)w = \alpha v_1.
\]
Therefore, $w_1=\frac{1}{\alpha}w$ is a generalized eigenvector
of $C$.  }
\end{proof}

The Cayley Hamilton theorem (see Section~\ref{S:6.6}) coupled with matrix exponentials 
(see Section~\ref{S:Matrixexp}) lead to a simple 
method for finding solutions to differential equations in the multiple eigenvalue 
case --- one that does not require solving for either the eigenvector $v_1$ 
or the generalized eigenvector $w_1$.  We next prove the special case of
Cayley-Hamilton that is needed.

\begin{lemma} \label{L:specialCH}
Let $C$ be a $2\times 2$ matrix with a double eigenvalue $\lambda_1\in\R$. Then
\begin{equation} \label{e:specialCH}
(C-\lambda_1 I_2)^2 = 0.
\end{equation}
\end{lemma}

\begin{proof}
Suppose that $C$ has two linearly independent eigenvectors. Then Lemma~\ref{L:1indeig}
implies that $C-\lambda_1 I_2 = 0$ and hence that $(C-\lambda_1 I_2)^2 = 0$. 

Suppose that $C$ has one linearly independent eigenvector $v_1$ and a generalized 
eigenvector $w_1$.  It follows from Lemma~\ref{L:geneig2}(a) that $\{v_1,w_1\}$ is a basis 
of $\R^2$.  It also follows by definition of eigenvector and generalized eigenvector that 
\[
\begin{array}{rclcl}
(C-\lambda_1 I_2)^2v_1 & = & (C-\lambda_1 I_2) 0  & = & 0 \\
(C-\lambda_1 I_2)^2w_1 & = & (C-\lambda_1 I_2) v_1 & = & 0
\end{array}
\]
Hence,  \eqref{e:specialCH} is valid.
\end{proof}

\subsubsection*{Independent Solutions to Differential Equations with
Equal Eigenvalues}

Suppose that the $2\times 2$ matric $C$ has a double eigenvalue $\lambda_1$.  
Then the general solution\index{general solution} to the initial value problem 
$\dot{X}=CX$ and $X(0) = X_0$ is:
\begin{equation} \label{e:exp1eva}
X(t) = e^{\lambda_1 t} [I_2 + t(C-\lambda_1I_2)]X_0.
\end{equation}
This is the form of the solution that is given by matrix exponentials.
We verify \eqref{e:exp1eva} by observing that $X(0) = X_0$ and calculating  
\[
CX(t) =  e^{\lambda_1 t}[ C+ t(C^2-\lambda_1 C)]X_0 
\]
\[
\dot{X}(t) = e^{\lambda_1 t}[ \lambda_1  (I_2 + t(C-\lambda_1 I_2)) + (C-\lambda_1I_2)]X_0.
\]
Therefore 
\[
CX - \dot{X} = e^{\lambda_1 t} M  X_0
\]
where \eqref{e:specialCH} implies
\[
\begin{array}{rcl}
M & = & C+ t(C^2 - \lambda_1 C) - \lambda_1 (I_2 + t(C - \lambda_1 I_2)) \\
& & - (C - \lambda_1I_2)\\
& = & t(C - \lambda_1 I_2)^2 \\
& = & 0.
\end{array}
\]
on use of \eqref{e:specialCH}.  A remarkable feature of formula \eqref{e:exp1eva} is that it is not 
necessary to compute either the eigenvector of $C$ or its generalized eigenvector.
\ignore{
\begin{equation}  \label{e:exp1eva}
X(t) = e^{\lambda_1 t}\left(\alpha v_1 +\beta (w_1+tv_1)\right),
\end{equation}
where $v_1$ is an eigenvector of $C$ and $w_1$ is the generalized
eigenvector.

Since $v_1$ is an eigenvector of $C$ with eigenvalue $\lambda_1$, the
function $X_1(t)=e^{\lambda_1 t}v_1$ is a solution to $\dot{X}=CX$.  Suppose
that we can show that $X_2(t)=e^{\lambda_1 t}(w_1+tv_1)$ is also a solution
to $\dot{X}=CX$.  Then \eqref{e:exp1eva} is the general solution, since
$X_1(0)=v_1$ and $X_2(0)=w_1$ are linearly independent by 
Lemma~\ref{L:geneig2}(a).  Apply Theorem~\ref{T:solvends}.

To verify that $X_2(t)$ is a solution (that is, that $\dot{X}_2=CX_2$),
calculate
\[
\dot{X}_2(t) = \lambda_1 e^{\lambda_1 t}(w_1+tv_1) + e^{\lambda_1 t}v_1=
e^{\lambda_1 t}(\lambda_1 w_1 + v_1 +t\lambda v_1)
\]
and
\[
CX_2(t) = e^{\lambda_1 t}(Cw_1+tCv_1) = e^{\lambda_1 t}
((v_1+\lambda_1 w_1)+t\lambda_1 v_1),
\]
using \eqref{e:Cw=lw+va}.  Note that $X(0)=\alpha v_1 + \beta w_1$, so $\alpha$
and $\beta$ are found by solving $X_0= \alpha v_1 + \beta w_1$.
}
\subsubsection*{An Example with Equal Eigenvalues}

Consider the system of differential equations
\begin{equation}  \label{e:shearexample}
\frac{dX}{dt} = \mattwo{1}{-1}{9}{-5} X
\end{equation}
with initial value
\[
X_0 = \vectwo{2}{3}.
\]
The characteristic polynomial for the matrix $\mattwo{1}{-1}{9}{-5}$ is
\[
p_C(\lambda) = \lambda^2 + 4\lambda +4 = (\lambda + 2)^2.
\]
Thus $\lambda_1 = -2$ is an eigenvalue of multiplicity two.  
It follows that 
\[
C - \lambda_1 I_2 =  \mattwo{3}{-1}{9}{-3} 
\]
and from \eqref{e:exp1eva} that 
\[
X(t) = e^{-2t}\mattwoc{1+3t}{-t}{9t}{1-3t} \vectwo{2}{3} = e^{-2t}\vectwoc{2+3t}{3+9t}.
\]

\ignore{
Since
$C$ is not a multiple of the identity matrix, it must have
precisely one linearly independent eigenvector $v_1$.  This eigenvector is
found by solving the equation
\[
0 = (C-\lambda_1I_2)v_1 = (C+2I_2)v_1 = \mattwo{3}{-1}{9}{-3}v_1
\]
for
\[
v_1=\vectwo{1}{3}.
\]
To find the generalized eigenvector $w_1$, we solve the system of linear
equations
\[
(C-\lambda_1I_2)w_1=(C+2I_2)w_1=\mattwo{3}{-1}{9}{-3}w_1= v_1=\vectwo{1}{3}
\]
by row reducing the augmented matrix
\[
\left(\begin{array}{rr|r} 3 & -1 & 1\\ 9 & -3 & 3 \end{array}\right)
\]
to obtain
\[
w_1 = \vectwo{1}{2}.
\]

We may now apply \eqref{e:exp1eva} to find the general solution to
\eqref{e:shearexample}
\[
X(t) = e^{-2t}\left(\alpha v_1 +\beta (w_1+tv_1)\right).
\]
We solve the initial value problem by solving
\[
\vectwo{2}{3} = X_0 = X(0) = \alpha v_1 +\beta w_1 =
\mattwo{1}{1}{3}{2}\vectwo{\alpha}{\beta}
\]
for $\alpha=-1$ and $\beta=3$.   So the closed form solution to this initial
value problem is
\begin{eqnarray*}
X(t) & = & e^{-2t}\left(-v_1 + 3(w_1+tv_1)\right)\\
& = & e^{-2t}\left(-\vectwo{1}{3}+3\vectwoc{1+t}{2+3t}\right)\\
& = & e^{-2t}\vectwo{2+3t}{3+9t}.
\end{eqnarray*}

The Cayley Hamilton theorem leads to a third method for finding this solution 
--- one that does not require solving for either the eigenvector $v_1$ or the 
generalized eigenvector $w_1$.  See Section~\ref{S:6.6}.
}


\includeexercises

\end{document}
