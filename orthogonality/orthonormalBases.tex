\documentclass{ximera}

\input{../preamble.tex}

\title{Orthonormal Bases and Orthogonal Matrices}

\begin{document}
\begin{abstract}
\end{abstract}
\maketitle


\label{S:orthonormal}

In Section~\ref{S:coordinates} we discussed how to write the coordinates of
a vector in a basis.  We now show that finding coordinates of vectors in
certain bases is a very simple task --- these bases are called orthonormal
bases.

Nonzero vectors $v_1,\ldots,v_k$ in $\R^n$ are
{\em orthogonal\/}\index{orthogonal} if the
dot products\index{dot product}
\[
v_i\cdot v_j  =  0
\]
when $i\neq j$.  The vectors are
{\em orthonormal\/}\index{orthonormal} if they are
orthogonal and of unit length, that is,
\[
v_i\cdot v_i=1.
\]
The standard example of a set of orthonormal vectors in $\R^n$ is the
standard basis $e_1,\ldots,e_n$.

\begin{lemma} \label{L:orthog}
Nonzero orthogonal vectors are
linearly independent\index{linearly!independent}.
\end{lemma}

\begin{proof}  Let $v_1,\ldots,v_k$ be a set of nonzero orthogonal vectors in $\R^n$
and suppose that
\[
\alpha_1v_1 + \cdots + \alpha_kv_k = 0.
\]
To prove the lemma we must show that each $\alpha_j=0$.  Since
$v_i\cdot v_j = 0$ for $i\not= j$,
\begin{align*}
  \alpha_jv_j\cdot v_j &= \alpha_1v_1\cdot v_j + \cdots + \alpha_kv_k\cdot v_j \\
  &= (\alpha_1v_1 + \cdots +\alpha_kv_k)\cdot v_j = 0\cdot v_j = 0.
\end{align*}
Since $v_j\cdot v_j = ||v_j||^2> 0$, it follows that $\alpha_j=0$.  \end{proof}

\begin{corollary} \label{C:symmetric_distinct}
A set of $n$ nonzero orthogonal vectors in $\R^n$ is a basis.
\end{corollary}

\begin{proof}  Lemma~\ref{L:orthog} implies that the $n$ vectors are linearly
independent, and Chapter~\ref{C:vectorspaces}, Corollary~\ref{C:dim=n} states
that $n$ linearly independent vectors in $\R^n$ form a basis.  \end{proof}

Next we discuss how to find coordinates of a vector in an
{\em orthonormal basis}\index{basis!orthonormal},
that is, a basis consisting of orthonormal vectors.

\begin{theorem}  \label{T:orthocoord}
Let $V\subset\R^n$ be a subspace\index{subspace} and
let $\{v_1,\ldots,v_k\}$ be an
orthonormal basis of $V$.  Let $v\in V$ be a vector.   Then
\[
v = \alpha_1v_1 + \cdots + \alpha_kv_k.
\]
where
\[
\alpha_i = v\cdot v_i.
\]
\end{theorem}

\begin{proof}  Since $\{v_1,\ldots,v_k\}$ is a basis of $V$, we can write
\[
v = \alpha_1v_1 + \cdots + \alpha_kv_k
\]
for some scalars $\alpha_j$.  It follows that
\[
v\cdot v_j = (\alpha_1v_1 + \cdots + \alpha_kv_k)\cdot v_j = \alpha_j,
\]
as claimed.   \end{proof}

\subsubsection{An Example in $\R^3$}

Let
\begin{align*}
  v_1 &= \frac{1}{\sqrt{3}}(1,1,1), \\
  v_2 &= \frac{1}{\sqrt{6}}(1,-2,1), \\
  v_3 &= \frac{1}{\sqrt{2}}(1,0,-1).
\end{align*}
A short calculation verifies that these vectors have
unit length and are pairwise orthogonal.  Let $v=(1,2,3)$ be a vector
and determine the coordinates of $v$ in the basis ${\cal V}=\{v_1,v_2,v_3\}$.
Theorem~\ref{T:orthocoord} states that these coordinates are:
\[
[v]_{\cal V} = (v\cdot v_1, v\cdot v_2, v\cdot v_3)
= (2\sqrt{3},\frac{7}{\sqrt{6}},-\sqrt{2}).
\]


\subsection*{Matrices in Orthonormal Coordinates}

Next we discuss how to find the matrix associated with a linear map in an
orthonormal basis.  Let $L:\R^n\to\R^n$ be a linear map and let
${\cal V} = \{v_1,\ldots,v_n\}$ be an orthonormal basis for $\R^n$.  Then
the matrix associated to $L$ in the basis ${\cal V}$ can be calculated 
in terms of dot product.  That matrix is:
\begin{equation}  \label{e:coordorthomat}
[L]_{\cal V} = L(v_j)\cdot v_i.
\end{equation}
To verify \eqref{e:coordorthomat}, recall from Definition~\ref{D:matrixincoord} that 
the $(i,j)^{th}$ entry of $[L]_{\cal V}$ is
the $i^{th}$ entry in the vector $[L(v_j)]_{\cal V}$ which is
$L(v_j)\cdot v_i$ by Theorem~\ref{T:orthocoord}.

\subsubsection{An Example in $\R^2$}

Let ${\cal V}=\{v_1,v_2\}\subset\R^2$ where
\[
v_1=\frac{1}{\sqrt{2}}\vectwo{1}{1} \AND
v_2=\frac{1}{\sqrt{2}}\vectwo{1}{-1}.
\]
The set ${\cal V}$ is an orthonormal basis of $\R^2$.  Using
\eqref{e:coordorthomat} we can find the matrix associated to the linear map
\[
L_A(x) = \mattwo{2}{1}{-1}{3}x
\]
in the basis ${\cal V}$.  That is, compute
\[
[L]_{\cal V} =
\mattwo{Av_1\cdot v_1}{Av_2\cdot v_1}{Av_1\cdot v_2}{Av_2\cdot v_2}
=\frac{1}{2}\mattwo{5}{-3}{1}{5}.
\]

\subsection*{Orthogonal Matrices}

\begin{definition} \label{def:orthmat}
\index{matrix!orthogonal}
An $n\times n$ matrix $Q$ is {\em orthogonal\/} if its columns form an
orthonormal basis\index{basis!orthonormal}
of $\R^n$.
\end{definition}



The following lemma states elementary properties of orthogonal matrices.  Particularly, 
an orthogonal matrix is invertible and it is straightforward to compute its inverse.
\begin{lemma} \label{lem:orthprop}
Let $Q$ be an $n\times n$ matrix.  Then
\begin{itemize}
\item[(a)] $Q$ is orthogonal if and only if $Q^tQ=I_n$;
\item[(b)] $Q$ is orthogonal if and only if $Q^{-1} = Q^t$;
\item[(c)] If $Q_1,Q_2$ are orthogonal matrices, then $Q_1Q_2$ is
an orthogonal matrix.
\end{itemize}
\end{lemma}
\begin{proof}  (a) Let $Q=(v_1|\cdots|v_n)$.  Since $Q$ is orthogonal, the $v_j$
form an orthonormal basis.  By direct computation note that
$Q^tQ=\{(v_i\cdot v_j)\}=I_n$, since the $v_j$ are orthonormal. Note that
(b) is simply a restatement of (a).

\noindent (c) Now let $Q_1,Q_2$ be orthogonal. Then (a) implies
\[
(Q_1Q_2)^t(Q_1Q_2) = Q_2^tQ_1^tQ_1Q_2 = Q_2^tI_nQ_2 = Q_2^tQ_2 = I_n,
\]
thus proving (c).  \end{proof}




\subsection*{Remarks Concerning \Matlab}

In the next section we prove that every vector subspace of $\R^n$ has an
orthonormal basis (see Theorem~\ref{T:orthobasis}), and we present a method
for constructing such a basis (the Gram-Schmidt orthonormalization process).
Here we note that certain commands in \Matlab produce bases for vector spaces.
For those commands \Matlab always produces an orthonormal basis.  For example,
{\tt null(A)}\index{\computer!null} produces a basis for the null space
\index{null space} of $A$.  Take the $3\times 5$ matrix
\begin{matlabEquation}
\label{eq:Anull1}
A = \left(\begin{array}{rrrrr} 1 & 2 & 3 & 4 & 5\\ 0 & 1 & 2 & 3 & 4\\
2 & 3 & 4 & 0 & 0 \end{array}\right).
\end{matlabEquation}
Since $\rank(A)=3$, it follows that the null space of $A$ is two-dimensional.
Typing {\tt B = null(A)} in \Matlab produces
\begin{verbatim}
B =
   -0.4019   -0.2371
    0.3789    0.7244
   -0.0833   -0.4247
    0.6372   -0.3753
   -0.5310    0.3127
\end{verbatim}
The columns of $B$ form an orthonormal basis for the null space of $A$.
This assertion can be checked by first typing
\begin{verbatim}
v1 = B(:, 1);
v2 = B(:, 2);
\end{verbatim}
and then typing
\begin{verbatim}
norm(v1)
norm(v2)
dot(v1,v2)
A*v1
A*v2
\end{verbatim}\index{\computer!norm}
yields answers $1,1,0$, $(0,0,0)^t,(0,0,0)^t$
(to within numerical accuracy).  Recall that the \Matlab
command {\tt norm(v)} computes the norm of a vector {\tt v}.







\includeexercises



\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../linearAlgebra"
%%% End:
