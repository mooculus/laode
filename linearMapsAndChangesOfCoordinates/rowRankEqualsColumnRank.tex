\documentclass{ximera}

\input{../preamble.tex}

\title{Row Rank Equals Column Rank}

\begin{document}
\begin{abstract}
\end{abstract}
\maketitle

 \label{S:5.8}

Let $A$ be an $m\times n$ matrix.  The {\em row space\/}
\index{row!space} of $A$ is the span of the row vectors of $A$
and is a subspace of $\R^n$.  The {\em column space\/}
\index{column!space} of $A$ is the span of the columns of $A$
and is a subspace of $\R^m$.
\begin{definition} 
The {\em row rank\/} of $A$ is the dimension of the
row space of $A$ and the {\em column rank\/} of $A$ is the
dimension of the column space of $A$.
\end{definition} \index{row!rank}  \index{column!rank}
Lemma~\ref{L:computerank} of Chapter~\ref{C:vectorspaces} states that
\[
\mbox{row rank}(A) = \rank(A).
\]
We show below that row ranks and column ranks are equal.  We
begin by continuing the discussion of the previous section on linear maps
between vector spaces.

\subsection*{Null Space and Range}

Each linear map between vector spaces defines two subspaces.  Let $V$ and $W$ 
be vector spaces and let $L:V\to W$ be a linear map.  Then
\[
\mbox{null space}(L) = \{v\in V: L(v)=0\} \subset V
\]
\index{null space} and \index{range}
\[
\mbox{range}(L) = \{L(v)\in W: v\in V \} \subset W.
\]

\begin{lemma} \label{L:nsr}
Let $L:V\to W$ be a linear map between vector spaces.  Then the null space of
$L$ is a subspace of $V$ and the range of $L$ is a subspace of $W$.
\end{lemma}\index{subspace}

\begin{proof}  The proof that the null space of $L$ is a subspace of $V$ follows
from linearity in precisely the same way that the null space of an
$m\times n$ matrix is a subspace of $\R^n$.  That is, if $v_1$ and $v_2$ are
in the null space of $L$, then
\[
L(v_1+v_2) = L(v_1) + L(v_2) = 0 + 0 = 0,
\]
and for $c\in\R$
\[
L(cv_1) = cL(v_1) = c0 = 0.
\]
So the null space of $L$ is closed under addition and scalar multiplication
and is a subspace of $V$.

To prove that the range of $L$ is a subspace of $W$, let $w_1$ and $w_2$ be
in the range of $L$.  Then, by definition, there exist $v_1$ and $v_2$ in $V$
such that $L(v_j)=w_j$.  It follows that
\[
L(v_1+v_2) = L(v_1) + L(v_2) = w_1 + w_2.
\]
Therefore, $w_1+w_2$ is in the range of $L$.  Similarly,
\[
L(cv_1) = cL(v_1) = cw_1.
\]
So the range of $L$ is closed under addition and scalar multiplication and is
a subspace of $W$.  \end{proof}

Suppose that $A$ is an $m\times n$ matrix and $L_A:\R^n\to\R^m$ is the
associated linear map.  Then the null space of $L_A$ is precisely the null
space of $A$, as defined in Definition~\ref{D:nullspace} of 
Chapter~\ref{C:vectorspaces}.  Moreover, the range of $L_A$ is the column 
space of $A$.  To verify this, write $A=(A_1|\cdots|A_n)$ where $A_j$ is the 
$j^{th}$ column of $A$ and let $v=(v_1,\ldots v_n)^t$.  Then, $L_A(v)$ is the 
linear combination of columns of $A$
\[
L_A(v)=Av = v_1A_1+\cdots+v_nA_n.
\]

There is a theorem that relates the dimensions of the null space and range
with the dimension of $V$.
\begin{theorem}  \label{T:nsr}
Let $V$ and $W$ be vector spaces with $V$ finite dimensional and let
$L:V\to W$ be a linear map.  Then
\[
\dim(V) = \dim(\mbox{\rm null space}(L)) + \dim({\rm range}(L)).
\]\index{dimension}\index{null space} \index{range}
\end{theorem}

\begin{proof}   Since $V$ is finite dimensional, the null space of $L$ is finite 
dimensional (since the null space is a subspace of $V$) and the range of $L$ 
is finite dimensional (since it is spanned by the vectors $L(v_j)$ where 
$v_1,\ldots,v_n$ is a basis for $V$).  Let $u_1,\ldots,u_k$ be a basis for 
the null space of $L$ and let $w_1,\ldots,w_\ell$ be a basis for the range of
$L$.   Choose vectors $y_j\in V$ such that $L(y_j)=w_j$.  We claim that
$u_1,\ldots,u_k,y_1,\ldots,y_\ell$ is a basis for $V$, which proves the
theorem.

To verify that $u_1,\ldots,u_k,y_1,\ldots,y_\ell$ are linear independent,
suppose that
\begin{equation}  \label{E:uy}
\alpha_1u_1+\cdots+\alpha_ku_k+\beta_1y_1+\cdots+\beta_\ell y_\ell = 0.
\end{equation}
Apply $L$ to both sides of \eqref{E:uy} to obtain
\[
\beta_1w_1+\cdots+\beta_\ell w_\ell = 0.
\]
Since the $w_j$ are linearly independent, it follows that $\beta_j=0$ for all
$j$.  Now  \eqref{E:uy} implies that
\[
\alpha_1u_1+\cdots+\alpha_ku_k = 0.
\]
Since the $u_j$ are linearly independent, it follows that $\alpha_j=0$ for
all $j$.

To verify that $u_1,\ldots,u_k,y_1,\ldots,y_\ell$ span $V$, let $v$ be in
$V$.  Since $w_1,\ldots,w_\ell$ span $W$, it follows that there exist scalars
$\beta_j$ such that
\[
L(v) = \beta_1w_1+\cdots+\beta_\ell w_\ell.
\]
Note that by choice of the $y_j$
\[
L(\beta_1y_1+\cdots+\beta_\ell y_\ell) = \beta_1w_1+\cdots+\beta_\ell w_\ell.
\]
It follows by linearity that
\[
u = v - (\beta_1y_1+\cdots+\beta_\ell y_\ell)
\]
is in the null space of $L$.  Hence there exist scalars $\alpha_j$ such that
\[
u = \alpha_1u_1+\cdots+\alpha_ku_k.
\]
Thus, $v$ is in the span of $u_1,\ldots,u_k,y_1,\ldots,y_\ell$, as desired.
\end{proof}

\subsection*{Row Rank and Column Rank}

Recall Theorem~\ref{T:dimsoln} of Chapter~\ref{C:vectorspaces} that states
that the nullity plus the rank of an $m\times n$ matrix equals $n$.  At first 
glance it might seem that this theorem and Theorem~\ref{T:nsr} contain the 
same information, but they do not.  Theorem~\ref{T:dimsoln} of 
Chapter~\ref{C:vectorspaces} is proved using a detailed analysis of solutions 
of linear equations based on Gaussian elimination, back substitution, and 
reduced echelon form, while Theorem~\ref{T:nsr} is proved using abstract 
properties of linear maps.

Let $A$ be an $m\times n$ matrix.  Theorem~\ref{T:dimsoln} of
Chapter~\ref{C:vectorspaces} states that 
\[
{\rm nullity}(A) + \rank(A) = n.
\]
Meanwhile, Theorem~\ref{T:nsr} states that 
\[
\dim(\mbox{\rm null space}(L_A)) + \dim({\rm range}(L_A)) = n.
\]
But the dimension of the null space of $L_A$ equals the nullity of $A$ 
and the dimension of the range of $A$ equals the dimension of the column 
space of $A$.  Therefore, 
\[
{\rm nullity}(A) + \dim(\mbox{column space}(A)) = n.
\]
Hence, the rank of $A$ equals the column rank of $A$.  Since rank and row rank 
are identical, we have proved:
\begin{theorem} \label{T:rowrank=columnrank}
Let $A$ be an $m\times n$ matrix.  Then
\[
\mbox{row rank } A=\mbox{column rank } A.
\]
\end{theorem}\index{row!rank}\index{column!rank}

Since the row rank of $A$ equals the column rank of $A^t$, we have:
\begin{corollary}
Let $A$ be an $m\times n$ matrix.  Then
\[
\rank(A) = \rank(A^t).
\]
\end{corollary}\index{matrix!transpose}


\EXER

\TEXER

\begin{exercise} \label{c5.8.1}
The $3\times 3$ matrix
\[
A = \left(\begin{array}{rrr} 1 & 2 & 5\\ 2 & -1 & 1\\ 3 & 1 & 6
\end{array}\right)
\]
has rank two.  Let $r_1,r_2,r_3$ be the rows of $A$ and
$c_1,c_2,c_3$ be the columns of $A$. Find scalars $\alpha_j$ and
$\beta_j$ such that
\begin{eqnarray*}
\alpha_1r_1+\alpha_2r_2+\alpha_3r_3 & = & 0 \\
\beta_1c_1+\beta_2c_2+\beta_3c_3 & = & 0.
\end{eqnarray*}

\begin{solution}

\ans The possible choices for the scalars $\alpha_j$ are
$\alpha = (\alpha_1,\alpha_2,\alpha_3) = \alpha_3(-1,-1,1)$ and
the possible choices for the scalars $\beta_j$ are $\beta = 
(\beta_1,\beta_2,\beta_3) = \beta_3(-\frac{7}{5},-\frac{9}{5},1)$.

\soln Find $A^t$ and solve by row reduction the equation
$A^t\alpha = 0$.  To find the scalars $\beta_j$, solve $A\beta =
0$.  These equations yield
\[
-r_1 - r_2 + r_3 = 0 \AND -7c_1 - 9c_2 + 5c_3 = 0.
\]

\end{solution}
\end{exercise}

\begin{exercise} \label{c5.8.2}
What is the largest row rank that a $5\times 3$ matrix can have?

\begin{solution}
The largest row rank that a $5 \times 3$ matrix can have
is $3$, since, by Theorem~\ref{T:rowrank=columnrank} the row rank is
equal to the column rank, and the matrix has $3$ columns.

\end{solution}
\end{exercise}

\begin{exercise} \label{c5.8.3}
Let
\[
A = \left(\begin{array}{rrrr} 1 & 1 & 0 & 1\\ 0 & -1 & 1 & 2\\
1 & 2 & -1 & 3 \end{array}\right).
\]
\begin{itemize}
\item[(a)]  Find a basis for the row space of $A$ and the row rank of $A$.
\item[(b)]  Find a basis for the column space of $A$ and the column rank of
$A$.
\item[(c)]  Find a basis for the null space of $A$ and the nullity of $A$.
\item[(d)]  Find a basis for the null space of $A^t$ and the nullity of $A^t$.
\end{itemize}

\begin{solution}

(a) \ans The vectors $(1,0,1,0)$, $(0,1,-1,0)$ and $(0,0,0,1)$ form a
basis for the row space of $A$, and the row rank of $A$ is $3$.

\soln Row reduce $A$:
\[
\left(\begin{array}{rrrr} 1 & 1 & 0 & 1 \\ 0 & -1 & 1 & 2 \\1 & 2
& -1 & 3 \end{array}\right) \longrightarrow \left(\begin{array}{rrrr}
1 & 0 & 1 & 0 \\ 0 & 1 & -1 & 0 \\0 & 0 & 0 & 1 \end{array}\right).
\]

(b) \ans The column rank of $A$ is $3$, and the vectors $(1,0,0)$,
$(0,1,0)$, and $(0,0,1)$ form a basis for the column space of $A$.

\soln Row reduce $A^t$:
\[
\left(\begin{array}{rrr} 1 & 0 & 1 \\ 1 & -1 & 2 \\ 0 & 1 & -1
\\ 1 & 2 & 3 \end{array}\right) \longrightarrow \left(\begin{array}{rrr}
1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \\ 0 & 0 & 0 \end{array}\right)
\]

(c) \ans The vector $(-1,1,1,0)$ is a basis for the null space.  Since one
vector forms the basis, the nullity of $A$ is $1$.  

\soln Solve $Ax = 0$ by row reducing $A$, which we have already done.

(d) \ans The null space is trivial and the nullity of $A^t$ is $0$.

\soln Find a basis by solving $A^tx = 0$ by row reduction.  The row
reduced matrix:
\[
\left(\begin{array}{rrr} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1
\\ 0 & 0 & 0 \end{array}\right)
\]
implies $x = (0,0,0)$.


\end{solution}
\end{exercise}

\begin{exercise} \label{c5.8.4}
Let $A$ be a nonzero $3\times 3$ matrix such that $A^2=0$.  Show that
$\rank(A)=1$.

\begin{solution}

Since $A$ is a nonzero $3 \times 3$ matrix, $\rank(A)$ can equal $1$,
$2$, or $3$.  If $\rank(A) = 3$, then $A$ is invertible, so there
exists a matrix $B$ such that $AB = I_3$.  Then $A^2B^2 = AABB = I_3$,
so $\rank(A^2) = 3$, which contradicts the assumption that $A^2 = 0$.
Therefore, $\rank(A) \neq 3$.

\para Let $\rank(A) = 2$ and let $v_1$ and $v_2$ be linearly independent
vectors such that $Av_1 \neq 0$ and $Av_2 \neq 0$.  By
Theorem~\ref{T:dimsoln}, the nullity of $A$ is $1$.  However,
$A^2v = 0$ for all vectors $v$.  In particular,
\[
A^2v_1 = A(Av_1) = 0 \AND A^2v_2 = A(Av_2) = 0.
\]
Since there are linearly independent vectors $Av_1$ and $Av_2$ such
that $A(Av_1) = A(Av_2) = 0$, $\null(A) \geq 2$, contradicting
Theorem~\ref{T:dimsoln}.  Thus, $\rank(A) \neq 2$, so, by elimination
$\rank(A) = 1$.

\end{solution}
\end{exercise}

\begin{exercise} \label{c5.8.5}
Let $B$ be an $m\times p$ matrix and let $C$ be a $p\times n$
matrix. Prove that the rank of the $m\times n$ matrix $A=BC$
satisfies
\[
\rank(A) \leq \min\{\rank(B),\;\rank(C)\}.
\]

\begin{solution}

We show that $\rank(A) \leq \min\{\rank(B),\rank(C)\}$ by noting that,
if $A = BC$, then the columns of $A$ are linear combinations of the
columns of $B$, so the span of the column space of $A$ cannot exceed
the span of the column space of $B$.  Therefore, $\rank(A) \leq
\rank(B)$.  Next, note that $A^t = C^tB^t$.  By a similar argument,
$\rank(A^t) \leq \rank(C^t)$.  Since $\rank(A) = \rank(A^t)$,
$\rank(A) \leq \min\{\rank(B),\rank(C)\}$.

\end{solution}
\end{exercise}



\CEXER

\begin{exercise} \label{c5.8.6}
Let
\begin{matlabEquation}\label{MATLAB:38}
A = \left(\begin{array}{rrrr} 1 & 1 & 2 & 2 \\ 0 & -1 & 3 & 1 \\
   2 & -1 & 1 & 0 \\ -1 & 0 & 7 & 4 \end{array}\right).
\end{matlabEquation}
\begin{itemize}
\item[(a)]  Compute $\rank(A)$ and exhibit a basis for the row space of $A$.
\item[(b)]  Find a basis for the column space of $A$.
\item[(c)]  Find all solutions to the homogeneous equation $Ax=0$.
\item[(d)]  Does
\[
Ax = \left( \begin{array}{c} 4 \\ 2\\ 2\\ 1 \end{array} \right)
\]
have a solution?
\end{itemize}

\begin{solution}

(a) \ans The vectors $(1,0,0,\frac{1}{12})$, $(0,1,0,\frac{3}{4})$,
and $(0,0,1,\frac{7}{12})$ form a basis for the row space of $A$ and
$\rank(A) = 3$.

\soln Row reducing {\tt A} in \Matlab yields:
\begin{verbatim}
ans =
    1.0000         0         0    0.0833
         0    1.0000         0    0.7500
         0         0    1.0000    0.5833
         0         0         0         0
\end{verbatim}

(b) \ans The vectors $(1,0,0,1)$, $(0,1,0,2)$, and $(0,0,1,-1)$ form a
basis for the column space of $A$.

\soln Row reduce $A^t$ in \Matlab with the command {\tt rref(A')} to obtain
\begin{verbatim}
ans =
      1            0            0            1      
      0            1            0            2      
      0            0            1           -1      
      0            0            0            0      
\end{verbatim}

(c) \ans $Ax = 0$ when $x = s(-\frac{1}{12},-\frac{3}{4}, -\frac{7}{12},1)$.

\soln The solutions to the homogeneous system $Ax = 0$ can be found
using the row reduced matrix $A$, which we computed in part (a).

(d) \ans The vector $(4,2,2,1)$ is not in the span of the columns of
$A$.

\soln Row reducing the augmented matrix
\[
\left(\begin{array}{rrrr|r} 1 & 1 & 2 & 2 & 4 \\ 0 & -1 & 3 &
1 & 2 \\ 2 & -1 & 1 & 0 & 2 \\ -1 & 0 & 7 & 4 & 1 \end{array}\right)
\]
in \Matlab yields
\begin{verbatim}
ans =
      1            0            0           1/12          0      
      0            1            0           3/4           0      
      0            0            1           7/12          0      
      0            0            0            0            1      
\end{verbatim}
Since there is a pivot point in the last column, the system is
inconsistent.




\end{solution}
\end{exercise}


\end{document}
